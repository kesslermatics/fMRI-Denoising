{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816525e4",
   "metadata": {
    "papermill": {
     "duration": 0.006576,
     "end_time": "2025-06-23T14:12:21.384204",
     "exception": false,
     "start_time": "2025-06-23T14:12:21.377628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# I-JEPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a9bb8",
   "metadata": {},
   "source": [
    "The relevant needed code snippets defining the model architecture and such things have been copied from the STRL_training notebook. The relevant part is at the very bottom of this notebook where the inference loop is defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02676079",
   "metadata": {
    "papermill": {
     "duration": 0.005357,
     "end_time": "2025-06-23T14:12:21.395208",
     "exception": false,
     "start_time": "2025-06-23T14:12:21.389851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dbf286",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:21.407377Z",
     "iopub.status.busy": "2025-06-23T14:12:21.406703Z",
     "iopub.status.idle": "2025-06-23T14:12:25.285847Z",
     "shell.execute_reply": "2025-06-23T14:12:25.285286Z"
    },
    "papermill": {
     "duration": 3.886706,
     "end_time": "2025-06-23T14:12:25.287188",
     "exception": false,
     "start_time": "2025-06-23T14:12:21.400482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# core train\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e32d5",
   "metadata": {
    "papermill": {
     "duration": 0.005226,
     "end_time": "2025-06-23T14:12:25.298215",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.292989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a45f76d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.310077Z",
     "iopub.status.busy": "2025-06-23T14:12:25.309770Z",
     "iopub.status.idle": "2025-06-23T14:12:25.313133Z",
     "shell.execute_reply": "2025-06-23T14:12:25.312661Z"
    },
    "papermill": {
     "duration": 0.010534,
     "end_time": "2025-06-23T14:12:25.314213",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.303679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "log_timings = True\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045a666",
   "metadata": {
    "papermill": {
     "duration": 0.005195,
     "end_time": "2025-06-23T14:12:25.324808",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.319613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a639648d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.336648Z",
     "iopub.status.busy": "2025-06-23T14:12:25.336199Z",
     "iopub.status.idle": "2025-06-23T14:12:25.345403Z",
     "shell.execute_reply": "2025-06-23T14:12:25.344523Z"
    },
    "papermill": {
     "duration": 0.016718,
     "end_time": "2025-06-23T14:12:25.346781",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.330063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic global config\n",
    "_GLOBAL_SEED = 0\n",
    "np.random.seed(_GLOBAL_SEED)\n",
    "torch.manual_seed(_GLOBAL_SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98806031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.360371Z",
     "iopub.status.busy": "2025-06-23T14:12:25.360125Z",
     "iopub.status.idle": "2025-06-23T14:12:25.365987Z",
     "shell.execute_reply": "2025-06-23T14:12:25.365325Z"
    },
    "papermill": {
     "duration": 0.013705,
     "end_time": "2025-06-23T14:12:25.367296",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.353591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data\": {\n",
    "        \"batch_size\": 4,\n",
    "        \"test_folders\": ['/kaggle/input/fmri-train-2-norm-v3/data/noisy_func_train_2.npy'],\n",
    "        \"num_workers\": 2,\n",
    "        \"pin_mem\": True,\n",
    "        \"root_path\": \"/kaggle/input\",\n",
    "        \"use_horizontal_flip\": False\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"folder\": \"/kaggle/working/logs\",\n",
    "        \"write_tag\": \"jepa\"\n",
    "    },\n",
    "    \"mask\": {\n",
    "        \"allow_overlap\": False,\n",
    "        \"aspect_ratio\": [0.75, 1.5],\n",
    "        \"enc_mask_scale\": [0.85, 1.0],\n",
    "        \"min_keep\": 10,\n",
    "        \"num_enc_masks\": 1,\n",
    "        \"num_pred_masks\": 1,\n",
    "        \"patch_size\": 16,\n",
    "        \"pred_mask_scale\": [0.15, 0.2]\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"copy_data\": False,\n",
    "        \"load_checkpoint\": True,\n",
    "        \"model_name\": \"vit_small\",\n",
    "        \"pred_depth\": 12,\n",
    "        \"pred_emb_dim\": 384,\n",
    "        \"read_checkpoint\": \"/kaggle/input/strl-jepa/pytorch/default/1/logs/jepa-latest.pth.tar\",\n",
    "        \"use_bfloat16\": True\n",
    "    },\n",
    "    \"optimization\": {\n",
    "        \"ema\": [0.996, 1.0],\n",
    "        \"epochs\": 25,\n",
    "        \"final_lr\": 1.0e-5,\n",
    "        \"final_weight_decay\": 0.4,\n",
    "        \"ipe_scale\": 1.0,\n",
    "        \"lr\": 0.001,\n",
    "        \"start_lr\": 0.0002,\n",
    "        \"warmup\": 50,\n",
    "        \"weight_decay\": 0.04\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3700f4e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.385140Z",
     "iopub.status.busy": "2025-06-23T14:12:25.384724Z",
     "iopub.status.idle": "2025-06-23T14:12:25.467913Z",
     "shell.execute_reply": "2025-06-23T14:12:25.467154Z"
    },
    "papermill": {
     "duration": 0.091976,
     "end_time": "2025-06-23T14:12:25.469314",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.377338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "resume_preempt = False\n",
    "rank = 0\n",
    "\n",
    "# -- META\n",
    "use_bfloat16 = args['meta']['use_bfloat16']\n",
    "model_name = args['meta']['model_name']\n",
    "load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
    "r_file = args['meta']['read_checkpoint']\n",
    "copy_data = args['meta']['copy_data']\n",
    "pred_depth = args['meta']['pred_depth']\n",
    "pred_emb_dim = args['meta']['pred_emb_dim']\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "# -- DATA\n",
    "batch_size = args['data']['batch_size']\n",
    "pin_mem = args['data']['pin_mem']\n",
    "num_workers = args['data']['num_workers']\n",
    "root_path = args['data']['root_path']\n",
    "test_folders = args['data']['test_folders']\n",
    "# --\n",
    "\n",
    "# -- MASK\n",
    "allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
    "patch_size = args['mask']['patch_size']  # patch-size for model training\n",
    "num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
    "min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
    "enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
    "num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
    "pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
    "aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
    "# --\n",
    "\n",
    "# -- OPTIMIZATION\n",
    "ema = args['optimization']['ema']\n",
    "ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
    "wd = float(args['optimization']['weight_decay'])\n",
    "final_wd = float(args['optimization']['final_weight_decay'])\n",
    "num_epochs = args['optimization']['epochs']\n",
    "warmup = args['optimization']['warmup']\n",
    "start_lr = args['optimization']['start_lr']\n",
    "lr = args['optimization']['lr']\n",
    "final_lr = args['optimization']['final_lr']\n",
    "\n",
    "# -- LOGGING\n",
    "folder = args['logging']['folder']\n",
    "tag = args['logging']['write_tag']\n",
    "\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "dump = os.path.join(folder, 'params-ijepa.yaml')\n",
    "with open(dump, 'w') as f:\n",
    "    yaml.dump(args, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a607614",
   "metadata": {
    "papermill": {
     "duration": 0.005297,
     "end_time": "2025-06-23T14:12:25.480901",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.475604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset creation and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd70360",
   "metadata": {
    "papermill": {
     "duration": 0.005148,
     "end_time": "2025-06-23T14:12:25.491492",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.486344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261d744",
   "metadata": {
    "papermill": {
     "duration": 0.005327,
     "end_time": "2025-06-23T14:12:25.502039",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.496712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b2470a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.513547Z",
     "iopub.status.busy": "2025-06-23T14:12:25.513257Z",
     "iopub.status.idle": "2025-06-23T14:12:25.517046Z",
     "shell.execute_reply": "2025-06-23T14:12:25.516213Z"
    },
    "papermill": {
     "duration": 0.010926,
     "end_time": "2025-06-23T14:12:25.518255",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.507329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from multiprocessing import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ea71b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.531600Z",
     "iopub.status.busy": "2025-06-23T14:12:25.531052Z",
     "iopub.status.idle": "2025-06-23T14:12:25.539831Z",
     "shell.execute_reply": "2025-06-23T14:12:25.539089Z"
    },
    "papermill": {
     "duration": 0.016518,
     "end_time": "2025-06-23T14:12:25.540880",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.524362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeSeriesMaskCollator:\n",
    "    def __init__(self, num_frames=300, frame_size=16, nenc=1, npred=1):\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self._itr_counter = Value('i', -1)\n",
    "        self.npred = 1\n",
    "        self.nenc = 1\n",
    "\n",
    "    def step(self):\n",
    "        i = self._itr_counter\n",
    "        with i.get_lock():\n",
    "            i.value += 1\n",
    "            v = i.value\n",
    "        return v\n",
    "    \n",
    "    def collate_merge_batches(self, batch):\n",
    "        merged = torch.cat([item for item in batch], dim=0)\n",
    "        return merged\n",
    "\n",
    "    def _sample_frame_mask(self, frame_idx):\n",
    "        mask = torch.zeros(self.num_frames, dtype=torch.int32)\n",
    "        mask[frame_idx] = 1\n",
    "        mask = torch.nonzero(mask.flatten()).squeeze()\n",
    "        return mask, frame_idx\n",
    "\n",
    "    def build_encoder_mask_from_pred(self, pred_masks):\n",
    "        enc_mask = torch.ones(self.num_frames, dtype=torch.int32)\n",
    "        for pred_mask in pred_masks:\n",
    "            enc_mask[pred_mask] = 0  # Zero out the masked regions\n",
    "        return torch.nonzero(enc_mask.flatten()).squeeze()\n",
    "\n",
    "    def __call__(self, batch, frame_idx=None):\n",
    "        collated_batch = self.collate_merge_batches(batch)\n",
    "        B = collated_batch.shape[0]\n",
    "\n",
    "        seed = self.step()\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "\n",
    "        collated_masks_enc, collated_masks_pred = [], []\n",
    "\n",
    "        for _ in range(B):\n",
    "            pred_masks = []\n",
    "            pred_frame_idxs = []\n",
    "            for _ in range(self.npred):\n",
    "                mask, idx = self._sample_frame_mask(frame_idx = frame_idx)\n",
    "                pred_masks.append(mask)\n",
    "                pred_frame_idxs.append(idx)\n",
    "\n",
    "            collated_masks_pred.append(pred_masks)\n",
    "\n",
    "            enc_masks = []\n",
    "            for _ in range(self.nenc):\n",
    "                enc_mask = self.build_encoder_mask_from_pred(pred_masks)\n",
    "                enc_masks.append(enc_mask)\n",
    "\n",
    "            collated_masks_enc.append(enc_masks)\n",
    "\n",
    "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
    "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
    "\n",
    "        return collated_batch, collated_masks_enc, collated_masks_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d077a358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.553335Z",
     "iopub.status.busy": "2025-06-23T14:12:25.553095Z",
     "iopub.status.idle": "2025-06-23T14:12:25.557188Z",
     "shell.execute_reply": "2025-06-23T14:12:25.556454Z"
    },
    "papermill": {
     "duration": 0.011774,
     "end_time": "2025-06-23T14:12:25.558404",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.546630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CollatorWrapper:\n",
    "    def __init__(self, collator):\n",
    "        self.collator = collator\n",
    "        self.frame_idx = 0\n",
    "\n",
    "    def set_frame_idx(self, idx):\n",
    "        self.frame_idx = idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.collator(batch, frame_idx=self.frame_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a210f5bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.571201Z",
     "iopub.status.busy": "2025-06-23T14:12:25.570661Z",
     "iopub.status.idle": "2025-06-23T14:12:25.574729Z",
     "shell.execute_reply": "2025-06-23T14:12:25.574195Z"
    },
    "papermill": {
     "duration": 0.011436,
     "end_time": "2025-06-23T14:12:25.575844",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.564408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_masks(x, masks):\n",
    "    \"\"\"\n",
    "    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
    "    :param masks: list of tensors containing indices of patches in [N] to keep\n",
    "    \"\"\"\n",
    "    all_x = []\n",
    "    for m in masks:\n",
    "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
    "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
    "    return torch.cat(all_x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cbc3704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.588103Z",
     "iopub.status.busy": "2025-06-23T14:12:25.587900Z",
     "iopub.status.idle": "2025-06-23T14:12:25.594678Z",
     "shell.execute_reply": "2025-06-23T14:12:25.594076Z"
    },
    "papermill": {
     "duration": 0.014198,
     "end_time": "2025-06-23T14:12:25.595804",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.581606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_collator = TimeSeriesMaskCollator() # defaults to 300 frames of size 16x16\n",
    "collate_wrapper = CollatorWrapper(mask_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a3951",
   "metadata": {
    "papermill": {
     "duration": 0.005445,
     "end_time": "2025-06-23T14:12:25.607053",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.601608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Image transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7db0f6de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.618512Z",
     "iopub.status.busy": "2025-06-23T14:12:25.618265Z",
     "iopub.status.idle": "2025-06-23T14:12:25.624584Z",
     "shell.execute_reply": "2025-06-23T14:12:25.623847Z"
    },
    "papermill": {
     "duration": 0.013418,
     "end_time": "2025-06-23T14:12:25.625758",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.612340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch # Import PyTorch\n",
    "\n",
    "class Compose:\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of callables): list of transforms to compose.\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img_array):\n",
    "        \"\"\"\n",
    "        Applies the composed transforms to the input array.\n",
    "        The input can be a NumPy array. The output will be a torch.Tensor\n",
    "        if torch.from_numpy is the last transform.\n",
    "        Args:\n",
    "            img_array (numpy.ndarray): Input image array (C, H, W).\n",
    "        Returns:\n",
    "            torch.Tensor: Transformed image tensor.\n",
    "        \"\"\"\n",
    "        for t in self.transforms:\n",
    "            img_array = t(img_array) # Note: img_array will become a torch.Tensor at the end\n",
    "        return img_array\n",
    "\n",
    "\n",
    "# Build the transform list\n",
    "transform_list = []\n",
    "\n",
    "def reshape_to_2d_timeline(patch_time_series):\n",
    "    # Convert to PyTorch tensor immediately for permute/reshape\n",
    "    patch_time_series_tensor = torch.from_numpy(patch_time_series).float()\n",
    "    height_patch = patch_time_series_tensor.shape[0]  # H\n",
    "    width_patch = patch_time_series_tensor.shape[1]   # W\n",
    "    total_time_steps = patch_time_series_tensor.shape[2]  # T   \n",
    "\n",
    "    # print(patch_time_series_tensor.shape)\n",
    "\n",
    "    \n",
    "    # Apply the corrected reshaping logic here\n",
    "    # Original tensor shape (H, W, T) -> (16, 16, total_time_steps)\n",
    "    # Permute to (H, T, W)\n",
    "    intermediate_tensor = patch_time_series_tensor.permute(0, 2, 1)\n",
    "    \n",
    "    sample = intermediate_tensor.reshape(height_patch, total_time_steps * width_patch)\n",
    "\n",
    "    # Add a channel dimension: (1, H, T*W)\n",
    "    sample = sample.unsqueeze(0)\n",
    "    return sample\n",
    "    #return patch_time_series_tensor\n",
    "\n",
    "transform_list += [reshape_to_2d_timeline]  # Add reshape transform\n",
    "transform_list += [lambda x: x.unsqueeze(0)] # add batch dimension\n",
    "\n",
    "# Composition of transforms\n",
    "transform = Compose(transform_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863bdc22",
   "metadata": {
    "papermill": {
     "duration": 0.005456,
     "end_time": "2025-06-23T14:12:25.636922",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.631466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72171b0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.648607Z",
     "iopub.status.busy": "2025-06-23T14:12:25.648100Z",
     "iopub.status.idle": "2025-06-23T14:12:25.651329Z",
     "shell.execute_reply": "2025-06-23T14:12:25.650655Z"
    },
    "papermill": {
     "duration": 0.010353,
     "end_time": "2025-06-23T14:12:25.652548",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.642195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3d1e292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.664073Z",
     "iopub.status.busy": "2025-06-23T14:12:25.663882Z",
     "iopub.status.idle": "2025-06-23T14:12:25.671248Z",
     "shell.execute_reply": "2025-06-23T14:12:25.670548Z"
    },
    "papermill": {
     "duration": 0.014569,
     "end_time": "2025-06-23T14:12:25.672403",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.657834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NoisyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, noisy_images_paths: list, transform=None, normalize=True):\n",
    "        \"\"\"Initialize fMRI dataset for denoising with memory-efficient loading,\n",
    "        extracting 16x16 patches with full time series per depth channel.\n",
    "\n",
    "        Args:\n",
    "            noisy_images_paths (list): List of paths to noisy fMRI volumes (.npy files)\n",
    "            stats: Dictionary containing 'mean' and 'std' for normalization.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            normalize (bool): Whether to apply channel-wise normalization.\n",
    "        \"\"\"\n",
    "        self.noisy_paths = noisy_images_paths\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.file_slice_mapping = [] # Stores (file_idx, z_idx, patch_y_idx, patch_x_idx)\n",
    "\n",
    "        patch_size = 16\n",
    "        dataset_length = 0\n",
    "\n",
    "        for i, path in enumerate(noisy_images_paths):\n",
    "            # Load metadata about the file shape without loading full content\n",
    "            data_shape = np.load(path, mmap_mode='r').shape\n",
    "            # Assuming data_shape is (H, W, Z, T)\n",
    "            total_height, total_width, depth_channels, total_time_steps = data_shape\n",
    "\n",
    "            # Calculate how many patches fit along each spatial dimension\n",
    "            num_patches_width = total_width // patch_size\n",
    "            num_patches_height = total_height // patch_size\n",
    "\n",
    "            # Iterate over all depth channels (Z)\n",
    "            for z_idx in range(depth_channels):\n",
    "                # Iterate through the spatial grid of patches\n",
    "                for patch_y_idx in range(num_patches_height):\n",
    "                    for patch_x_idx in range(num_patches_width):\n",
    "                        # Each combination of (file, z_idx, patch_y_idx, patch_x_idx) is a unique item\n",
    "                        self.file_slice_mapping.append((i, z_idx, patch_y_idx, patch_x_idx))\n",
    "                        dataset_length += 1\n",
    "\n",
    "        self.data_len = dataset_length\n",
    "        self.patch_size = patch_size # Store patch_size for __getitem__\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Use the mapping to determine which file, depth channel, and spatial patch to load\n",
    "        file_idx, z_idx, patch_y_idx, patch_x_idx = self.file_slice_mapping[index]\n",
    "\n",
    "        # Load data from the specific file\n",
    "        noisy_file_path = self.noisy_paths[file_idx]\n",
    "\n",
    "        # Load the full 4D array with mmap_mode to avoid loading everything into RAM\n",
    "        noisy_volume = np.load(noisy_file_path, mmap_mode='r')\n",
    "\n",
    "        # Calculate the starting and ending coordinates for the current patch\n",
    "        start_h = patch_y_idx * self.patch_size\n",
    "        end_h = start_h + self.patch_size\n",
    "        start_w = patch_x_idx * self.patch_size\n",
    "        end_w = start_w + self.patch_size\n",
    "\n",
    "        # Extract the 16x16 patch for the specific depth channel (z_idx)\n",
    "        # and include the entire time series.\n",
    "        # The resulting shape will be (patch_size, patch_size, time_steps)\n",
    "        patch_time_series = noisy_volume[start_h:end_h, start_w:end_w, z_idx, :].copy()\n",
    "\n",
    "        # patch = patch_time_series[:,:,0]\n",
    "        # patch_sum = np.sum(patch)\n",
    "        # if patch_sum < -25:\n",
    "        #     indices_to_zero_out.append(index)\n",
    "            \n",
    "\n",
    "        # Apply transformations if specified\n",
    "        if self.transform is not None:\n",
    "            # Your transform should expect a tensor of shape (C, H, W, T) or (H, W, T)\n",
    "            # depending on how you structure it. Adjust accordingly.\n",
    "            sample = self.transform(patch_time_series)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b42cbaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.683787Z",
     "iopub.status.busy": "2025-06-23T14:12:25.683600Z",
     "iopub.status.idle": "2025-06-23T14:12:25.752743Z",
     "shell.execute_reply": "2025-06-23T14:12:25.752178Z"
    },
    "papermill": {
     "duration": 0.076107,
     "end_time": "2025-06-23T14:12:25.753972",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.677865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = NoisyDataset(noisy_images_paths=test_folders, transform=transform, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd1f9e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.781236Z",
     "iopub.status.busy": "2025-06-23T14:12:25.781051Z",
     "iopub.status.idle": "2025-06-23T14:12:25.784720Z",
     "shell.execute_reply": "2025-06-23T14:12:25.784198Z"
    },
    "papermill": {
     "duration": 0.01057,
     "end_time": "2025-06-23T14:12:25.785742",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.775172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=collate_wrapper,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    "    pin_memory=pin_mem,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e46c5",
   "metadata": {
    "papermill": {
     "duration": 0.005148,
     "end_time": "2025-06-23T14:12:25.796337",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.791189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9af7c025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.808156Z",
     "iopub.status.busy": "2025-06-23T14:12:25.807960Z",
     "iopub.status.idle": "2025-06-23T14:12:25.811990Z",
     "shell.execute_reply": "2025-06-23T14:12:25.811186Z"
    },
    "papermill": {
     "duration": 0.011523,
     "end_time": "2025-06-23T14:12:25.813123",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.801600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7668ee3",
   "metadata": {
    "papermill": {
     "duration": 0.005587,
     "end_time": "2025-06-23T14:12:25.827972",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.822385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model backbone (vision transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0d103c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.841384Z",
     "iopub.status.busy": "2025-06-23T14:12:25.841125Z",
     "iopub.status.idle": "2025-06-23T14:12:25.849028Z",
     "shell.execute_reply": "2025-06-23T14:12:25.848518Z"
    },
    "papermill": {
     "duration": 0.016724,
     "end_time": "2025-06-23T14:12:25.850092",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.833368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "\n",
    "def repeat_interleave_batch(x, B, repeat):\n",
    "    N = len(x) // B\n",
    "    x = torch.cat([\n",
    "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
    "        for i in range(N)\n",
    "    ], dim=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03d6ef52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.862070Z",
     "iopub.status.busy": "2025-06-23T14:12:25.861897Z",
     "iopub.status.idle": "2025-06-23T14:12:25.905086Z",
     "shell.execute_reply": "2025-06-23T14:12:25.904591Z"
    },
    "papermill": {
     "duration": 0.050455,
     "end_time": "2025-06-23T14:12:25.906098",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.855643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, timeline_pixel_width=16*300, patch_size=16, in_chans=1, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = timeline_pixel_width // patch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=1, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "class VisionTransformerPredictor(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=6,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        # --\n",
    "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n",
    "                                                requires_grad=False)\n",
    "        predictor_pos_embed = get_1d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n",
    "                                                      int(num_patches),\n",
    "                                                      cls_token=False)\n",
    "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        self.predictor_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
    "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        trunc_normal_(self.mask_token, std=self.init_std)\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks_x, masks):\n",
    "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
    "\n",
    "        if not isinstance(masks_x, list):\n",
    "            masks_x = [masks_x]\n",
    "\n",
    "        if not isinstance(masks, list):\n",
    "            masks = [masks]\n",
    "\n",
    "        # -- Batch Size\n",
    "        B = len(x) // len(masks_x)\n",
    "\n",
    "        # -- map from encoder-dim to pedictor-dim\n",
    "        x = self.predictor_embed(x)\n",
    "\n",
    "        # -- add positional embedding to x tokens\n",
    "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        x += apply_masks(x_pos_embed, masks_x)\n",
    "\n",
    "        _, N_ctxt, D = x.shape\n",
    "\n",
    "        # -- concat mask tokens to x\n",
    "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        pos_embs = apply_masks(pos_embs, masks)\n",
    "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
    "        # --\n",
    "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
    "        # --\n",
    "        pred_tokens += pos_embs\n",
    "        x = x.repeat(len(masks), 1, 1)\n",
    "        x = torch.cat([x, pred_tokens], dim=1)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for blk in self.predictor_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.predictor_norm(x)\n",
    "\n",
    "        # -- return preds for mask tokens\n",
    "        x = x[:, N_ctxt:]\n",
    "        x = self.predictor_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        timeline_pixel_width=(16*300),\n",
    "        patch_size=16,\n",
    "        in_chans=1,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        # --\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            timeline_pixel_width=timeline_pixel_width,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # --\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks=None):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "        x = x + pos_embed\n",
    "\n",
    "        # -- mask x\n",
    "        if masks is not None:\n",
    "            x = apply_masks(x, masks)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "\n",
    "def vit_predictor(**kwargs):\n",
    "    model = VisionTransformerPredictor(\n",
    "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_huge(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_giant(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385797c1",
   "metadata": {
    "papermill": {
     "duration": 0.005544,
     "end_time": "2025-06-23T14:12:25.917268",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.911724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4e46ef6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.930293Z",
     "iopub.status.busy": "2025-06-23T14:12:25.930051Z",
     "iopub.status.idle": "2025-06-23T14:12:25.941320Z",
     "shell.execute_reply": "2025-06-23T14:12:25.940757Z"
    },
    "papermill": {
     "duration": 0.01869,
     "end_time": "2025-06-23T14:12:25.942429",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.923739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    device,\n",
    "    r_path,\n",
    "    encoder,\n",
    "    predictor,\n",
    "    target_encoder,\n",
    "    excluded_layers = None\n",
    "):\n",
    "    epoch = 0\n",
    "    checkpoint = torch.load(r_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    # -- loading encoder with filtering\n",
    "    pretrained_dict = checkpoint['encoder']\n",
    "    \n",
    "    # Remove 'module.' prefix if it exists\n",
    "    new_pretrained_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_pretrained_dict[k[7:]] = v  # Remove 'module.' prefix\n",
    "        else:\n",
    "            new_pretrained_dict[k] = v\n",
    "    pretrained_dict = new_pretrained_dict\n",
    "    \n",
    "    # Apply excluded_layers filtering\n",
    "    if excluded_layers != None:\n",
    "        filtered_dict = {k: v for k, v in pretrained_dict.items() \n",
    "                        if not any(keyword in k for keyword in excluded_layers)}\n",
    "        print(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n",
    "        logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n",
    "        pretrained_dict = filtered_dict\n",
    "    msg = encoder.load_state_dict(pretrained_dict, strict=False)\n",
    "    print(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
    "    logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "    # -- loading predictor\n",
    "    pretrained_dict = checkpoint['predictor']\n",
    "    \n",
    "    # Remove 'module.' prefix if it exists\n",
    "    new_pretrained_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_pretrained_dict[k[7:]] = v  # Remove 'module.' prefix\n",
    "        else:\n",
    "            new_pretrained_dict[k] = v\n",
    "    pretrained_dict = new_pretrained_dict\n",
    "\n",
    "    # Apply excluded_layers filtering\n",
    "    if excluded_layers != None:\n",
    "        filtered_dict = {k: v for k, v in pretrained_dict.items() \n",
    "                        if not any(keyword in k for keyword in excluded_layers)}\n",
    "        print(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n",
    "        logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n",
    "        pretrained_dict = filtered_dict\n",
    "\n",
    "    msg = predictor.load_state_dict(pretrained_dict, strict=False)\n",
    "    logger.info(f'loaded pretrained predictor from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "    # -- loading target_encoder\n",
    "    if target_encoder is not None:\n",
    "        pretrained_dict = checkpoint['target_encoder']\n",
    "        \n",
    "        # Remove 'module.' prefix if it exists\n",
    "        new_pretrained_dict = {}\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if k.startswith('module.'):\n",
    "                new_pretrained_dict[k[7:]] = v  # Remove 'module.' prefix\n",
    "            else:\n",
    "                new_pretrained_dict[k] = v\n",
    "        pretrained_dict = new_pretrained_dict\n",
    "        \n",
    "        if excluded_layers != None:\n",
    "            filtered_dict = {k: v for k, v in pretrained_dict.items() \n",
    "                            if not any(keyword in k for keyword in excluded_layers)}\n",
    "            logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n",
    "            pretrained_dict = filtered_dict\n",
    "\n",
    "        msg = target_encoder.load_state_dict(pretrained_dict, strict=False)\n",
    "        logger.info(f'loaded pretrained target_encoder from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "\n",
    "    return encoder, predictor, target_encoder\n",
    "\n",
    "\n",
    "\n",
    "def init_model(\n",
    "    device,\n",
    "    patch_size=16,\n",
    "    model_name='vit_base',\n",
    "    timeline_pixel_width=16*300,\n",
    "    pred_depth=6,\n",
    "    pred_emb_dim=384\n",
    "):\n",
    "    encoder = vit_small(\n",
    "        timeline_pixel_width=timeline_pixel_width,\n",
    "        patch_size=patch_size)\n",
    "    predictor = vit_predictor(\n",
    "        num_patches=encoder.patch_embed.num_patches,\n",
    "        embed_dim=encoder.embed_dim,\n",
    "        predictor_embed_dim=pred_emb_dim,\n",
    "        depth=pred_depth,\n",
    "        num_heads=encoder.num_heads)\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, torch.nn.LayerNorm):\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "            torch.nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    for m in encoder.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    for m in predictor.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    encoder.to(device)\n",
    "    predictor.to(device)\n",
    "    logger.info(encoder)\n",
    "    return encoder, predictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e1f51",
   "metadata": {
    "papermill": {
     "duration": 0.005105,
     "end_time": "2025-06-23T14:12:25.953000",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.947895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71a32b78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:25.964676Z",
     "iopub.status.busy": "2025-06-23T14:12:25.964074Z",
     "iopub.status.idle": "2025-06-23T14:12:27.320555Z",
     "shell.execute_reply": "2025-06-23T14:12:27.319964Z"
    },
    "papermill": {
     "duration": 1.363534,
     "end_time": "2025-06-23T14:12:27.321868",
     "exception": false,
     "start_time": "2025-06-23T14:12:25.958334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- init model\n",
    "encoder, predictor = init_model(\n",
    "    device=device,\n",
    "    patch_size=patch_size,\n",
    "    timeline_pixel_width=300*16,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim,\n",
    "    model_name=model_name)\n",
    "target_encoder = copy.deepcopy(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87c294e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:27.380370Z",
     "iopub.status.busy": "2025-06-23T14:12:27.379698Z",
     "iopub.status.idle": "2025-06-23T14:12:31.557122Z",
     "shell.execute_reply": "2025-06-23T14:12:31.556310Z"
    },
    "papermill": {
     "duration": 4.185226,
     "end_time": "2025-06-23T14:12:31.558409",
     "exception": false,
     "start_time": "2025-06-23T14:12:27.373183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained encoder from epoch 0 with msg: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# -- load training checkpoint\n",
    "if load_model:\n",
    "    encoder, predictor, target_encoder = load_checkpoint(\n",
    "        device=device,\n",
    "        r_path=r_file,\n",
    "        encoder=encoder,\n",
    "        predictor=predictor,\n",
    "        target_encoder=target_encoder,\n",
    "        excluded_layers = None)\n",
    "    # for _ in range(start_epoch*ipe):\n",
    "    #     scheduler.step()\n",
    "    #     wd_scheduler.step()\n",
    "    #     next(momentum_scheduler)\n",
    "    #     mask_collator.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1bdb88",
   "metadata": {},
   "source": [
    "### Inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3d1059f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:31.571436Z",
     "iopub.status.busy": "2025-06-23T14:12:31.571190Z",
     "iopub.status.idle": "2025-06-23T14:12:31.581529Z",
     "shell.execute_reply": "2025-06-23T14:12:31.580788Z"
    },
    "papermill": {
     "duration": 0.018215,
     "end_time": "2025-06-23T14:12:31.582652",
     "exception": false,
     "start_time": "2025-06-23T14:12:31.564437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    \"\"\"Run validation on the validation dataset and return grouped z tensors.\"\"\"\n",
    "    logger.info('Running validation...')\n",
    "    \n",
    "    # Set models to eval mode\n",
    "    encoder.eval()\n",
    "    predictor.eval()\n",
    "    target_encoder.eval()\n",
    "\n",
    "    num_epochs = 300\n",
    "    group_size = 4\n",
    "    all_zs = [[] for _ in range(num_epochs)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch%10==0:\n",
    "                print(f\"epochs: {epoch+1}/300\")\n",
    "            collate_wrapper.set_frame_idx(epoch)\n",
    "            for itr, (udata, masks_enc, masks_pred) in enumerate(test_loader):\n",
    "                # if itr > 20:\n",
    "                #     break\n",
    "                # Load and process images\n",
    "                imgs = udata.to(device, non_blocking=True)\n",
    "                indices_to_zero = []\n",
    "                for i in range(imgs.shape[0]):\n",
    "                    slice_2d = imgs[i, 0, :, :16]  # First 16x16 slice\n",
    "                    value_sum = slice_2d.sum().item()\n",
    "                    if value_sum < -25:\n",
    "                        indices_to_zero.append(i)\n",
    "                \n",
    "                masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
    "                masks_2 = [u.unsqueeze(-1).to(device, non_blocking=True) for u in masks_pred]\n",
    "                \n",
    "                # Forward pass\n",
    "                def val_step():\n",
    "                    with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
    "                        # # Target encoding\n",
    "                        # h = target_encoder(imgs)\n",
    "                        # h = F.layer_norm(h, (h.size(-1),))\n",
    "                        # B = len(h)\n",
    "                        # h = apply_masks(h, masks_2)\n",
    "                        # h = repeat_interleave_batch(h, B, repeat=len(masks_1))\n",
    "                        \n",
    "                        # Context encoding and prediction\n",
    "                        z = encoder(imgs, masks_1)\n",
    "                        B, T, F = z.shape  # B=4, T=299, F=384\n",
    "                        group_size = 299\n",
    "                        num_groups = T // group_size  # 299 // 13 = 23\n",
    "                        \n",
    "                        # Trim to fit exact number of groups\n",
    "                        z_trimmed = z[:, :num_groups * group_size, :]  # Shape: (4, 299 -> 299, 384) → then trimmed to (4, 299, 384)\n",
    "                        \n",
    "                        # Reshape and average\n",
    "                        z_avg = z_trimmed.view(B, num_groups, group_size, F).mean(dim=2)  # Shape: (4, 23, 384)\n",
    "                        for idx in indices_to_zero:\n",
    "                            # print(f\"{idx} set to zero\")\n",
    "                            z_avg[idx] = 0\n",
    "                        # print(z_avg)\n",
    "                        return z_avg\n",
    "    \n",
    "                z = val_step()\n",
    "                all_zs[epoch].append(z.cpu())\n",
    "\n",
    "    # Stack each epoch's results\n",
    "    epoch_tensors = []\n",
    "    for epoch_zs in all_zs:\n",
    "        z_epoch_tensor = torch.stack(epoch_zs)  # shape (num_batches_epoch, B, ...)\n",
    "        num_batches = z_epoch_tensor.size(0)\n",
    "        num_groups = num_batches // group_size\n",
    "        z_epoch_tensor = z_epoch_tensor[:num_groups * group_size]  # Trim\n",
    "        z_epoch_tensor = z_epoch_tensor.view(num_groups, group_size, *z_epoch_tensor.shape[1:])\n",
    "        epoch_tensors.append(z_epoch_tensor)\n",
    "\n",
    "    # Final tensor shape: (num_epochs=299, num_groups, group_size, B, ...)\n",
    "    z_grouped = torch.stack(epoch_tensors)\n",
    "\n",
    "    logger.info(f\"Validation completed. Grouped tensor shape: {z_grouped.shape}\")\n",
    "    return z_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7837e8dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T14:12:31.594519Z",
     "iopub.status.busy": "2025-06-23T14:12:31.594269Z",
     "iopub.status.idle": "2025-06-23T15:15:21.163674Z",
     "shell.execute_reply": "2025-06-23T15:15:21.162646Z"
    },
    "papermill": {
     "duration": 3769.577396,
     "end_time": "2025-06-23T15:15:21.165573",
     "exception": false,
     "start_time": "2025-06-23T14:12:31.588177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1310537116.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 11/300\n",
      "epochs: 21/300\n",
      "epochs: 31/300\n",
      "epochs: 41/300\n",
      "epochs: 51/300\n",
      "epochs: 61/300\n",
      "epochs: 71/300\n",
      "epochs: 81/300\n",
      "epochs: 91/300\n",
      "epochs: 101/300\n",
      "epochs: 111/300\n",
      "epochs: 121/300\n",
      "epochs: 131/300\n",
      "epochs: 141/300\n",
      "epochs: 151/300\n",
      "epochs: 161/300\n",
      "epochs: 171/300\n",
      "epochs: 181/300\n",
      "epochs: 191/300\n",
      "epochs: 201/300\n",
      "epochs: 211/300\n",
      "epochs: 221/300\n",
      "epochs: 231/300\n",
      "epochs: 241/300\n",
      "epochs: 251/300\n",
      "epochs: 261/300\n",
      "epochs: 271/300\n",
      "epochs: 281/300\n",
      "epochs: 291/300\n"
     ]
    }
   ],
   "source": [
    "features = validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d8c9b8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T15:15:21.181016Z",
     "iopub.status.busy": "2025-06-23T15:15:21.180771Z",
     "iopub.status.idle": "2025-06-23T15:15:21.187053Z",
     "shell.execute_reply": "2025-06-23T15:15:21.186389Z"
    },
    "papermill": {
     "duration": 0.014955,
     "end_time": "2025-06-23T15:15:21.188182",
     "exception": false,
     "start_time": "2025-06-23T15:15:21.173227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 156, 4, 4, 1, 384])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd951476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T15:15:21.202781Z",
     "iopub.status.busy": "2025-06-23T15:15:21.202590Z",
     "iopub.status.idle": "2025-06-23T15:15:21.206100Z",
     "shell.execute_reply": "2025-06-23T15:15:21.205454Z"
    },
    "papermill": {
     "duration": 0.01226,
     "end_time": "2025-06-23T15:15:21.207176",
     "exception": false,
     "start_time": "2025-06-23T15:15:21.194916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = features.squeeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd1b4a81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T15:15:21.221079Z",
     "iopub.status.busy": "2025-06-23T15:15:21.220887Z",
     "iopub.status.idle": "2025-06-23T15:15:21.225081Z",
     "shell.execute_reply": "2025-06-23T15:15:21.224419Z"
    },
    "papermill": {
     "duration": 0.012292,
     "end_time": "2025-06-23T15:15:21.226181",
     "exception": false,
     "start_time": "2025-06-23T15:15:21.213889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 156, 4, 4, 384])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3d3b310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T15:15:21.240491Z",
     "iopub.status.busy": "2025-06-23T15:15:21.240246Z",
     "iopub.status.idle": "2025-06-23T15:15:21.980064Z",
     "shell.execute_reply": "2025-06-23T15:15:21.979497Z"
    },
    "papermill": {
     "duration": 0.748443,
     "end_time": "2025-06-23T15:15:21.981385",
     "exception": false,
     "start_time": "2025-06-23T15:15:21.232942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"temporal_features_1.npy\", features.numpy())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7419718,
     "sourceId": 11813229,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 382326,
     "modelInstanceId": 361276,
     "sourceId": 444964,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3786.79406,
   "end_time": "2025-06-23T15:15:24.113000",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-23T14:12:17.318940",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
