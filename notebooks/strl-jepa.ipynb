{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11813224,"sourceType":"datasetVersion","datasetId":7419713},{"sourceId":11813229,"sourceType":"datasetVersion","datasetId":7419718},{"sourceId":11813237,"sourceType":"datasetVersion","datasetId":7419725},{"sourceId":12128618,"sourceType":"datasetVersion","datasetId":7637421},{"sourceId":432598,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":352670,"modelId":373951},{"sourceId":433754,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":353674,"modelId":374979}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I-JEPA","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# core train\nimport os\nimport copy\nimport sys\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport yaml","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-06-13T15:28:57.873341Z","iopub.execute_input":"2025-06-13T15:28:57.874045Z","iopub.status.idle":"2025-06-13T15:29:03.005679Z","shell.execute_reply.started":"2025-06-13T15:28:57.874005Z","shell.execute_reply":"2025-06-13T15:29:03.004797Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"# Basic global config\n_GLOBAL_SEED = 0\nnp.random.seed(_GLOBAL_SEED)\ntorch.manual_seed(_GLOBAL_SEED)\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.007364Z","iopub.execute_input":"2025-06-13T15:29:03.007714Z","iopub.status.idle":"2025-06-13T15:29:03.017609Z","shell.execute_reply.started":"2025-06-13T15:29:03.007694Z","shell.execute_reply":"2025-06-13T15:29:03.016790Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"args = {\n    \"data\": {\n        \"batch_size\": 1,\n        \"image_folders\": ['/kaggle/input/fmri-train-1-norm-v3/data/noisy_func_train_1.npy',\n                '/kaggle/input/fmri-train-2-norm-v3/data/noisy_func_train_2.npy',\n                '/kaggle/input/fmri-train-3-norm-v3/data/noisy_func_train_3.npy'],\n        \"validation_folders\": ['/kaggle/input/fmri-val-norm-v3/data/noisy_func_val.npy'],\n        \"num_workers\": 2,\n        \"pin_mem\": True,\n        \"root_path\": \"/kaggle/input\",\n        \"use_horizontal_flip\": False\n    },\n    \"logging\": {\n        \"folder\": \"/kaggle/working/logs\",\n        \"write_tag\": \"jepa\"\n    },\n    \"mask\": {\n        \"allow_overlap\": False,\n        \"aspect_ratio\": [0.75, 1.5],\n        \"enc_mask_scale\": [0.85, 1.0],\n        \"min_keep\": 10,\n        \"num_enc_masks\": 1,\n        \"num_pred_masks\": 1,\n        \"patch_size\": 16,\n        \"pred_mask_scale\": [0.15, 0.2]\n    },\n    \"meta\": {\n        \"copy_data\": False,\n        \"load_checkpoint\": True,\n        \"model_name\": \"vit_huge\",\n        \"pred_depth\": 12,\n        \"pred_emb_dim\": 384,\n        \"read_checkpoint\": \"/kaggle/input/jepa-14x14/pytorch/default/1/IN1K-vit.h.14-300e.pth.tar\",\n        \"use_bfloat16\": True\n    },\n    \"optimization\": {\n        \"ema\": [0.996, 1.0],\n        \"epochs\": 2,\n        \"final_lr\": 1.0e-5,\n        \"final_weight_decay\": 0.4,\n        \"ipe_scale\": 1.0,\n        \"lr\": 0.001,\n        \"start_lr\": 0.0002,\n        \"warmup\": 20,\n        \"weight_decay\": 0.04\n    }\n}\n","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.018527Z","iopub.execute_input":"2025-06-13T15:29:03.018782Z","iopub.status.idle":"2025-06-13T15:29:03.040292Z","shell.execute_reply.started":"2025-06-13T15:29:03.018757Z","shell.execute_reply":"2025-06-13T15:29:03.039246Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"resume_preempt = False\nrank = 0\n\n# -- META\nuse_bfloat16 = args['meta']['use_bfloat16']\nmodel_name = args['meta']['model_name']\nload_model = args['meta']['load_checkpoint'] or resume_preempt\nr_file = args['meta']['read_checkpoint']\ncopy_data = args['meta']['copy_data']\npred_depth = args['meta']['pred_depth']\npred_emb_dim = args['meta']['pred_emb_dim']\nif not torch.cuda.is_available():\n    device = torch.device('cpu')\nelse:\n    device = torch.device('cuda:0')\n    torch.cuda.set_device(device)\n\n# -- DATA\nbatch_size = args['data']['batch_size']\npin_mem = args['data']['pin_mem']\nnum_workers = args['data']['num_workers']\nroot_path = args['data']['root_path']\nimage_folders = args['data']['image_folders']\nvalidation_folders = args['data']['validation_folders']\n# --\n\n# -- MASK\nallow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\npatch_size = args['mask']['patch_size']  # patch-size for model training\nnum_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\nmin_keep = args['mask']['min_keep']  # min number of patches in context block\nenc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\nnum_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\npred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\naspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n# --\n\n# -- OPTIMIZATION\nema = args['optimization']['ema']\nipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\nwd = float(args['optimization']['weight_decay'])\nfinal_wd = float(args['optimization']['final_weight_decay'])\nnum_epochs = args['optimization']['epochs']\nwarmup = args['optimization']['warmup']\nstart_lr = args['optimization']['start_lr']\nlr = args['optimization']['lr']\nfinal_lr = args['optimization']['final_lr']\n\n# -- LOGGING\nfolder = args['logging']['folder']\ntag = args['logging']['write_tag']\n\nos.makedirs(folder, exist_ok=True)\ndump = os.path.join(folder, 'params-ijepa.yaml')\nwith open(dump, 'w') as f:\n    yaml.dump(args, f)","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.042651Z","iopub.execute_input":"2025-06-13T15:29:03.042949Z","iopub.status.idle":"2025-06-13T15:29:03.077837Z","shell.execute_reply.started":"2025-06-13T15:29:03.042929Z","shell.execute_reply":"2025-06-13T15:29:03.076813Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Logging","metadata":{}},{"cell_type":"code","source":"import logging\nlog_timings = True\nlog_freq = 10\ncheckpoint_freq = 1\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogger = logging.getLogger()","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.078884Z","iopub.execute_input":"2025-06-13T15:29:03.079200Z","iopub.status.idle":"2025-06-13T15:29:03.089966Z","shell.execute_reply.started":"2025-06-13T15:29:03.079171Z","shell.execute_reply":"2025-06-13T15:29:03.088935Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def gpu_timer(closure, log_timings=True):\n    \"\"\" Helper to time gpu-time to execute closure() \"\"\"\n    log_timings = log_timings and torch.cuda.is_available()\n\n    elapsed_time = -1.\n    if log_timings:\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record()\n\n    result = closure()\n\n    if log_timings:\n        end.record()\n        torch.cuda.synchronize()\n        elapsed_time = start.elapsed_time(end)\n\n    return result, elapsed_time","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.091186Z","iopub.execute_input":"2025-06-13T15:29:03.091534Z","iopub.status.idle":"2025-06-13T15:29:03.112541Z","shell.execute_reply.started":"2025-06-13T15:29:03.091508Z","shell.execute_reply":"2025-06-13T15:29:03.111578Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class CSVLogger(object):\n\n    def __init__(self, fname, *argv):\n        self.fname = fname\n        self.types = []\n        # -- print headers\n        with open(self.fname, '+a') as f:\n            for i, v in enumerate(argv, 1):\n                self.types.append(v[0])\n                if i < len(argv):\n                    print(v[1], end=',', file=f)\n                else:\n                    print(v[1], end='\\n', file=f)\n\n    def log(self, *argv):\n        with open(self.fname, '+a') as f:\n            for i, tv in enumerate(zip(self.types, argv), 1):\n                end = ',' if i < len(argv) else '\\n'\n                print(tv[0] % tv[1], end=end, file=f)\n\n\nclass AverageMeter(object):\n    \"\"\"computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.max = float('-inf')\n        self.min = float('inf')\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        try:\n            self.max = max(val, self.max)\n            self.min = min(val, self.min)\n        except Exception:\n            pass\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.113561Z","iopub.execute_input":"2025-06-13T15:29:03.113895Z","iopub.status.idle":"2025-06-13T15:29:03.138591Z","shell.execute_reply.started":"2025-06-13T15:29:03.113865Z","shell.execute_reply":"2025-06-13T15:29:03.137518Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# -- log/checkpointing paths\nlog_file = os.path.join(folder, f'{tag}_r{rank}.csv')\nsave_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\nlatest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\nload_path = None\nif load_model:\n    load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n\n# csv logger\ncsv_logger = CSVLogger(log_file,\n                       ('%d', 'epoch'),\n                       ('%d', 'itr'),\n                       ('%.5f', 'train_loss'),\n                       ('%.5f', 'val_loss'),  # Added validation loss\n                       ('%.5f', 'mask-A'),\n                       ('%.5f', 'mask-B'),\n                       ('%d', 'time (ms)'))","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.139794Z","iopub.execute_input":"2025-06-13T15:29:03.140811Z","iopub.status.idle":"2025-06-13T15:29:03.159070Z","shell.execute_reply.started":"2025-06-13T15:29:03.140780Z","shell.execute_reply":"2025-06-13T15:29:03.157950Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Dataset creation and preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Data transformation","metadata":{}},{"cell_type":"markdown","source":"#### Masking","metadata":{}},{"cell_type":"code","source":"import math\nfrom multiprocessing import Value","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.160170Z","iopub.execute_input":"2025-06-13T15:29:03.160420Z","iopub.status.idle":"2025-06-13T15:29:03.180430Z","shell.execute_reply.started":"2025-06-13T15:29:03.160401Z","shell.execute_reply":"2025-06-13T15:29:03.179523Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class TimeSeriesMaskCollator:\n    def __init__(self, num_frames=300, frame_size=16, nenc=1, npred=1):\n        self.num_frames = num_frames\n        self.frame_size = frame_size\n        self.height = frame_size  # fixed spatial height of a frame\n        self.width = num_frames * frame_size  # time flows horizontally\n        self.nenc = nenc\n        self.npred = npred\n        self._itr_counter = Value('i', -1)\n\n    def step(self):\n        i = self._itr_counter\n        with i.get_lock():\n            i.value += 1\n            v = i.value\n        return v\n    \n    def collate_merge_batches(self, batch):\n        merged = torch.cat([item for item in batch], dim=0)\n        return merged\n\n    def _sample_frame_mask(self, generator, exclude_frames=None):\n        # Build list of available frame indices\n        choices = torch.tensor(\n            [i for i in range(self.num_frames) if (exclude_frames is None or i not in exclude_frames)],\n            dtype=torch.long\n        )\n        # Sample one index using the generator\n        idx = torch.randint(0, len(choices), (1,), generator=generator).item()\n        frame_idx = choices[idx]\n\n        # Calculate top-left corner in new layout (horizontal stacking)\n        top = 0\n        left = frame_idx * self.frame_size\n        mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n        mask[top:top+self.frame_size, left:left+self.frame_size] = 1\n        return mask, frame_idx\n\n    def build_encoder_mask_from_pred(self, pred_masks):\n        enc_mask = torch.ones((self.height, self.width), dtype=torch.int32)\n        for pred_mask in pred_masks:\n            enc_mask.view(-1)[pred_mask] = 0  # Zero out the masked regions\n        return torch.nonzero(enc_mask.flatten()).squeeze()\n\n    def __call__(self, batch):\n        collated_batch = self.collate_merge_batches(batch)\n        B = collated_batch.shape[0]\n\n        seed = self.step()\n        g = torch.Generator()\n        g.manual_seed(seed)\n\n        collated_masks_enc, collated_masks_pred = [], []\n\n        for _ in range(B):\n            pred_masks = []\n            pred_frame_idxs = []\n            for _ in range(self.npred):\n                mask, idx = self._sample_frame_mask(generator=g, exclude_frames=pred_frame_idxs)\n                pred_masks.append(torch.nonzero(mask.flatten()).squeeze())\n                pred_frame_idxs.append(idx)\n\n            collated_masks_pred.append(pred_masks)\n\n            enc_masks = []\n            for _ in range(self.nenc):\n                enc_mask = self.build_encoder_mask_from_pred(pred_masks)\n                enc_masks.append(enc_mask)\n\n            collated_masks_enc.append(enc_masks)\n\n        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n\n        return collated_batch, collated_masks_enc, collated_masks_pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:03.183710Z","iopub.execute_input":"2025-06-13T15:29:03.184025Z","iopub.status.idle":"2025-06-13T15:29:03.200180Z","shell.execute_reply.started":"2025-06-13T15:29:03.183996Z","shell.execute_reply":"2025-06-13T15:29:03.199126Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def apply_masks(x, masks):\n    \"\"\"\n    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n    :param masks: list of tensors containing indices of patches in [N] to keep\n    \"\"\"\n    all_x = []\n    for m in masks:\n        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n    return torch.cat(all_x, dim=0)","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.200927Z","iopub.execute_input":"2025-06-13T15:29:03.201248Z","iopub.status.idle":"2025-06-13T15:29:03.223218Z","shell.execute_reply.started":"2025-06-13T15:29:03.201221Z","shell.execute_reply":"2025-06-13T15:29:03.222297Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"mask_collator = TimeSeriesMaskCollator() # defaults to 300 frames of size 16x16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:03.224267Z","iopub.execute_input":"2025-06-13T15:29:03.224641Z","iopub.status.idle":"2025-06-13T15:29:03.245589Z","shell.execute_reply.started":"2025-06-13T15:29:03.224616Z","shell.execute_reply":"2025-06-13T15:29:03.244757Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"#### Image transforms","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch # Import PyTorch\n\nclass Compose:\n    \"\"\"\n    Composes several transforms together.\n    Args:\n        transforms (list of callables): list of transforms to compose.\n    \"\"\"\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img_array):\n        \"\"\"\n        Applies the composed transforms to the input array.\n        The input can be a NumPy array. The output will be a torch.Tensor\n        if torch.from_numpy is the last transform.\n        Args:\n            img_array (numpy.ndarray): Input image array (C, H, W).\n        Returns:\n            torch.Tensor: Transformed image tensor.\n        \"\"\"\n        for t in self.transforms:\n            img_array = t(img_array) # Note: img_array will become a torch.Tensor at the end\n        return img_array\n\n\n# Build the transform list\ntransform_list = []\n\ndef reshape_to_2d_timeline(patch_time_series):\n    # Convert to PyTorch tensor immediately for permute/reshape\n    patch_time_series_tensor = torch.from_numpy(patch_time_series).float()\n    height_patch = patch_time_series_tensor.shape[0]  # H\n    width_patch = patch_time_series_tensor.shape[1]   # W\n    total_time_steps = patch_time_series_tensor.shape[2]  # T   \n\n    print(patch_time_series_tensor.shape)\n\n    \n    # Apply the corrected reshaping logic here\n    # Original tensor shape (H, W, T) -> (16, 16, total_time_steps)\n    # Permute to (H, T, W)\n    intermediate_tensor = patch_time_series_tensor.permute(0, 2, 1)\n    \n    sample = intermediate_tensor.reshape(height_patch, total_time_steps * width_patch)\n\n    # Add a channel dimension: (1, H, T*W)\n    sample = sample.unsqueeze(0)\n    # return sample\n    return patch_time_series_tensor\n\ntransform_list += [reshape_to_2d_timeline]  # Add reshape transform\ntransform_list += [lambda x: x.unsqueeze(0)] # add batch dimension\n\n# Composition of transforms\ntransform = Compose(transform_list)","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.246633Z","iopub.execute_input":"2025-06-13T15:29:03.247486Z","iopub.status.idle":"2025-06-13T15:29:03.258964Z","shell.execute_reply.started":"2025-06-13T15:29:03.247451Z","shell.execute_reply":"2025-06-13T15:29:03.258122Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Dataset & Dataloader","metadata":{}},{"cell_type":"code","source":"import random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:03.259813Z","iopub.execute_input":"2025-06-13T15:29:03.260070Z","iopub.status.idle":"2025-06-13T15:29:03.279057Z","shell.execute_reply.started":"2025-06-13T15:29:03.260044Z","shell.execute_reply":"2025-06-13T15:29:03.278118Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class NoisyDataset(torch.utils.data.Dataset):\n    def __init__(self, noisy_images_paths: list, stats, transform=None, normalize=True):\n        \"\"\"Initialize fMRI dataset for denoising with memory-efficient loading,\n        extracting 16x16 patches with full time series per depth channel.\n\n        Args:\n            noisy_images_paths (list): List of paths to noisy fMRI volumes (.npy files)\n            stats: Dictionary containing 'mean' and 'std' for normalization.\n            transform (callable, optional): Optional transform to be applied on a sample.\n            normalize (bool): Whether to apply channel-wise normalization.\n        \"\"\"\n        self.noisy_paths = noisy_images_paths\n        self.transform = transform\n        self.normalize = normalize\n        self.stats = stats\n\n        self.file_slice_mapping = [] # Stores (file_idx, z_idx, patch_y_idx, patch_x_idx)\n\n        patch_size = 16\n        dataset_length = 0\n\n        for i, path in enumerate(noisy_images_paths):\n            # Load metadata about the file shape without loading full content\n            data_shape = np.load(path, mmap_mode='r').shape\n            # Assuming data_shape is (H, W, Z, T)\n            total_height, total_width, depth_channels, total_time_steps = data_shape\n\n            # Calculate how many patches fit along each spatial dimension\n            num_patches_width = total_width // patch_size\n            num_patches_height = total_height // patch_size\n\n            # Iterate over all depth channels (Z)\n            for z_idx in range(depth_channels):\n                # Iterate through the spatial grid of patches\n                for patch_y_idx in range(num_patches_height):\n                    for patch_x_idx in range(num_patches_width):\n                        # Each combination of (file, z_idx, patch_y_idx, patch_x_idx) is a unique item\n                        self.file_slice_mapping.append((i, z_idx, patch_y_idx, patch_x_idx))\n                        dataset_length += 1\n\n        self.data_len = dataset_length\n        self.patch_size = patch_size # Store patch_size for __getitem__\n\n    def __len__(self):\n        return self.data_len\n\n    def __getitem__(self, index):\n\n        valid_patch = False\n        while valid_patch == False:\n            # Use the mapping to determine which file, depth channel, and spatial patch to load\n            file_idx, z_idx, patch_y_idx, patch_x_idx = self.file_slice_mapping[index]\n    \n            # Load data from the specific file\n            noisy_file_path = self.noisy_paths[file_idx]\n    \n            # Load the full 4D array with mmap_mode to avoid loading everything into RAM\n            noisy_volume = np.load(noisy_file_path, mmap_mode='r')\n    \n            # Calculate the starting and ending coordinates for the current patch\n            start_h = patch_y_idx * self.patch_size\n            end_h = start_h + self.patch_size\n            start_w = patch_x_idx * self.patch_size\n            end_w = start_w + self.patch_size\n    \n            # Extract the 16x16 patch for the specific depth channel (z_idx)\n            # and include the entire time series.\n            # The resulting shape will be (patch_size, patch_size, time_steps)\n            patch_time_series = noisy_volume[start_h:end_h, start_w:end_w, z_idx, :].copy()\n\n            patch = patch_time_series[:,:,0]\n            print(patch.shape)\n            patch_sum = np.sum(patch)\n            if patch_sum >= -25:\n                valid_patch = True\n            else:\n                index = random.randint(0, self.data_len)  # includes both 1 and 10\n            \n\n        # Apply transformations if specified\n        if self.transform is not None:\n            # Your transform should expect a tensor of shape (C, H, W, T) or (H, W, T)\n            # depending on how you structure it. Adjust accordingly.\n            sample = self.transform(patch_time_series)\n\n        # Apply normalization if specified (after other transforms but before returning)\n        if self.normalize:\n            # Ensure sample is a tensor and normalize channel-wise\n            # Assuming sample shape is (C, H, W, T) where C=1 here\n            sample = self.normalize_sample(sample)\n\n        return sample\n\n    def normalize_sample(self, sample):\n        \"\"\"Apply channel-wise normalization to the sample.\"\"\"\n        # Ensure the mean and std are on the same device as the sample\n        # Assuming self.stats['mean'] and self.stats['std'] are 1D tensors\n        mean = self.stats['mean'].to(sample.device)\n        std = self.stats['std'].to(sample.device)\n\n        mean = mean.view(1, -1, 1, 1)\n        std = std.view(1, -1, 1, 1)\n\n        # Normalize\n        return (sample - mean) / (std + 1e-8) # Add small epsilon for numerical stability","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:03.280046Z","iopub.execute_input":"2025-06-13T15:29:03.280324Z","iopub.status.idle":"2025-06-13T15:29:03.299915Z","shell.execute_reply.started":"2025-06-13T15:29:03.280296Z","shell.execute_reply":"2025-06-13T15:29:03.298640Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"stats = {\"mean\": torch.tensor([18598.638916899643]), \n         \"std\": torch.tensor([31381.96243127712])}","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.301169Z","iopub.execute_input":"2025-06-13T15:29:03.301612Z","iopub.status.idle":"2025-06-13T15:29:03.334602Z","shell.execute_reply.started":"2025-06-13T15:29:03.301580Z","shell.execute_reply":"2025-06-13T15:29:03.333545Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# dataset\ndataset = NoisyDataset(noisy_images_paths=image_folders, stats = stats, transform=transform, normalize=True)\n\n# Save the stats for future use\ntorch.save(stats, os.path.join(folder, 'dataset_stats.pth'))\n# logger.info('Initial Dataset Finished')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:03.335688Z","iopub.execute_input":"2025-06-13T15:29:03.336007Z","iopub.status.idle":"2025-06-13T15:29:03.454822Z","shell.execute_reply.started":"2025-06-13T15:29:03.335980Z","shell.execute_reply":"2025-06-13T15:29:03.453917Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"val_dataset = NoisyDataset(noisy_images_paths=validation_folders, stats = stats, transform=transform, normalize=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:03.455869Z","iopub.execute_input":"2025-06-13T15:29:03.456186Z","iopub.status.idle":"2025-06-13T15:29:03.500624Z","shell.execute_reply.started":"2025-06-13T15:29:03.456158Z","shell.execute_reply":"2025-06-13T15:29:03.499681Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"dataset[100].shape","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:03.501838Z","iopub.execute_input":"2025-06-13T15:29:03.502415Z","iopub.status.idle":"2025-06-13T15:29:07.261739Z","shell.execute_reply.started":"2025-06-13T15:29:03.502382Z","shell.execute_reply":"2025-06-13T15:29:07.260786Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(16, 16)\n(16, 16)\n(16, 16)\n(16, 16)\ntorch.Size([16, 16, 300])\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 16, 16, 300])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"dataset[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:07.262755Z","iopub.execute_input":"2025-06-13T15:29:07.263004Z","iopub.status.idle":"2025-06-13T15:29:12.290001Z","shell.execute_reply.started":"2025-06-13T15:29:07.262982Z","shell.execute_reply":"2025-06-13T15:29:12.289020Z"}},"outputs":[{"name":"stdout","text":"(16, 16)\n(16, 16)\n(16, 16)\n(16, 16)\n(16, 16)\ntorch.Size([16, 16, 300])\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"tensor([[[[-0.5925, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          ...,\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926]],\n\n         [[-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          ...,\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926]],\n\n         [[-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          ...,\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926]],\n\n         ...,\n\n         [[-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          ...,\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926]],\n\n         [[-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          ...,\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926]],\n\n         [[-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          ...,\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926],\n          [-0.5926, -0.5926, -0.5926,  ..., -0.5926, -0.5926, -0.5926]]]])"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"np.random.randn(64, 64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:12.291115Z","iopub.execute_input":"2025-06-13T15:29:12.291417Z","iopub.status.idle":"2025-06-13T15:29:12.299692Z","shell.execute_reply.started":"2025-06-13T15:29:12.291393Z","shell.execute_reply":"2025-06-13T15:29:12.298784Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"array([[ 1.76405235,  0.40015721,  0.97873798, ..., -0.35955316,\n        -0.81314628, -1.7262826 ],\n       [ 0.17742614, -0.40178094, -1.63019835, ..., -0.14963454,\n        -0.43515355,  1.84926373],\n       [ 0.67229476,  0.40746184, -0.76991607, ...,  1.5430146 ,\n        -1.29285691,  0.26705087],\n       ...,\n       [-0.03013331, -0.06472838,  0.72474915, ...,  0.6220752 ,\n         0.86097327, -1.69099776],\n       [-0.69043143,  1.42889242,  1.00610171, ..., -0.8586684 ,\n         0.4361871 ,  1.57146305],\n       [ 1.07731488,  0.81108968, -2.23153764, ...,  0.46387277,\n         0.61766085,  2.49641706]])"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# import numpy as np\n# import matplotlib.pyplot as plt\n\n# # Create a 64x64 tensor with random values (can be negative)\n# tensor = dataset[900]\n# tensor = tensor.detach().cpu().numpy()\n\n# # Output tensor: initialized with NaNs for discarded patches (visually blank)\n# visualized = np.full_like(tensor, np.nan)\n\n# patch_size = 16\n# for i in range(0, 64, patch_size):\n#     for j in range(0, 64, patch_size):\n#         patch = tensor[i:i+patch_size, j:j+patch_size]\n#         patch_sum = np.sum(patch)\n\n#         if patch_sum >= -25:\n#             visualized[i:i+patch_size, j:j+patch_size] = patch\n#         # else: keep as NaN for blank display\n\n# # Display: use masked array to make NaNs invisible in the colormap\n# masked = np.ma.masked_invalid(visualized)\n\n# plt.figure(figsize=(6, 6))\n# plt.imshow(masked, cmap='gray')\n# plt.title(\"Visible Patches (sum >= 0)\")\n# plt.axis('off')\n# plt.colorbar(label='Value Intensity')\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:12.300902Z","iopub.execute_input":"2025-06-13T15:29:12.301236Z","iopub.status.idle":"2025-06-13T15:29:12.318506Z","shell.execute_reply.started":"2025-06-13T15:29:12.301213Z","shell.execute_reply":"2025-06-13T15:29:12.317511Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"dataset[1][:][:][:].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:12.319503Z","iopub.execute_input":"2025-06-13T15:29:12.319791Z","iopub.status.idle":"2025-06-13T15:29:14.495984Z","shell.execute_reply.started":"2025-06-13T15:29:12.319763Z","shell.execute_reply":"2025-06-13T15:29:14.494928Z"}},"outputs":[{"name":"stdout","text":"(16, 16)\n(16, 16)\n(16, 16)\ntorch.Size([16, 16, 300])\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 16, 16, 300])"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"dataset[0][0].permute(2,0,1)[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:14.496989Z","iopub.execute_input":"2025-06-13T15:29:14.497264Z","iopub.status.idle":"2025-06-13T15:29:18.320622Z","shell.execute_reply.started":"2025-06-13T15:29:14.497229Z","shell.execute_reply":"2025-06-13T15:29:18.319588Z"}},"outputs":[{"name":"stdout","text":"(16, 16)\n(16, 16)\n(16, 16)\n(16, 16)\ntorch.Size([16, 16, 300])\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 16])"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# import torch\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# # Create a dummy tensor of the specified size for demonstration.\n# # In your actual code, this would be your loaded image tensor.\n# # Using torch.rand to get values between 0 and 1, suitable for image display.\n# image_tensor = dataset[1000]# dataset[1][0][0]\n\n# # Convert the PyTorch tensor to a NumPy array.\n# # Use .detach().cpu().numpy() if your tensor is on a GPU or requires gradients.\n# image_np = image_tensor.detach().cpu().numpy()\n\n# plt.figure(figsize=(4, 4)) # Adjust figure size for a wide, short image\n\n# # Display the image\n# # 'cmap=\"gray\"' or 'cmap=\"Greys_r\"' is standard for grayscale images.\n# # 'aspect=\"auto\"' allows the image to stretch to fill the figure, which is helpful\n# # for unusual aspect ratios like 16x4800.\n# plt.imshow(image_np, cmap='gray', aspect='auto') \n\n# plt.title('Visualization of a 16x4800 Grayscale Image')\n# plt.xlabel('Width (pixels)')\n# plt.ylabel('Height (pixels)')\n# plt.colorbar(label='Pixel Intensity') # Add a colorbar to show intensity mapping\n# plt.savefig(\"fig_new.png\")\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:18.321642Z","iopub.execute_input":"2025-06-13T15:29:18.321905Z","iopub.status.idle":"2025-06-13T15:29:18.327345Z","shell.execute_reply.started":"2025-06-13T15:29:18.321886Z","shell.execute_reply":"2025-06-13T15:29:18.326399Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# import torch\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# # Create a dummy tensor of the specified size for demonstration.\n# # In your actual code, this would be your loaded image tensor.\n# # Using torch.rand to get values between 0 and 1, suitable for image display.\n# image_tensor = dataset[1000][32:48,32:48]# dataset[1][0][0]\n\n# # Convert the PyTorch tensor to a NumPy array.\n# # Use .detach().cpu().numpy() if your tensor is on a GPU or requires gradients.\n# image_np = image_tensor.detach().cpu().numpy()\n\n# plt.figure(figsize=(5, 5)) # Adjust figure size for a wide, short image\n\n# # Display the image\n# # 'cmap=\"gray\"' or 'cmap=\"Greys_r\"' is standard for grayscale images.\n# # 'aspect=\"auto\"' allows the image to stretch to fill the figure, which is helpful\n# # for unusual aspect ratios like 16x4800.\n# plt.imshow(image_np, cmap='gray', aspect='auto') \n\n# plt.title('Visualization of a 16x4800 Grayscale Image')\n# plt.xlabel('Width (pixels)')\n# plt.ylabel('Height (pixels)')\n# plt.colorbar(label='Pixel Intensity') # Add a colorbar to show intensity mapping\n# plt.savefig(\"fig_new.png\")\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:18.328256Z","iopub.execute_input":"2025-06-13T15:29:18.328523Z","iopub.status.idle":"2025-06-13T15:29:18.352697Z","shell.execute_reply.started":"2025-06-13T15:29:18.328496Z","shell.execute_reply":"2025-06-13T15:29:18.351778Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"data_loader = torch.utils.data.DataLoader(\n    dataset,\n    collate_fn=mask_collator,\n    batch_size=batch_size,\n    drop_last=True,\n    pin_memory=pin_mem,\n    num_workers=num_workers,\n    persistent_workers=False)\n\nipe = len(data_loader)","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:18.353608Z","iopub.execute_input":"2025-06-13T15:29:18.353869Z","iopub.status.idle":"2025-06-13T15:29:18.379470Z","shell.execute_reply.started":"2025-06-13T15:29:18.353848Z","shell.execute_reply":"2025-06-13T15:29:18.378302Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"val_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    collate_fn=mask_collator,\n    batch_size=batch_size,\n    drop_last=True,\n    pin_memory=pin_mem,\n    num_workers=num_workers,\n    persistent_workers=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:29:18.384826Z","iopub.execute_input":"2025-06-13T15:29:18.385105Z","iopub.status.idle":"2025-06-13T15:29:18.398216Z","shell.execute_reply.started":"2025-06-13T15:29:18.385086Z","shell.execute_reply":"2025-06-13T15:29:18.397271Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"import math\nfrom functools import partial\nimport numpy as np\nimport torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:18.399188Z","iopub.execute_input":"2025-06-13T15:29:18.399510Z","iopub.status.idle":"2025-06-13T15:29:18.417041Z","shell.execute_reply.started":"2025-06-13T15:29:18.399485Z","shell.execute_reply":"2025-06-13T15:29:18.416089Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"### Model backbone (vision transformer)","metadata":{}},{"cell_type":"code","source":"def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    with torch.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.erfinv_()\n\n        # Transform to proper mean, std\n        tensor.mul_(std * math.sqrt(2.))\n        tensor.add_(mean)\n\n        # Clamp to ensure it's in the proper range\n        tensor.clamp_(min=a, max=b)\n        return tensor\n\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n\n\ndef repeat_interleave_batch(x, B, repeat):\n    N = len(x) // B\n    x = torch.cat([\n        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n        for i in range(N)\n    ], dim=0)\n    return x","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:18.418009Z","iopub.execute_input":"2025-06-13T15:29:18.418271Z","iopub.status.idle":"2025-06-13T15:29:18.434180Z","shell.execute_reply.started":"2025-06-13T15:29:18.418244Z","shell.execute_reply":"2025-06-13T15:29:18.433159Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=float)\n    grid_w = np.arange(grid_size, dtype=float)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n\n\ndef get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    assert embed_dim % 2 == 0\n\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n    return emb\n\n\ndef get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid length\n    return:\n    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid = np.arange(grid_size, dtype=float)\n    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n\n\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=float)\n    omega /= embed_dim / 2.\n    omega = 1. / 10000**omega   # (D/2,)\n\n    pos = pos.reshape(-1)   # (M,)\n    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False):\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x, attn\n\n\nclass Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x, return_attention=False):\n        y, attn = self.attn(self.norm1(x))\n        if return_attention:\n            return attn\n        x = x + self.drop_path(y)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, timeline_pixel_width=16*300, patch_size=16, in_chans=1, embed_dim=768):\n        super().__init__()\n        num_patches = timeline_pixel_width // patch_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass ConvEmbed(nn.Module):\n    \"\"\"\n    3x3 Convolution stems for ViT following ViTC models\n    \"\"\"\n\n    def __init__(self, channels, strides, img_size=224, in_chans=1, batch_norm=True):\n        super().__init__()\n        # Build the stems\n        stem = []\n        channels = [in_chans] + channels\n        for i in range(len(channels) - 2):\n            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n                               stride=strides[i], padding=1, bias=(not batch_norm))]\n            if batch_norm:\n                stem += [nn.BatchNorm2d(channels[i+1])]\n            stem += [nn.ReLU(inplace=True)]\n        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n        self.stem = nn.Sequential(*stem)\n\n        # Comptute the number of patches\n        stride_prod = int(np.prod(strides))\n        self.num_patches = (img_size[0] // stride_prod)**2\n\n    def forward(self, x):\n        p = self.stem(x)\n        return p.flatten(2).transpose(1, 2)\n\n\nclass VisionTransformerPredictor(nn.Module):\n    \"\"\" Vision Transformer \"\"\"\n    def __init__(\n        self,\n        num_patches,\n        embed_dim=768,\n        predictor_embed_dim=384,\n        depth=6,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_std=0.02,\n        **kwargs\n    ):\n        super().__init__()\n        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        # --\n        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n                                                requires_grad=False)\n        predictor_pos_embed = get_1d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n                                                      int(num_patches),\n                                                      cls_token=False)\n        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n        # --\n        self.predictor_blocks = nn.ModuleList([\n            Block(\n                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n        self.predictor_norm = norm_layer(predictor_embed_dim)\n        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n        # ------\n        self.init_std = init_std\n        trunc_normal_(self.mask_token, std=self.init_std)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.predictor_blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=self.init_std)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            trunc_normal_(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x, masks_x, masks):\n        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n\n        if not isinstance(masks_x, list):\n            masks_x = [masks_x]\n\n        if not isinstance(masks, list):\n            masks = [masks]\n\n        # -- Batch Size\n        B = len(x) // len(masks_x)\n\n        # -- map from encoder-dim to pedictor-dim\n        x = self.predictor_embed(x)\n\n        # -- add positional embedding to x tokens\n        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n        x += apply_masks(x_pos_embed, masks_x)\n\n        _, N_ctxt, D = x.shape\n\n        # -- concat mask tokens to x\n        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n        pos_embs = apply_masks(pos_embs, masks)\n        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n        # --\n        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n        # --\n        pred_tokens += pos_embs\n        x = x.repeat(len(masks), 1, 1)\n        x = torch.cat([x, pred_tokens], dim=1)\n\n        # -- fwd prop\n        for blk in self.predictor_blocks:\n            x = blk(x)\n        x = self.predictor_norm(x)\n\n        # -- return preds for mask tokens\n        x = x[:, N_ctxt:]\n        x = self.predictor_proj(x)\n\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer \"\"\"\n    def __init__(\n        self,\n        timeline_pixel_width=(16*300),\n        patch_size=16,\n        in_chans=1,\n        embed_dim=768,\n        predictor_embed_dim=384,\n        depth=12,\n        predictor_depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_std=0.02,\n        **kwargs\n    ):\n        super().__init__()\n        self.num_features = self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        # --\n        self.patch_embed = PatchEmbed(\n            timeline_pixel_width=timeline_pixel_width,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n        # --\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1],\n                                            int(self.patch_embed.num_patches),\n                                            cls_token=False)\n        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n        # --\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n        # ------\n        self.init_std = init_std\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=self.init_std)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            trunc_normal_(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x, masks=None):\n        if masks is not None:\n            if not isinstance(masks, list):\n                masks = [masks]\n\n        # -- patchify x\n        x = self.patch_embed(x)\n        B, N, D = x.shape\n\n        # -- add positional embedding to x\n        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n        x = x + pos_embed\n\n        # -- mask x\n        if masks is not None:\n            x = apply_masks(x, masks)\n\n        # -- fwd prop\n        for i, blk in enumerate(self.blocks):\n            x = blk(x)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        return x\n\n    def interpolate_pos_encoding(self, x, pos_embed):\n        npatch = x.shape[1] - 1\n        N = pos_embed.shape[1] - 1\n        if npatch == N:\n            return pos_embed\n        class_emb = pos_embed[:, 0]\n        pos_embed = pos_embed[:, 1:]\n        dim = x.shape[-1]\n        pos_embed = nn.functional.interpolate(\n            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n            scale_factor=math.sqrt(npatch / N),\n            mode='bicubic',\n        )\n        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n\n\ndef vit_predictor(**kwargs):\n    model = VisionTransformerPredictor(\n        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    return model\n\n\ndef vit_tiny(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_small(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_base(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_large(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_huge(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_giant(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\nVIT_EMBED_DIMS = {\n    'vit_tiny': 192,\n    'vit_small': 384,\n    'vit_base': 768,\n    'vit_large': 1024,\n    'vit_huge': 1280,\n    'vit_giant': 1408,\n}","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:29:18.435801Z","iopub.execute_input":"2025-06-13T15:29:18.436204Z","iopub.status.idle":"2025-06-13T15:29:18.493585Z","shell.execute_reply.started":"2025-06-13T15:29:18.436179Z","shell.execute_reply":"2025-06-13T15:29:18.492577Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"### Helper functions","metadata":{}},{"cell_type":"code","source":"def load_checkpoint(\n    device,\n    r_path,\n    encoder,\n    predictor,\n    target_encoder,\n    opt,\n    scaler,\n    excluded_layers = None\n):\n\n    checkpoint = torch.load(r_path, map_location=torch.device('cpu'))\n    epoch = checkpoint['epoch']\n\n    # -- loading encoder with filtering\n    pretrained_dict = checkpoint['encoder']\n    \n    # Remove 'module.' prefix if it exists\n    new_pretrained_dict = {}\n    for k, v in pretrained_dict.items():\n        if k.startswith('module.'):\n            new_pretrained_dict[k[7:]] = v  # Remove 'module.' prefix\n        else:\n            new_pretrained_dict[k] = v\n    pretrained_dict = new_pretrained_dict\n    \n    # Apply excluded_layers filtering\n    if excluded_layers != None:\n        filtered_dict = {k: v for k, v in pretrained_dict.items() \n                        if not any(keyword in k for keyword in excluded_layers)}\n        print(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n        logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n        pretrained_dict = filtered_dict\n    msg = encoder.load_state_dict(pretrained_dict, strict=False)\n    print(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n    logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n\n    # -- loading predictor\n    pretrained_dict = checkpoint['predictor']\n    \n    # Remove 'module.' prefix if it exists\n    new_pretrained_dict = {}\n    for k, v in pretrained_dict.items():\n        if k.startswith('module.'):\n            new_pretrained_dict[k[7:]] = v  # Remove 'module.' prefix\n        else:\n            new_pretrained_dict[k] = v\n    pretrained_dict = new_pretrained_dict\n\n    # Apply excluded_layers filtering\n    if excluded_layers != None:\n        filtered_dict = {k: v for k, v in pretrained_dict.items() \n                        if not any(keyword in k for keyword in excluded_layers)}\n        print(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n        logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n        pretrained_dict = filtered_dict\n\n    msg = predictor.load_state_dict(pretrained_dict, strict=False)\n    logger.info(f'loaded pretrained predictor from epoch {epoch} with msg: {msg}')\n\n    # -- loading target_encoder\n    if target_encoder is not None:\n        pretrained_dict = checkpoint['target_encoder']\n        \n        # Remove 'module.' prefix if it exists\n        new_pretrained_dict = {}\n        for k, v in pretrained_dict.items():\n            if k.startswith('module.'):\n                new_pretrained_dict[k[7:]] = v  # Remove 'module.' prefix\n            else:\n                new_pretrained_dict[k] = v\n        pretrained_dict = new_pretrained_dict\n        \n        if excluded_layers != None:\n            filtered_dict = {k: v for k, v in pretrained_dict.items() \n                            if not any(keyword in k for keyword in excluded_layers)}\n            logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n            pretrained_dict = filtered_dict\n\n        msg = target_encoder.load_state_dict(pretrained_dict, strict=False)\n        logger.info(f'loaded pretrained target_encoder from epoch {epoch} with msg: {msg}')\n\n    # -- loading optimizer (no need to modify keys for optimizer)\n    opt.load_state_dict(checkpoint['opt'])\n    if scaler is not None and 'scaler' in checkpoint:\n        scaler.load_state_dict(checkpoint['scaler'])\n    logger.info(f'loaded optimizers from epoch {epoch}')\n    logger.info(f'read-path: {r_path}')\n    del checkpoint\n\n    # except Exception as e:\n    #     logger.info(f'Encountered exception when loading checkpoint: {e}')\n    #     epoch = 0\n\n    return encoder, predictor, target_encoder, opt, scaler, epoch\n\n\n\ndef init_model(\n    device,\n    patch_size=16,\n    model_name='vit_base',\n    timeline_pixel_width=16*300,\n    pred_depth=6,\n    pred_emb_dim=384\n):\n    encoder = vit_huge(\n        timeline_pixel_width=timeline_pixel_width,\n        patch_size=patch_size)\n    predictor = vit_predictor(\n        num_patches=encoder.patch_embed.num_patches,\n        embed_dim=encoder.embed_dim,\n        predictor_embed_dim=pred_emb_dim,\n        depth=pred_depth,\n        num_heads=encoder.num_heads)\n\n    def init_weights(m):\n        if isinstance(m, torch.nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                torch.nn.init.constant_(m.bias, 0)\n        elif isinstance(m, torch.nn.LayerNorm):\n            torch.nn.init.constant_(m.bias, 0)\n            torch.nn.init.constant_(m.weight, 1.0)\n\n    for m in encoder.modules():\n        init_weights(m)\n\n    for m in predictor.modules():\n        init_weights(m)\n\n    encoder.to(device)\n    predictor.to(device)\n    logger.info(encoder)\n    return encoder, predictor\n\n\ndef init_opt(\n    encoder,\n    predictor,\n    iterations_per_epoch,\n    start_lr,\n    ref_lr,\n    warmup,\n    num_epochs,\n    wd=1e-6,\n    final_wd=1e-6,\n    final_lr=0.0,\n    use_bfloat16=False,\n    ipe_scale=1.25\n):\n    param_groups = [\n        {\n            'params': (p for n, p in encoder.named_parameters()\n                       if ('bias' not in n) and (len(p.shape) != 1))\n        }, {\n            'params': (p for n, p in predictor.named_parameters()\n                       if ('bias' not in n) and (len(p.shape) != 1))\n        }, {\n            'params': (p for n, p in encoder.named_parameters()\n                       if ('bias' in n) or (len(p.shape) == 1)),\n            'WD_exclude': True,\n            'weight_decay': 0\n        }, {\n            'params': (p for n, p in predictor.named_parameters()\n                       if ('bias' in n) or (len(p.shape) == 1)),\n            'WD_exclude': True,\n            'weight_decay': 0\n        }\n    ]\n\n    logger.info('Using AdamW')\n    optimizer = torch.optim.AdamW(param_groups)\n    scheduler = WarmupCosineSchedule(\n        optimizer,\n        warmup_steps=int(warmup*iterations_per_epoch),\n        start_lr=start_lr,\n        ref_lr=ref_lr,\n        final_lr=final_lr,\n        T_max=int(ipe_scale*num_epochs*iterations_per_epoch))\n    wd_scheduler = CosineWDSchedule(\n        optimizer,\n        ref_wd=wd,\n        final_wd=final_wd,\n        T_max=int(ipe_scale*num_epochs*iterations_per_epoch))\n    scaler = torch.cuda.amp.GradScaler() if use_bfloat16 else None\n    return optimizer, scaler, scheduler, wd_scheduler","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:33:24.603067Z","iopub.execute_input":"2025-06-13T15:33:24.603538Z","iopub.status.idle":"2025-06-13T15:33:24.626466Z","shell.execute_reply.started":"2025-06-13T15:33:24.603513Z","shell.execute_reply":"2025-06-13T15:33:24.625301Z"},"trusted":true},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# encoder, predictor = init_model(\n#     device=device,\n#     patch_size=patch_size,\n#     timeline_pixel_width=300*16,\n#     pred_depth=pred_depth,\n#     pred_emb_dim=pred_emb_dim,\n#     model_name=model_name)\n# target_encoder = copy.deepcopy(encoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:33:29.433183Z","iopub.execute_input":"2025-06-13T15:33:29.433568Z","iopub.status.idle":"2025-06-13T15:33:29.438341Z","shell.execute_reply.started":"2025-06-13T15:33:29.433543Z","shell.execute_reply":"2025-06-13T15:33:29.437381Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# r_path = load_path\n\n# checkpoint = torch.load(r_path, map_location=torch.device('cpu'))\n# epoch = checkpoint['epoch']\n\n# # -- loading encoder with filtering\n# pretrained_dict = checkpoint['encoder']\n\n# # Remove 'module.' prefix if it exists\n# new_pretrained_dict = {}\n# for k, v in pretrained_dict.items():\n#     if k.startswith('module.'):\n#         new_pretrained_dict[k[7:]] = v  # Remove 'module.' prefix\n#     else:\n#         new_pretrained_dict[k] = v\n# pretrained_dict = new_pretrained_dict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:33:29.588412Z","iopub.execute_input":"2025-06-13T15:33:29.589078Z","iopub.status.idle":"2025-06-13T15:33:29.592843Z","shell.execute_reply.started":"2025-06-13T15:33:29.589045Z","shell.execute_reply":"2025-06-13T15:33:29.591939Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# pretrained_dict.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:33:29.711377Z","iopub.execute_input":"2025-06-13T15:33:29.712091Z","iopub.status.idle":"2025-06-13T15:33:29.715504Z","shell.execute_reply.started":"2025-06-13T15:33:29.712065Z","shell.execute_reply":"2025-06-13T15:33:29.714673Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# excluded_layers = [\"pos_embed\", \"patch_embed\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:33:29.826896Z","iopub.execute_input":"2025-06-13T15:33:29.827232Z","iopub.status.idle":"2025-06-13T15:33:29.831504Z","shell.execute_reply.started":"2025-06-13T15:33:29.827210Z","shell.execute_reply":"2025-06-13T15:33:29.830522Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# filtered_dict = {k: v for k, v in pretrained_dict.items() \n#                 if not any(keyword in k for keyword in excluded_layers)}\n# print(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n# logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n# pretrained_dict = filtered_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:33:31.574520Z","iopub.execute_input":"2025-06-13T15:33:31.574911Z","iopub.status.idle":"2025-06-13T15:33:31.580203Z","shell.execute_reply.started":"2025-06-13T15:33:31.574885Z","shell.execute_reply":"2025-06-13T15:33:31.579068Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# pretrained_dict.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:33:31.745700Z","iopub.execute_input":"2025-06-13T15:33:31.746599Z","iopub.status.idle":"2025-06-13T15:33:31.751265Z","shell.execute_reply.started":"2025-06-13T15:33:31.746567Z","shell.execute_reply":"2025-06-13T15:33:31.750248Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"class WarmupCosineSchedule(object):\n\n    def __init__(\n        self,\n        optimizer,\n        warmup_steps,\n        start_lr,\n        ref_lr,\n        T_max,\n        last_epoch=-1,\n        final_lr=0.\n    ):\n        self.optimizer = optimizer\n        self.start_lr = start_lr\n        self.ref_lr = ref_lr\n        self.final_lr = final_lr\n        self.warmup_steps = warmup_steps\n        self.T_max = T_max - warmup_steps\n        self._step = 0.\n\n    def step(self):\n        self._step += 1\n        if self._step < self.warmup_steps:\n            progress = float(self._step) / float(max(1, self.warmup_steps))\n            new_lr = self.start_lr + progress * (self.ref_lr - self.start_lr)\n        else:\n            # -- progress after warmup\n            progress = float(self._step - self.warmup_steps) / float(max(1, self.T_max))\n            new_lr = max(self.final_lr,\n                         self.final_lr + (self.ref_lr - self.final_lr) * 0.5 * (1. + math.cos(math.pi * progress)))\n\n        for group in self.optimizer.param_groups:\n            group['lr'] = new_lr\n\n        return new_lr\n\n\nclass CosineWDSchedule(object):\n\n    def __init__(\n        self,\n        optimizer,\n        ref_wd,\n        T_max,\n        final_wd=0.\n    ):\n        self.optimizer = optimizer\n        self.ref_wd = ref_wd\n        self.final_wd = final_wd\n        self.T_max = T_max\n        self._step = 0.\n\n    def step(self):\n        self._step += 1\n        progress = self._step / self.T_max\n        new_wd = self.final_wd + (self.ref_wd - self.final_wd) * 0.5 * (1. + math.cos(math.pi * progress))\n\n        if self.final_wd <= self.ref_wd:\n            new_wd = max(self.final_wd, new_wd)\n        else:\n            new_wd = min(self.final_wd, new_wd)\n\n        for group in self.optimizer.param_groups:\n            if ('WD_exclude' not in group) or not group['WD_exclude']:\n                group['weight_decay'] = new_wd\n        return new_wd","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:33:32.092182Z","iopub.execute_input":"2025-06-13T15:33:32.092515Z","iopub.status.idle":"2025-06-13T15:33:32.103707Z","shell.execute_reply.started":"2025-06-13T15:33:32.092491Z","shell.execute_reply":"2025-06-13T15:33:32.102559Z"},"trusted":true},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"### Initializing model","metadata":{}},{"cell_type":"code","source":"# -- init model\nencoder, predictor = init_model(\n    device=device,\n    patch_size=patch_size,\n    timeline_pixel_width=300*16,\n    pred_depth=pred_depth,\n    pred_emb_dim=pred_emb_dim,\n    model_name=model_name)\ntarget_encoder = copy.deepcopy(encoder)","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:33:32.389600Z","iopub.execute_input":"2025-06-13T15:33:32.390006Z","iopub.status.idle":"2025-06-13T15:33:52.013551Z","shell.execute_reply.started":"2025-06-13T15:33:32.389980Z","shell.execute_reply":"2025-06-13T15:33:52.012429Z"},"trusted":true},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# load_model = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:33:52.015507Z","iopub.execute_input":"2025-06-13T15:33:52.015878Z","iopub.status.idle":"2025-06-13T15:33:52.020647Z","shell.execute_reply.started":"2025-06-13T15:33:52.015846Z","shell.execute_reply":"2025-06-13T15:33:52.019763Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# -- init optimizer and scheduler\noptimizer, scaler, scheduler, wd_scheduler = init_opt(\n    encoder=encoder,\n    predictor=predictor,\n    wd=wd,\n    final_wd=final_wd,\n    start_lr=start_lr,\n    ref_lr=lr,\n    final_lr=final_lr,\n    iterations_per_epoch=ipe,\n    warmup=warmup,\n    num_epochs=num_epochs,\n    ipe_scale=ipe_scale,\n    use_bfloat16=use_bfloat16)\n    # encoder = DistributedDataParallel(encoder, static_graph=True)\n    # predictor = DistributedDataParallel(predictor, static_graph=True)\n    # target_encoder = DistributedDataParallel(target_encoder)\nfor p in target_encoder.parameters():\n    p.requires_grad = False\n\n# -- momentum schedule\nmomentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n                      for i in range(int(ipe*num_epochs*ipe_scale)+1))\n\nstart_epoch = 0\n# -- load training checkpoint\nif load_model:\n    encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n        device=device,\n        r_path=load_path,\n        encoder=encoder,\n        predictor=predictor,\n        target_encoder=target_encoder,\n        opt=optimizer,\n        scaler=scaler,\n        excluded_layers = [\"pos_embed\", \"patch_embed\"])\n    # for _ in range(start_epoch*ipe):\n    #     scheduler.step()\n    #     wd_scheduler.step()\n    #     next(momentum_scheduler)\n    #     mask_collator.step()\n\ndef save_checkpoint(epoch):\n    save_dict = {\n        'encoder': encoder.state_dict(),\n        'predictor': predictor.state_dict(),\n        'target_encoder': target_encoder.state_dict(),\n        'opt': optimizer.state_dict(),\n        'scaler': None if scaler is None else scaler.state_dict(),\n        'epoch': epoch,\n        'loss': loss_meter.avg,\n        'batch_size': batch_size,\n        'lr': lr\n    }\n    if rank == 0:\n        torch.save(save_dict, latest_path) \n        if (epoch + 1) % checkpoint_freq == 0:\n            torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:33:52.021600Z","iopub.execute_input":"2025-06-13T15:33:52.021909Z","iopub.status.idle":"2025-06-13T15:34:48.733084Z","shell.execute_reply.started":"2025-06-13T15:33:52.021875Z","shell.execute_reply":"2025-06-13T15:34:48.732034Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_71/3450473587.py:186: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler() if use_bfloat16 else None\n","output_type":"stream"},{"name":"stdout","text":"Excluded 3 parameters containing: ['pos_embed', 'patch_embed']\nloaded pretrained encoder from epoch 299 with msg: _IncompatibleKeys(missing_keys=['pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias'], unexpected_keys=[])\nExcluded 1 parameters containing: ['pos_embed', 'patch_embed']\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"encoder.state_dict().keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:30:39.357856Z","iopub.status.idle":"2025-06-13T15:30:39.358390Z","shell.execute_reply.started":"2025-06-13T15:30:39.358201Z","shell.execute_reply":"2025-06-13T15:30:39.358226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:30:39.359326Z","iopub.status.idle":"2025-06-13T15:30:39.359740Z","shell.execute_reply.started":"2025-06-13T15:30:39.359529Z","shell.execute_reply":"2025-06-13T15:30:39.359547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:30:39.360835Z","iopub.status.idle":"2025-06-13T15:30:39.361102Z","shell.execute_reply.started":"2025-06-13T15:30:39.360981Z","shell.execute_reply":"2025-06-13T15:30:39.360993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(epoch):\n    \"\"\"Run validation on the validation dataset and return average loss.\"\"\"\n    logger.info('Running validation...')\n    val_loss_meter = AverageMeter()\n    val_maskA_meter = AverageMeter()\n    val_maskB_meter = AverageMeter()\n    val_time_meter = AverageMeter()\n    \n    # Set models to eval mode\n    encoder.eval()\n    predictor.eval()\n    target_encoder.eval()\n    \n    with torch.no_grad():\n        for itr, (udata, masks_enc, masks_pred) in enumerate(val_loader):\n            # Load and process images\n            imgs = udata.to(device, non_blocking=True)\n            masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n            masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n            \n            val_maskA_meter.update(len(masks_1[0][0]))\n            val_maskB_meter.update(len(masks_2[0][0]))\n            \n            # Forward pass\n            def val_step():\n                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n                    # Target encoding\n                    h = target_encoder(imgs)\n                    h = F.layer_norm(h, (h.size(-1),))\n                    B = len(h)\n                    h = apply_masks(h, masks_2)\n                    h = repeat_interleave_batch(h, B, repeat=len(masks_1))\n                    \n                    # Context encoding and prediction\n                    z = encoder(imgs, masks_1)\n                    z = predictor(z, masks_1, masks_2)\n                    \n                    # Loss calculation\n                    loss = F.smooth_l1_loss(z, h)\n                    return float(loss)\n            \n            loss, etime = gpu_timer(val_step)\n            val_loss_meter.update(loss)\n            val_time_meter.update(etime)\n            \n            # Log progress occasionally\n            if itr % (log_freq * 2) == 0:\n                logger.info(f'Val: [{epoch + 1}, {itr}] loss: {val_loss_meter.avg:.3f} '\n                           f'masks: {val_maskA_meter.avg:.1f} {val_maskB_meter.avg:.1f} '\n                           f'({val_time_meter.avg:.1f} ms)')\n    \n    # Set models back to training mode\n    encoder.train()\n    predictor.train()\n    \n    return val_loss_meter.avg","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:30:39.363115Z","iopub.status.idle":"2025-06-13T15:30:39.363701Z","shell.execute_reply.started":"2025-06-13T15:30:39.363453Z","shell.execute_reply":"2025-06-13T15:30:39.363482Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train loop","metadata":{}},{"cell_type":"code","source":"# Add this configuration near your other parameters\nval_frequency = 156  # Run validation every 100 training steps\nbest_val_loss = float('inf')","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:30:39.364881Z","iopub.status.idle":"2025-06-13T15:30:39.365254Z","shell.execute_reply.started":"2025-06-13T15:30:39.365110Z","shell.execute_reply":"2025-06-13T15:30:39.365126Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -- TRAINING LOOP\nglobal_step = 0\nfor epoch in range(start_epoch, num_epochs):\n    logger.info('Epoch %d' % (epoch + 1))\n\n    loss_meter = AverageMeter()\n    maskA_meter = AverageMeter()\n    maskB_meter = AverageMeter()\n    time_meter = AverageMeter()\n\n    for itr, (udata, masks_enc, masks_pred) in enumerate(data_loader):\n        new_validation = False\n        print(udata.shape)\n        if global_step % val_frequency == 0:\n            new_validation = True\n            # Run validation and check if we need to save the model\n            val_loss = validate(epoch)\n            print(f\"Validation loss: {val_loss}\")\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                save_checkpoint(epoch)\n        \n        def load_imgs():\n            # -- unsupervised imgs\n            imgs = udata.to(device, non_blocking=True) # udata[0]\n            masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n            masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n            return (imgs, masks_1, masks_2)\n        imgs, masks_enc, masks_pred = load_imgs()\n        maskA_meter.update(len(masks_enc[0][0]))\n        maskB_meter.update(len(masks_pred[0][0]))\n\n        def train_step():\n            _new_lr = scheduler.step()\n            _new_wd = wd_scheduler.step()\n            # --\n\n            def forward_target():\n                with torch.no_grad():\n                    h = target_encoder(imgs)\n                    h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n                    B = len(h)\n                    # -- create targets (masked regions of h)\n                    h = apply_masks(h, masks_pred)\n                    h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n                    return h\n\n            def forward_context():\n                z = encoder(imgs, masks_enc)\n                z = predictor(z, masks_enc, masks_pred)\n                return z\n\n            def loss_fn(z, h):\n                loss = F.smooth_l1_loss(z, h)\n                return loss\n\n            # Step 1. Forward\n            with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n                h = forward_target()\n                z = forward_context()\n                loss = loss_fn(z, h)\n\n            #  Step 2. Backward & step\n            if use_bfloat16:\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                optimizer.step()\n            # grad_stats = grad_logger(encoder.named_parameters())\n            optimizer.zero_grad()\n\n            # Step 3. momentum update of target encoder\n            with torch.no_grad():\n                m = next(momentum_scheduler)\n                for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n                    param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n\n            return (float(loss), _new_lr, _new_wd, None)#grad_stats)\n        (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n        loss_meter.update(loss)\n        time_meter.update(etime)\n\n        # -- Logging\n        def log_stats():\n            if new_validation:\n                log_val_value = val_loss\n            else:\n                log_val_value = None\n            csv_logger.log(epoch + 1, itr, loss, log_val_value, maskA_meter.val, maskB_meter.val, etime)\n            if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n                logger.info('[%d, %5d] loss: %.3f '\n                            'masks: %.1f %.1f '\n                            '[wd: %.2e] [lr: %.2e] '\n                            '[mem: %.2e] '\n                            '(%.1f ms)'\n                            % (epoch + 1, itr,\n                               loss_meter.avg,\n                               maskA_meter.avg,\n                               maskB_meter.avg,\n                               _new_wd,\n                               _new_lr,\n                               torch.cuda.max_memory_allocated() / 1024.**2,\n                               time_meter.avg))\n                print(f\"loss: {loss_meter.avg}, maskA: {maskA_meter.avg}, maskB: {maskB_meter.avg}\")\n\n                if grad_stats is not None:\n                    logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n                                % (epoch + 1, itr,\n                                   grad_stats.first_layer,\n                                   grad_stats.last_layer,\n                                   grad_stats.min,\n                                   grad_stats.max))\n\n        log_stats()\n\n        assert not np.isnan(loss), 'loss is nan'\n\n        global_step += 1\n\n    # -- Save Checkpoint after every epoch\n    logger.info('avg. loss %.3f' % loss_meter.avg)\n    save_checkpoint(epoch+1)","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:30:39.365897Z","iopub.status.idle":"2025-06-13T15:30:39.366152Z","shell.execute_reply.started":"2025-06-13T15:30:39.366036Z","shell.execute_reply":"2025-06-13T15:30:39.366048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_checkpoint(epoch, global_step=None, val_loss=None):\n    save_dict = {\n        'encoder': encoder.state_dict(),\n        'predictor': predictor.state_dict(),\n        'target_encoder': target_encoder.state_dict(),\n        'opt': optimizer.state_dict(),\n        'scaler': None if scaler is None else scaler.state_dict(),\n        'epoch': epoch,\n        'step': global_step,\n        'train_loss': loss_meter.avg,\n        'val_loss': val_loss,\n        'batch_size': batch_size,\n        'lr': lr\n    }\n    if rank == 0:\n        torch.save(save_dict, latest_path)\n        if (epoch + 1) % checkpoint_freq == 0:\n            torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))","metadata":{"execution":{"iopub.status.busy":"2025-06-13T15:30:39.367327Z","iopub.status.idle":"2025-06-13T15:30:39.367652Z","shell.execute_reply.started":"2025-06-13T15:30:39.367522Z","shell.execute_reply":"2025-06-13T15:30:39.367537Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}