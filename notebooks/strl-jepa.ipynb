{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11813224,"sourceType":"datasetVersion","datasetId":7419713},{"sourceId":11813229,"sourceType":"datasetVersion","datasetId":7419718},{"sourceId":11813237,"sourceType":"datasetVersion","datasetId":7419725},{"sourceId":12128618,"sourceType":"datasetVersion","datasetId":7637421},{"sourceId":432598,"sourceType":"modelInstanceVersion","modelInstanceId":352670,"modelId":373951}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I-JEPA","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# core train\nimport os\nimport copy\nimport sys\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport yaml","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-06-16T15:22:44.296532Z","iopub.execute_input":"2025-06-16T15:22:44.296725Z","iopub.status.idle":"2025-06-16T15:22:46.011984Z","shell.execute_reply.started":"2025-06-16T15:22:44.296709Z","shell.execute_reply":"2025-06-16T15:22:46.011157Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"# Basic global config\n_GLOBAL_SEED = 0\nnp.random.seed(_GLOBAL_SEED)\ntorch.manual_seed(_GLOBAL_SEED)\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:46.012854Z","iopub.execute_input":"2025-06-16T15:22:46.013231Z","iopub.status.idle":"2025-06-16T15:22:46.019878Z","shell.execute_reply.started":"2025-06-16T15:22:46.013203Z","shell.execute_reply":"2025-06-16T15:22:46.019141Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"args = {\n    \"data\": {\n        \"batch_size\": 8,\n        \"image_folders\": ['/kaggle/input/fmri-train-1-norm-v3/data/noisy_func_train_1.npy',\n                '/kaggle/input/fmri-train-2-norm-v3/data/noisy_func_train_2.npy',\n                '/kaggle/input/fmri-train-3-norm-v3/data/noisy_func_train_3.npy'],\n        \"validation_folders\": ['/kaggle/input/fmri-val-norm-v3/data/noisy_func_val.npy'],\n        \"num_workers\": 2,\n        \"pin_mem\": True,\n        \"root_path\": \"/kaggle/input\",\n        \"use_horizontal_flip\": False\n    },\n    \"logging\": {\n        \"folder\": \"/kaggle/working/logs\",\n        \"write_tag\": \"jepa\"\n    },\n    \"mask\": {\n        \"allow_overlap\": False,\n        \"aspect_ratio\": [0.75, 1.5],\n        \"enc_mask_scale\": [0.85, 1.0],\n        \"min_keep\": 10,\n        \"num_enc_masks\": 1,\n        \"num_pred_masks\": 1,\n        \"patch_size\": 16,\n        \"pred_mask_scale\": [0.15, 0.2]\n    },\n    \"meta\": {\n        \"copy_data\": False,\n        \"load_checkpoint\": False,\n        \"model_name\": \"vit_small\",\n        \"pred_depth\": 12,\n        \"pred_emb_dim\": 384,\n        \"read_checkpoint\": \"/kaggle/input/jepa-14x14/pytorch/default/1/IN1K-vit.h.14-300e.pth.tar\",\n        \"use_bfloat16\": True\n    },\n    \"optimization\": {\n        \"ema\": [0.996, 1.0],\n        \"epochs\": 5,\n        \"final_lr\": 1.0e-6,\n        \"final_weight_decay\": 0.4,\n        \"ipe_scale\": 1.0,\n        \"lr\": 0.001,\n        \"start_lr\": 0.0002,\n        \"warmup\": 50,\n        \"weight_decay\": 0.04\n    }\n}\n","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:46.020691Z","iopub.execute_input":"2025-06-16T15:22:46.020913Z","iopub.status.idle":"2025-06-16T15:22:46.037086Z","shell.execute_reply.started":"2025-06-16T15:22:46.020885Z","shell.execute_reply":"2025-06-16T15:22:46.036326Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resume_preempt = False\nrank = 0\n\n# -- META\nuse_bfloat16 = args['meta']['use_bfloat16']\nmodel_name = args['meta']['model_name']\nload_model = args['meta']['load_checkpoint'] or resume_preempt\nr_file = args['meta']['read_checkpoint']\ncopy_data = args['meta']['copy_data']\npred_depth = args['meta']['pred_depth']\npred_emb_dim = args['meta']['pred_emb_dim']\nif not torch.cuda.is_available():\n    device = torch.device('cpu')\nelse:\n    device = torch.device('cuda:0')\n    torch.cuda.set_device(device)\n\n# -- DATA\nbatch_size = args['data']['batch_size']\npin_mem = args['data']['pin_mem']\nnum_workers = args['data']['num_workers']\nroot_path = args['data']['root_path']\nimage_folders = args['data']['image_folders']\nvalidation_folders = args['data']['validation_folders']\n# --\n\n# -- MASK\nallow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\npatch_size = args['mask']['patch_size']  # patch-size for model training\nnum_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\nmin_keep = args['mask']['min_keep']  # min number of patches in context block\nenc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\nnum_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\npred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\naspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n# --\n\n# -- OPTIMIZATION\nema = args['optimization']['ema']\nipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\nwd = float(args['optimization']['weight_decay'])\nfinal_wd = float(args['optimization']['final_weight_decay'])\nnum_epochs = args['optimization']['epochs']\nwarmup = args['optimization']['warmup']\nstart_lr = args['optimization']['start_lr']\nlr = args['optimization']['lr']\nfinal_lr = args['optimization']['final_lr']\n\n# -- LOGGING\nfolder = args['logging']['folder']\ntag = args['logging']['write_tag']\n\nos.makedirs(folder, exist_ok=True)\ndump = os.path.join(folder, 'params-ijepa.yaml')\nwith open(dump, 'w') as f:\n    yaml.dump(args, f)","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:46.037842Z","iopub.execute_input":"2025-06-16T15:22:46.038087Z","iopub.status.idle":"2025-06-16T15:22:46.113390Z","shell.execute_reply.started":"2025-06-16T15:22:46.038057Z","shell.execute_reply":"2025-06-16T15:22:46.112704Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Logging","metadata":{}},{"cell_type":"code","source":"import logging\nlog_timings = True\nlog_freq = 10\ncheckpoint_freq = 1\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogger = logging.getLogger()","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:46.114291Z","iopub.execute_input":"2025-06-16T15:22:46.114567Z","iopub.status.idle":"2025-06-16T15:22:46.119359Z","shell.execute_reply.started":"2025-06-16T15:22:46.114537Z","shell.execute_reply":"2025-06-16T15:22:46.118598Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gpu_timer(closure, log_timings=True):\n    \"\"\" Helper to time gpu-time to execute closure() \"\"\"\n    log_timings = log_timings and torch.cuda.is_available()\n\n    elapsed_time = -1.\n    if log_timings:\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record()\n\n    result = closure()\n\n    if log_timings:\n        end.record()\n        torch.cuda.synchronize()\n        elapsed_time = start.elapsed_time(end)\n\n    return result, elapsed_time","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:46.121964Z","iopub.execute_input":"2025-06-16T15:22:46.122159Z","iopub.status.idle":"2025-06-16T15:22:46.133154Z","shell.execute_reply.started":"2025-06-16T15:22:46.122144Z","shell.execute_reply":"2025-06-16T15:22:46.132395Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CSVLogger(object):\n\n    def __init__(self, fname, *argv):\n        self.fname = fname\n        self.types = []\n        # -- print headers\n        with open(self.fname, '+a') as f:\n            for i, v in enumerate(argv, 1):\n                self.types.append(v[0])\n                if i < len(argv):\n                    print(v[1], end=',', file=f)\n                else:\n                    print(v[1], end='\\n', file=f)\n\n    def log(self, *argv):\n        with open(self.fname, '+a') as f:\n            for i, tv in enumerate(zip(self.types, argv), 1):\n                end = ',' if i < len(argv) else '\\n'\n                print(tv[0] % tv[1], end=end, file=f)\n\n\nclass AverageMeter(object):\n    \"\"\"computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.max = float('-inf')\n        self.min = float('inf')\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        try:\n            self.max = max(val, self.max)\n            self.min = min(val, self.min)\n        except Exception:\n            pass\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:46.133957Z","iopub.execute_input":"2025-06-16T15:22:46.134190Z","iopub.status.idle":"2025-06-16T15:22:46.149386Z","shell.execute_reply.started":"2025-06-16T15:22:46.134164Z","shell.execute_reply":"2025-06-16T15:22:46.148640Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -- log/checkpointing paths\nlog_file = os.path.join(folder, f'{tag}_r{rank}.csv')\nsave_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\nlatest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\nload_path = None\nif load_model:\n    load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n\n# csv logger\ncsv_logger = CSVLogger(log_file,\n                       ('%d', 'epoch'),\n                       ('%d', 'itr'),\n                       ('%.5f', 'train_loss'),\n                       ('%.5f', 'val_loss'),  # Added validation loss\n                       ('%.5f', 'mask-A'),\n                       ('%.5f', 'mask-B'),\n                       ('%d', 'time (ms)'))","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:46.150193Z","iopub.execute_input":"2025-06-16T15:22:46.150681Z","iopub.status.idle":"2025-06-16T15:22:46.170045Z","shell.execute_reply.started":"2025-06-16T15:22:46.150658Z","shell.execute_reply":"2025-06-16T15:22:46.169237Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset creation and preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Data transformation","metadata":{}},{"cell_type":"markdown","source":"#### Masking","metadata":{}},{"cell_type":"code","source":"import math\nfrom multiprocessing import Value","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:46.171118Z","iopub.execute_input":"2025-06-16T15:22:46.171383Z","iopub.status.idle":"2025-06-16T15:22:46.184485Z","shell.execute_reply.started":"2025-06-16T15:22:46.171360Z","shell.execute_reply":"2025-06-16T15:22:46.183792Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TimeSeriesMaskCollator:\n    def __init__(self, num_frames=300, frame_size=16, nenc=1, npred=1):\n        self.num_frames = num_frames\n        self.frame_size = frame_size\n        self._itr_counter = Value('i', -1)\n        self.npred = 1\n        self.nenc = 1\n\n    def step(self):\n        i = self._itr_counter\n        with i.get_lock():\n            i.value += 1\n            v = i.value\n        return v\n    \n    def collate_merge_batches(self, batch):\n        merged = torch.cat([item for item in batch], dim=0)\n        return merged\n\n    def _sample_frame_mask(self, generator, exclude_frames=None):\n        # Build list of available frame indices\n        choices = torch.tensor(\n            [i for i in range(self.num_frames) if (exclude_frames is None or i not in exclude_frames)],\n            dtype=torch.long\n        )\n        # Sample one index using the generator\n        idx = torch.randint(0, len(choices), (1,), generator=generator).item()\n        frame_idx = choices[idx]\n\n        mask = torch.zeros(self.num_frames, dtype=torch.int32)\n        mask[frame_idx] = 1\n        mask = torch.nonzero(mask.flatten()).squeeze()\n        return mask, frame_idx\n\n    def build_encoder_mask_from_pred(self, pred_masks):\n        enc_mask = torch.ones(self.num_frames, dtype=torch.int32)\n        for pred_mask in pred_masks:\n            enc_mask[pred_mask] = 0  # Zero out the masked regions\n        return torch.nonzero(enc_mask.flatten()).squeeze()\n\n    def __call__(self, batch):\n        collated_batch = self.collate_merge_batches(batch)\n        B = collated_batch.shape[0]\n\n        seed = self.step()\n        g = torch.Generator()\n        g.manual_seed(seed)\n\n        collated_masks_enc, collated_masks_pred = [], []\n\n        for _ in range(B):\n            pred_masks = []\n            pred_frame_idxs = []\n            for _ in range(self.npred):\n                mask, idx = self._sample_frame_mask(generator=g, exclude_frames=pred_frame_idxs)\n                pred_masks.append(mask)\n                pred_frame_idxs.append(idx)\n\n            collated_masks_pred.append(pred_masks)\n\n            enc_masks = []\n            for _ in range(self.nenc):\n                enc_mask = self.build_encoder_mask_from_pred(pred_masks)\n                enc_masks.append(enc_mask)\n\n            collated_masks_enc.append(enc_masks)\n\n        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n\n        return collated_batch, collated_masks_enc, collated_masks_pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:46.185295Z","iopub.execute_input":"2025-06-16T15:22:46.185507Z","iopub.status.idle":"2025-06-16T15:22:46.201269Z","shell.execute_reply.started":"2025-06-16T15:22:46.185492Z","shell.execute_reply":"2025-06-16T15:22:46.200529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_masks(x, masks):\n    \"\"\"\n    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n    :param masks: list of tensors containing indices of patches in [N] to keep\n    \"\"\"\n    all_x = []\n    for m in masks:\n        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n    return torch.cat(all_x, dim=0)","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:46.201953Z","iopub.execute_input":"2025-06-16T15:22:46.202146Z","iopub.status.idle":"2025-06-16T15:22:46.217574Z","shell.execute_reply.started":"2025-06-16T15:22:46.202131Z","shell.execute_reply":"2025-06-16T15:22:46.216852Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask_collator = TimeSeriesMaskCollator() # defaults to 300 frames of size 16x16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:46.218307Z","iopub.execute_input":"2025-06-16T15:22:46.218576Z","iopub.status.idle":"2025-06-16T15:22:46.234214Z","shell.execute_reply.started":"2025-06-16T15:22:46.218540Z","shell.execute_reply":"2025-06-16T15:22:46.233649Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Image transforms","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch # Import PyTorch\n\nclass Compose:\n    \"\"\"\n    Composes several transforms together.\n    Args:\n        transforms (list of callables): list of transforms to compose.\n    \"\"\"\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img_array):\n        \"\"\"\n        Applies the composed transforms to the input array.\n        The input can be a NumPy array. The output will be a torch.Tensor\n        if torch.from_numpy is the last transform.\n        Args:\n            img_array (numpy.ndarray): Input image array (C, H, W).\n        Returns:\n            torch.Tensor: Transformed image tensor.\n        \"\"\"\n        for t in self.transforms:\n            img_array = t(img_array) # Note: img_array will become a torch.Tensor at the end\n        return img_array\n\n\n# Build the transform list\ntransform_list = []\n\ndef reshape_to_2d_timeline(patch_time_series):\n    # Convert to PyTorch tensor immediately for permute/reshape\n    patch_time_series_tensor = torch.from_numpy(patch_time_series).float()\n    height_patch = patch_time_series_tensor.shape[0]  # H\n    width_patch = patch_time_series_tensor.shape[1]   # W\n    total_time_steps = patch_time_series_tensor.shape[2]  # T   \n\n    # print(patch_time_series_tensor.shape)\n\n    \n    # Apply the corrected reshaping logic here\n    # Original tensor shape (H, W, T) -> (16, 16, total_time_steps)\n    # Permute to (H, T, W)\n    intermediate_tensor = patch_time_series_tensor.permute(0, 2, 1)\n    \n    sample = intermediate_tensor.reshape(height_patch, total_time_steps * width_patch)\n\n    # Add a channel dimension: (1, H, T*W)\n    sample = sample.unsqueeze(0)\n    return sample\n    #return patch_time_series_tensor\n\ntransform_list += [reshape_to_2d_timeline]  # Add reshape transform\ntransform_list += [lambda x: x.unsqueeze(0)] # add batch dimension\n\n# Composition of transforms\ntransform = Compose(transform_list)","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:46.234984Z","iopub.execute_input":"2025-06-16T15:22:46.235236Z","iopub.status.idle":"2025-06-16T15:22:46.243072Z","shell.execute_reply.started":"2025-06-16T15:22:46.235217Z","shell.execute_reply":"2025-06-16T15:22:46.242312Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset & Dataloader","metadata":{}},{"cell_type":"code","source":"import random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:46.243761Z","iopub.execute_input":"2025-06-16T15:22:46.243994Z","iopub.status.idle":"2025-06-16T15:22:46.261671Z","shell.execute_reply.started":"2025-06-16T15:22:46.243977Z","shell.execute_reply":"2025-06-16T15:22:46.260977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NoisyDataset(torch.utils.data.Dataset):\n    def __init__(self, noisy_images_paths: list, transform=None, normalize=True):\n        \"\"\"Initialize fMRI dataset for denoising with memory-efficient loading,\n        extracting 16x16 patches with full time series per depth channel.\n\n        Args:\n            noisy_images_paths (list): List of paths to noisy fMRI volumes (.npy files)\n            stats: Dictionary containing 'mean' and 'std' for normalization.\n            transform (callable, optional): Optional transform to be applied on a sample.\n            normalize (bool): Whether to apply channel-wise normalization.\n        \"\"\"\n        self.noisy_paths = noisy_images_paths\n        self.transform = transform\n        self.normalize = normalize\n\n        self.file_slice_mapping = [] # Stores (file_idx, z_idx, patch_y_idx, patch_x_idx)\n\n        patch_size = 16\n        dataset_length = 0\n\n        for i, path in enumerate(noisy_images_paths):\n            # Load metadata about the file shape without loading full content\n            data_shape = np.load(path, mmap_mode='r').shape\n            # Assuming data_shape is (H, W, Z, T)\n            total_height, total_width, depth_channels, total_time_steps = data_shape\n\n            # Calculate how many patches fit along each spatial dimension\n            num_patches_width = total_width // patch_size\n            num_patches_height = total_height // patch_size\n\n            # Iterate over all depth channels (Z)\n            for z_idx in range(depth_channels):\n                # Iterate through the spatial grid of patches\n                for patch_y_idx in range(num_patches_height):\n                    for patch_x_idx in range(num_patches_width):\n                        # Each combination of (file, z_idx, patch_y_idx, patch_x_idx) is a unique item\n                        self.file_slice_mapping.append((i, z_idx, patch_y_idx, patch_x_idx))\n                        dataset_length += 1\n\n        self.data_len = dataset_length\n        self.patch_size = patch_size # Store patch_size for __getitem__\n\n    def __len__(self):\n        return self.data_len\n\n    def __getitem__(self, index):\n\n        valid_patch = False\n        while valid_patch == False:\n            # Use the mapping to determine which file, depth channel, and spatial patch to load\n            file_idx, z_idx, patch_y_idx, patch_x_idx = self.file_slice_mapping[index]\n    \n            # Load data from the specific file\n            noisy_file_path = self.noisy_paths[file_idx]\n    \n            # Load the full 4D array with mmap_mode to avoid loading everything into RAM\n            noisy_volume = np.load(noisy_file_path, mmap_mode='r')\n    \n            # Calculate the starting and ending coordinates for the current patch\n            start_h = patch_y_idx * self.patch_size\n            end_h = start_h + self.patch_size\n            start_w = patch_x_idx * self.patch_size\n            end_w = start_w + self.patch_size\n    \n            # Extract the 16x16 patch for the specific depth channel (z_idx)\n            # and include the entire time series.\n            # The resulting shape will be (patch_size, patch_size, time_steps)\n            patch_time_series = noisy_volume[start_h:end_h, start_w:end_w, z_idx, :].copy()\n\n            patch = patch_time_series[:,:,0]\n            patch_sum = np.sum(patch)\n            if patch_sum >= -25:\n                valid_patch = True\n            else:\n                index = random.randint(0, self.data_len-1)\n            \n\n        # Apply transformations if specified\n        if self.transform is not None:\n            # Your transform should expect a tensor of shape (C, H, W, T) or (H, W, T)\n            # depending on how you structure it. Adjust accordingly.\n            sample = self.transform(patch_time_series)\n\n        return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:46.262443Z","iopub.execute_input":"2025-06-16T15:22:46.262646Z","iopub.status.idle":"2025-06-16T15:22:46.278825Z","shell.execute_reply.started":"2025-06-16T15:22:46.262624Z","shell.execute_reply":"2025-06-16T15:22:46.278124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dataset\ndataset = NoisyDataset(noisy_images_paths=image_folders, transform=transform, normalize=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:46.279528Z","iopub.execute_input":"2025-06-16T15:22:46.279752Z","iopub.status.idle":"2025-06-16T15:22:46.331464Z","shell.execute_reply.started":"2025-06-16T15:22:46.279733Z","shell.execute_reply":"2025-06-16T15:22:46.330714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_dataset = NoisyDataset(noisy_images_paths=validation_folders, transform=transform, normalize=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:46.332311Z","iopub.execute_input":"2025-06-16T15:22:46.332564Z","iopub.status.idle":"2025-06-16T15:22:46.350855Z","shell.execute_reply.started":"2025-06-16T15:22:46.332540Z","shell.execute_reply":"2025-06-16T15:22:46.350145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[100].shape","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:46.351580Z","iopub.execute_input":"2025-06-16T15:22:46.351929Z","iopub.status.idle":"2025-06-16T15:22:46.454904Z","shell.execute_reply.started":"2025-06-16T15:22:46.351904Z","shell.execute_reply":"2025-06-16T15:22:46.454124Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[1].max()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:46.455712Z","iopub.execute_input":"2025-06-16T15:22:46.456344Z","iopub.status.idle":"2025-06-16T15:22:48.160118Z","shell.execute_reply.started":"2025-06-16T15:22:46.456322Z","shell.execute_reply":"2025-06-16T15:22:48.159356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import matplotlib.pyplot as plt\n\n# # Create a 64x64 tensor with random values (can be negative)\n# tensor = dataset[900]\n# tensor = tensor.detach().cpu().numpy()\n\n# # Output tensor: initialized with NaNs for discarded patches (visually blank)\n# visualized = np.full_like(tensor, np.nan)\n\n# patch_size = 16\n# for i in range(0, 64, patch_size):\n#     for j in range(0, 64, patch_size):\n#         patch = tensor[i:i+patch_size, j:j+patch_size]\n#         patch_sum = np.sum(patch)\n\n#         if patch_sum >= -25:\n#             visualized[i:i+patch_size, j:j+patch_size] = patch\n#         # else: keep as NaN for blank display\n\n# # Display: use masked array to make NaNs invisible in the colormap\n# masked = np.ma.masked_invalid(visualized)\n\n# plt.figure(figsize=(6, 6))\n# plt.imshow(masked, cmap='gray')\n# plt.title(\"Visible Patches (sum >= 0)\")\n# plt.axis('off')\n# plt.colorbar(label='Value Intensity')\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:48.161241Z","iopub.execute_input":"2025-06-16T15:22:48.161548Z","iopub.status.idle":"2025-06-16T15:22:48.166748Z","shell.execute_reply.started":"2025-06-16T15:22:48.161521Z","shell.execute_reply":"2025-06-16T15:22:48.166094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[1][:][:][:].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:48.172654Z","iopub.execute_input":"2025-06-16T15:22:48.172966Z","iopub.status.idle":"2025-06-16T15:22:48.947646Z","shell.execute_reply.started":"2025-06-16T15:22:48.172948Z","shell.execute_reply":"2025-06-16T15:22:48.947020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[0][0].permute(2,0,1)[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:48.948405Z","iopub.execute_input":"2025-06-16T15:22:48.948978Z","iopub.status.idle":"2025-06-16T15:22:49.890918Z","shell.execute_reply.started":"2025-06-16T15:22:48.948950Z","shell.execute_reply":"2025-06-16T15:22:49.890293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create a dummy tensor of the specified size for demonstration.\n# In your actual code, this would be your loaded image tensor.\n# Using torch.rand to get values between 0 and 1, suitable for image display.\nimage_tensor = dataset[1012][0][0]\n\n# Convert the PyTorch tensor to a NumPy array.\n# Use .detach().cpu().numpy() if your tensor is on a GPU or requires gradients.\nimage_np = image_tensor.detach().cpu().numpy()\n\nplt.figure(figsize=(480, 1.6)) # Adjust figure size for a wide, short image\n\n# Display the image\n# 'cmap=\"gray\"' or 'cmap=\"Greys_r\"' is standard for grayscale images.\n# 'aspect=\"auto\"' allows the image to stretch to fill the figure, which is helpful\n# for unusual aspect ratios like 16x4800.\nplt.imshow(image_np, cmap='gray', aspect='auto') \n\nplt.title('Visualization of a 16x4800 Grayscale Image')\nplt.xlabel('Width (pixels)')\nplt.ylabel('Height (pixels)')\nplt.colorbar(label='Pixel Intensity') # Add a colorbar to show intensity mapping\nplt.savefig(\"fig_new.png\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:49.891641Z","iopub.execute_input":"2025-06-16T15:22:49.891919Z","iopub.status.idle":"2025-06-16T15:22:52.770638Z","shell.execute_reply.started":"2025-06-16T15:22:49.891903Z","shell.execute_reply":"2025-06-16T15:22:52.769780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# # Create a dummy tensor of the specified size for demonstration.\n# # In your actual code, this would be your loaded image tensor.\n# # Using torch.rand to get values between 0 and 1, suitable for image display.\n# image_tensor = dataset[1000][32:48,32:48]# dataset[1][0][0]\n\n# # Convert the PyTorch tensor to a NumPy array.\n# # Use .detach().cpu().numpy() if your tensor is on a GPU or requires gradients.\n# image_np = image_tensor.detach().cpu().numpy()\n\n# plt.figure(figsize=(5, 5)) # Adjust figure size for a wide, short image\n\n# # Display the image\n# # 'cmap=\"gray\"' or 'cmap=\"Greys_r\"' is standard for grayscale images.\n# # 'aspect=\"auto\"' allows the image to stretch to fill the figure, which is helpful\n# # for unusual aspect ratios like 16x4800.\n# plt.imshow(image_np, cmap='gray', aspect='auto') \n\n# plt.title('Visualization of a 16x4800 Grayscale Image')\n# plt.xlabel('Width (pixels)')\n# plt.ylabel('Height (pixels)')\n# plt.colorbar(label='Pixel Intensity') # Add a colorbar to show intensity mapping\n# plt.savefig(\"fig_new.png\")\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:52.771629Z","iopub.execute_input":"2025-06-16T15:22:52.771882Z","iopub.status.idle":"2025-06-16T15:22:52.776343Z","shell.execute_reply.started":"2025-06-16T15:22:52.771862Z","shell.execute_reply":"2025-06-16T15:22:52.775504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_loader = torch.utils.data.DataLoader(\n    dataset,\n    collate_fn=mask_collator,\n    batch_size=batch_size,\n    drop_last=True,\n    pin_memory=pin_mem,\n    num_workers=num_workers,\n    persistent_workers=False)\n\nipe = len(data_loader)","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:52.777114Z","iopub.execute_input":"2025-06-16T15:22:52.777414Z","iopub.status.idle":"2025-06-16T15:22:52.794413Z","shell.execute_reply.started":"2025-06-16T15:22:52.777397Z","shell.execute_reply":"2025-06-16T15:22:52.793741Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    collate_fn=mask_collator,\n    batch_size=batch_size,\n    drop_last=True,\n    pin_memory=pin_mem,\n    num_workers=num_workers,\n    persistent_workers=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:28:53.372808Z","iopub.execute_input":"2025-06-16T15:28:53.373119Z","iopub.status.idle":"2025-06-16T15:28:53.377673Z","shell.execute_reply.started":"2025-06-16T15:28:53.373088Z","shell.execute_reply":"2025-06-16T15:28:53.377067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CUDA_LAUNCH_BLOCKING = 1\n# for data in data_loader:\n#     print(data[0].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:52.811637Z","iopub.execute_input":"2025-06-16T15:22:52.811884Z","iopub.status.idle":"2025-06-16T15:22:52.826101Z","shell.execute_reply.started":"2025-06-16T15:22:52.811863Z","shell.execute_reply":"2025-06-16T15:22:52.825494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"import math\nfrom functools import partial\nimport numpy as np\nimport torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:52.826727Z","iopub.execute_input":"2025-06-16T15:22:52.826977Z","iopub.status.idle":"2025-06-16T15:22:52.841161Z","shell.execute_reply.started":"2025-06-16T15:22:52.826956Z","shell.execute_reply":"2025-06-16T15:22:52.840674Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model backbone (vision transformer)","metadata":{}},{"cell_type":"code","source":"def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    with torch.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.erfinv_()\n\n        # Transform to proper mean, std\n        tensor.mul_(std * math.sqrt(2.))\n        tensor.add_(mean)\n\n        # Clamp to ensure it's in the proper range\n        tensor.clamp_(min=a, max=b)\n        return tensor\n\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n\n\ndef repeat_interleave_batch(x, B, repeat):\n    N = len(x) // B\n    x = torch.cat([\n        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n        for i in range(N)\n    ], dim=0)\n    return x","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:52.841717Z","iopub.execute_input":"2025-06-16T15:22:52.841893Z","iopub.status.idle":"2025-06-16T15:22:52.857534Z","shell.execute_reply.started":"2025-06-16T15:22:52.841879Z","shell.execute_reply":"2025-06-16T15:22:52.856989Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=float)\n    grid_w = np.arange(grid_size, dtype=float)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n\n\ndef get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    assert embed_dim % 2 == 0\n\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n    return emb\n\n\ndef get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid length\n    return:\n    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid = np.arange(grid_size, dtype=float)\n    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n\n\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=float)\n    omega /= embed_dim / 2.\n    omega = 1. / 10000**omega   # (D/2,)\n\n    pos = pos.reshape(-1)   # (M,)\n    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False):\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x, attn\n\n\nclass Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x, return_attention=False):\n        y, attn = self.attn(self.norm1(x))\n        if return_attention:\n            return attn\n        x = x + self.drop_path(y)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, timeline_pixel_width=16*300, patch_size=16, in_chans=1, embed_dim=768):\n        super().__init__()\n        num_patches = timeline_pixel_width // patch_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass ConvEmbed(nn.Module):\n    \"\"\"\n    3x3 Convolution stems for ViT following ViTC models\n    \"\"\"\n\n    def __init__(self, channels, strides, img_size=224, in_chans=1, batch_norm=True):\n        super().__init__()\n        # Build the stems\n        stem = []\n        channels = [in_chans] + channels\n        for i in range(len(channels) - 2):\n            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n                               stride=strides[i], padding=1, bias=(not batch_norm))]\n            if batch_norm:\n                stem += [nn.BatchNorm2d(channels[i+1])]\n            stem += [nn.ReLU(inplace=True)]\n        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n        self.stem = nn.Sequential(*stem)\n\n        # Comptute the number of patches\n        stride_prod = int(np.prod(strides))\n        self.num_patches = (img_size[0] // stride_prod)**2\n\n    def forward(self, x):\n        p = self.stem(x)\n        return p.flatten(2).transpose(1, 2)\n\n\nclass VisionTransformerPredictor(nn.Module):\n    \"\"\" Vision Transformer \"\"\"\n    def __init__(\n        self,\n        num_patches,\n        embed_dim=768,\n        predictor_embed_dim=384,\n        depth=6,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_std=0.02,\n        **kwargs\n    ):\n        super().__init__()\n        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        # --\n        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n                                                requires_grad=False)\n        predictor_pos_embed = get_1d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n                                                      int(num_patches),\n                                                      cls_token=False)\n        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n        # --\n        self.predictor_blocks = nn.ModuleList([\n            Block(\n                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n        self.predictor_norm = norm_layer(predictor_embed_dim)\n        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n        # ------\n        self.init_std = init_std\n        trunc_normal_(self.mask_token, std=self.init_std)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.predictor_blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=self.init_std)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            trunc_normal_(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x, masks_x, masks):\n        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n\n        if not isinstance(masks_x, list):\n            masks_x = [masks_x]\n\n        if not isinstance(masks, list):\n            masks = [masks]\n\n        # -- Batch Size\n        B = len(x) // len(masks_x)\n\n        # -- map from encoder-dim to pedictor-dim\n        x = self.predictor_embed(x)\n\n        # -- add positional embedding to x tokens\n        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n        x += apply_masks(x_pos_embed, masks_x)\n\n        _, N_ctxt, D = x.shape\n\n        # -- concat mask tokens to x\n        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n        pos_embs = apply_masks(pos_embs, masks)\n        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n        # --\n        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n        # --\n        pred_tokens += pos_embs\n        x = x.repeat(len(masks), 1, 1)\n        x = torch.cat([x, pred_tokens], dim=1)\n\n        # -- fwd prop\n        for blk in self.predictor_blocks:\n            x = blk(x)\n        x = self.predictor_norm(x)\n\n        # -- return preds for mask tokens\n        x = x[:, N_ctxt:]\n        x = self.predictor_proj(x)\n\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer \"\"\"\n    def __init__(\n        self,\n        timeline_pixel_width=(16*300),\n        patch_size=16,\n        in_chans=1,\n        embed_dim=768,\n        predictor_embed_dim=384,\n        depth=12,\n        predictor_depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_std=0.02,\n        **kwargs\n    ):\n        super().__init__()\n        self.num_features = self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        # --\n        self.patch_embed = PatchEmbed(\n            timeline_pixel_width=timeline_pixel_width,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n        # --\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1],\n                                            int(self.patch_embed.num_patches),\n                                            cls_token=False)\n        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n        # --\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n        # ------\n        self.init_std = init_std\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=self.init_std)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            trunc_normal_(m.weight, std=self.init_std)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x, masks=None):\n        if masks is not None:\n            if not isinstance(masks, list):\n                masks = [masks]\n\n        # -- patchify x\n        x = self.patch_embed(x)\n        B, N, D = x.shape\n\n        # -- add positional embedding to x\n        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n        x = x + pos_embed\n\n        # -- mask x\n        if masks is not None:\n            x = apply_masks(x, masks)\n\n        # -- fwd prop\n        for i, blk in enumerate(self.blocks):\n            x = blk(x)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        return x\n\n    def interpolate_pos_encoding(self, x, pos_embed):\n        npatch = x.shape[1] - 1\n        N = pos_embed.shape[1] - 1\n        if npatch == N:\n            return pos_embed\n        class_emb = pos_embed[:, 0]\n        pos_embed = pos_embed[:, 1:]\n        dim = x.shape[-1]\n        pos_embed = nn.functional.interpolate(\n            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n            scale_factor=math.sqrt(npatch / N),\n            mode='bicubic',\n        )\n        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n\n\ndef vit_predictor(**kwargs):\n    model = VisionTransformerPredictor(\n        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    return model\n\n\ndef vit_tiny(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_small(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_base(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_large(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_huge(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_giant(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\nVIT_EMBED_DIMS = {\n    'vit_tiny': 192,\n    'vit_small': 384,\n    'vit_base': 768,\n    'vit_large': 1024,\n    'vit_huge': 1280,\n    'vit_giant': 1408,\n}","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:52.858330Z","iopub.execute_input":"2025-06-16T15:22:52.858556Z","iopub.status.idle":"2025-06-16T15:22:52.905045Z","shell.execute_reply.started":"2025-06-16T15:22:52.858536Z","shell.execute_reply":"2025-06-16T15:22:52.904492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Helper functions","metadata":{}},{"cell_type":"code","source":"def load_checkpoint(\n    device,\n    r_path,\n    encoder,\n    predictor,\n    target_encoder,\n    opt,\n    scaler,\n    excluded_layers = None\n):\n\n    checkpoint = torch.load(r_path, map_location=torch.device('cpu'))\n    epoch = checkpoint['epoch']\n\n    # -- loading encoder with filtering\n    pretrained_dict = checkpoint['encoder']\n    \n    # Remove 'module.' prefix if it exists\n    new_pretrained_dict = {}\n    for k, v in pretrained_dict.items():\n        if k.startswith('module.'):\n            new_pretrained_dict[k[7:]] = v  # Remove 'module.' prefix\n        else:\n            new_pretrained_dict[k] = v\n    pretrained_dict = new_pretrained_dict\n    \n    # Apply excluded_layers filtering\n    if excluded_layers != None:\n        filtered_dict = {k: v for k, v in pretrained_dict.items() \n                        if not any(keyword in k for keyword in excluded_layers)}\n        print(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n        logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n        pretrained_dict = filtered_dict\n    msg = encoder.load_state_dict(pretrained_dict, strict=False)\n    print(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n    logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n\n    # -- loading predictor\n    pretrained_dict = checkpoint['predictor']\n    \n    # Remove 'module.' prefix if it exists\n    new_pretrained_dict = {}\n    for k, v in pretrained_dict.items():\n        if k.startswith('module.'):\n            new_pretrained_dict[k[7:]] = v  # Remove 'module.' prefix\n        else:\n            new_pretrained_dict[k] = v\n    pretrained_dict = new_pretrained_dict\n\n    # Apply excluded_layers filtering\n    if excluded_layers != None:\n        filtered_dict = {k: v for k, v in pretrained_dict.items() \n                        if not any(keyword in k for keyword in excluded_layers)}\n        print(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n        logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n        pretrained_dict = filtered_dict\n\n    msg = predictor.load_state_dict(pretrained_dict, strict=False)\n    logger.info(f'loaded pretrained predictor from epoch {epoch} with msg: {msg}')\n\n    # -- loading target_encoder\n    if target_encoder is not None:\n        pretrained_dict = checkpoint['target_encoder']\n        \n        # Remove 'module.' prefix if it exists\n        new_pretrained_dict = {}\n        for k, v in pretrained_dict.items():\n            if k.startswith('module.'):\n                new_pretrained_dict[k[7:]] = v  # Remove 'module.' prefix\n            else:\n                new_pretrained_dict[k] = v\n        pretrained_dict = new_pretrained_dict\n        \n        if excluded_layers != None:\n            filtered_dict = {k: v for k, v in pretrained_dict.items() \n                            if not any(keyword in k for keyword in excluded_layers)}\n            logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n            pretrained_dict = filtered_dict\n\n        msg = target_encoder.load_state_dict(pretrained_dict, strict=False)\n        logger.info(f'loaded pretrained target_encoder from epoch {epoch} with msg: {msg}')\n\n    # -- loading optimizer (no need to modify keys for optimizer)\n    opt.load_state_dict(checkpoint['opt'])\n    if scaler is not None and 'scaler' in checkpoint:\n        scaler.load_state_dict(checkpoint['scaler'])\n    logger.info(f'loaded optimizers from epoch {epoch}')\n    logger.info(f'read-path: {r_path}')\n    del checkpoint\n\n    # except Exception as e:\n    #     logger.info(f'Encountered exception when loading checkpoint: {e}')\n    #     epoch = 0\n\n    return encoder, predictor, target_encoder, opt, scaler, epoch\n\n\n\ndef init_model(\n    device,\n    patch_size=16,\n    model_name='vit_base',\n    timeline_pixel_width=16*300,\n    pred_depth=6,\n    pred_emb_dim=384\n):\n    encoder = vit_small(\n        timeline_pixel_width=timeline_pixel_width,\n        patch_size=patch_size)\n    predictor = vit_predictor(\n        num_patches=encoder.patch_embed.num_patches,\n        embed_dim=encoder.embed_dim,\n        predictor_embed_dim=pred_emb_dim,\n        depth=pred_depth,\n        num_heads=encoder.num_heads)\n\n    def init_weights(m):\n        if isinstance(m, torch.nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                torch.nn.init.constant_(m.bias, 0)\n        elif isinstance(m, torch.nn.LayerNorm):\n            torch.nn.init.constant_(m.bias, 0)\n            torch.nn.init.constant_(m.weight, 1.0)\n\n    for m in encoder.modules():\n        init_weights(m)\n\n    for m in predictor.modules():\n        init_weights(m)\n\n    encoder.to(device)\n    predictor.to(device)\n    logger.info(encoder)\n    return encoder, predictor\n\n\ndef init_opt(\n    encoder,\n    predictor,\n    iterations_per_epoch,\n    start_lr,\n    ref_lr,\n    warmup,\n    num_epochs,\n    wd=1e-6,\n    final_wd=1e-6,\n    final_lr=0.0,\n    use_bfloat16=False,\n    ipe_scale=1.25\n):\n    param_groups = [\n        {\n            'params': (p for n, p in encoder.named_parameters()\n                       if ('bias' not in n) and (len(p.shape) != 1))\n        }, {\n            'params': (p for n, p in predictor.named_parameters()\n                       if ('bias' not in n) and (len(p.shape) != 1))\n        }, {\n            'params': (p for n, p in encoder.named_parameters()\n                       if ('bias' in n) or (len(p.shape) == 1)),\n            'WD_exclude': True,\n            'weight_decay': 0\n        }, {\n            'params': (p for n, p in predictor.named_parameters()\n                       if ('bias' in n) or (len(p.shape) == 1)),\n            'WD_exclude': True,\n            'weight_decay': 0\n        }\n    ]\n\n    logger.info('Using AdamW')\n    optimizer = torch.optim.AdamW(param_groups)\n    scheduler = WarmupCosineSchedule(\n        optimizer,\n        warmup_steps=int(warmup*iterations_per_epoch),\n        start_lr=start_lr,\n        ref_lr=ref_lr,\n        final_lr=final_lr,\n        T_max=int(ipe_scale*num_epochs*iterations_per_epoch))\n    wd_scheduler = CosineWDSchedule(\n        optimizer,\n        ref_wd=wd,\n        final_wd=final_wd,\n        T_max=int(ipe_scale*num_epochs*iterations_per_epoch))\n    scaler = torch.cuda.amp.GradScaler() if use_bfloat16 else None\n    return optimizer, scaler, scheduler, wd_scheduler","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:52.905676Z","iopub.execute_input":"2025-06-16T15:22:52.905832Z","iopub.status.idle":"2025-06-16T15:22:53.003074Z","shell.execute_reply.started":"2025-06-16T15:22:52.905820Z","shell.execute_reply":"2025-06-16T15:22:53.002315Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# encoder, predictor = init_model(\n#     device=device,\n#     patch_size=patch_size,\n#     timeline_pixel_width=300*16,\n#     pred_depth=pred_depth,\n#     pred_emb_dim=pred_emb_dim,\n#     model_name=model_name)\n# target_encoder = copy.deepcopy(encoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:53.003872Z","iopub.execute_input":"2025-06-16T15:22:53.004129Z","iopub.status.idle":"2025-06-16T15:22:53.023655Z","shell.execute_reply.started":"2025-06-16T15:22:53.004111Z","shell.execute_reply":"2025-06-16T15:22:53.023007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# r_path = load_path\n\n# checkpoint = torch.load(r_path, map_location=torch.device('cpu'))\n# epoch = checkpoint['epoch']\n\n# # -- loading encoder with filtering\n# pretrained_dict = checkpoint['encoder']\n\n# # Remove 'module.' prefix if it exists\n# new_pretrained_dict = {}\n# for k, v in pretrained_dict.items():\n#     if k.startswith('module.'):\n#         new_pretrained_dict[k[7:]] = v  # Remove 'module.' prefix\n#     else:\n#         new_pretrained_dict[k] = v\n# pretrained_dict = new_pretrained_dict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:53.024531Z","iopub.execute_input":"2025-06-16T15:22:53.024864Z","iopub.status.idle":"2025-06-16T15:22:53.038272Z","shell.execute_reply.started":"2025-06-16T15:22:53.024842Z","shell.execute_reply":"2025-06-16T15:22:53.037670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pretrained_dict.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:53.038893Z","iopub.execute_input":"2025-06-16T15:22:53.039131Z","iopub.status.idle":"2025-06-16T15:22:53.053266Z","shell.execute_reply.started":"2025-06-16T15:22:53.039108Z","shell.execute_reply":"2025-06-16T15:22:53.052702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# excluded_layers = [\"pos_embed\", \"patch_embed\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:53.053862Z","iopub.execute_input":"2025-06-16T15:22:53.054118Z","iopub.status.idle":"2025-06-16T15:22:53.067633Z","shell.execute_reply.started":"2025-06-16T15:22:53.054095Z","shell.execute_reply":"2025-06-16T15:22:53.067044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filtered_dict = {k: v for k, v in pretrained_dict.items() \n#                 if not any(keyword in k for keyword in excluded_layers)}\n# print(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n# logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n# pretrained_dict = filtered_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:53.068283Z","iopub.execute_input":"2025-06-16T15:22:53.068457Z","iopub.status.idle":"2025-06-16T15:22:53.082856Z","shell.execute_reply.started":"2025-06-16T15:22:53.068444Z","shell.execute_reply":"2025-06-16T15:22:53.082362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pretrained_dict.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:22:53.083461Z","iopub.execute_input":"2025-06-16T15:22:53.083733Z","iopub.status.idle":"2025-06-16T15:22:53.101379Z","shell.execute_reply.started":"2025-06-16T15:22:53.083717Z","shell.execute_reply":"2025-06-16T15:22:53.100789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WarmupCosineSchedule(object):\n\n    def __init__(\n        self,\n        optimizer,\n        warmup_steps,\n        start_lr,\n        ref_lr,\n        T_max,\n        last_epoch=-1,\n        final_lr=0.\n    ):\n        self.optimizer = optimizer\n        self.start_lr = start_lr\n        self.ref_lr = ref_lr\n        self.final_lr = final_lr\n        self.warmup_steps = warmup_steps\n        self.T_max = T_max - warmup_steps\n        self._step = 0.\n\n    def step(self):\n        self._step += 1\n        if self._step < self.warmup_steps:\n            progress = float(self._step) / float(max(1, self.warmup_steps))\n            new_lr = self.start_lr + progress * (self.ref_lr - self.start_lr)\n        else:\n            # -- progress after warmup\n            progress = float(self._step - self.warmup_steps) / float(max(1, self.T_max))\n            new_lr = max(self.final_lr,\n                         self.final_lr + (self.ref_lr - self.final_lr) * 0.5 * (1. + math.cos(math.pi * progress)))\n\n        for group in self.optimizer.param_groups:\n            group['lr'] = new_lr\n\n        return new_lr\n\n\nclass CosineWDSchedule(object):\n\n    def __init__(\n        self,\n        optimizer,\n        ref_wd,\n        T_max,\n        final_wd=0.\n    ):\n        self.optimizer = optimizer\n        self.ref_wd = ref_wd\n        self.final_wd = final_wd\n        self.T_max = T_max\n        self._step = 0.\n\n    def step(self):\n        self._step += 1\n        progress = self._step / self.T_max\n        new_wd = self.final_wd + (self.ref_wd - self.final_wd) * 0.5 * (1. + math.cos(math.pi * progress))\n\n        if self.final_wd <= self.ref_wd:\n            new_wd = max(self.final_wd, new_wd)\n        else:\n            new_wd = min(self.final_wd, new_wd)\n\n        for group in self.optimizer.param_groups:\n            if ('WD_exclude' not in group) or not group['WD_exclude']:\n                group['weight_decay'] = new_wd\n        return new_wd","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:53.102089Z","iopub.execute_input":"2025-06-16T15:22:53.102568Z","iopub.status.idle":"2025-06-16T15:22:53.116786Z","shell.execute_reply.started":"2025-06-16T15:22:53.102528Z","shell.execute_reply":"2025-06-16T15:22:53.116224Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initializing model","metadata":{}},{"cell_type":"code","source":"# -- init model\nencoder, predictor = init_model(\n    device=device,\n    patch_size=patch_size,\n    timeline_pixel_width=300*16,\n    pred_depth=pred_depth,\n    pred_emb_dim=pred_emb_dim,\n    model_name=model_name)\ntarget_encoder = copy.deepcopy(encoder)","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:22:53.117553Z","iopub.execute_input":"2025-06-16T15:22:53.117789Z","iopub.status.idle":"2025-06-16T15:23:08.653739Z","shell.execute_reply.started":"2025-06-16T15:22:53.117773Z","shell.execute_reply":"2025-06-16T15:23:08.653158Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load_model = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:23:08.654344Z","iopub.execute_input":"2025-06-16T15:23:08.654537Z","iopub.status.idle":"2025-06-16T15:23:08.657986Z","shell.execute_reply.started":"2025-06-16T15:23:08.654521Z","shell.execute_reply":"2025-06-16T15:23:08.657298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -- init optimizer and scheduler\noptimizer, scaler, scheduler, wd_scheduler = init_opt(\n    encoder=encoder,\n    predictor=predictor,\n    wd=wd,\n    final_wd=final_wd,\n    start_lr=start_lr,\n    ref_lr=lr,\n    final_lr=final_lr,\n    iterations_per_epoch=ipe,\n    warmup=warmup,\n    num_epochs=num_epochs,\n    ipe_scale=ipe_scale,\n    use_bfloat16=use_bfloat16)\n    # encoder = DistributedDataParallel(encoder, static_graph=True)\n    # predictor = DistributedDataParallel(predictor, static_graph=True)\n    # target_encoder = DistributedDataParallel(target_encoder)\nfor p in target_encoder.parameters():\n    p.requires_grad = False\n\n# -- momentum schedule\nmomentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n                      for i in range(int(ipe*num_epochs*ipe_scale)+1))\n\nstart_epoch = 0\n# -- load training checkpoint\nif load_model:\n    encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n        device=device,\n        r_path=load_path,\n        encoder=encoder,\n        predictor=predictor,\n        target_encoder=target_encoder,\n        opt=optimizer,\n        scaler=scaler,\n        excluded_layers = [\"pos_embed\", \"patch_embed\"])\n    # for _ in range(start_epoch*ipe):\n    #     scheduler.step()\n    #     wd_scheduler.step()\n    #     next(momentum_scheduler)\n    #     mask_collator.step()\n\ndef save_checkpoint(epoch):\n    save_dict = {\n        'encoder': encoder.state_dict(),\n        'predictor': predictor.state_dict(),\n        'target_encoder': target_encoder.state_dict(),\n        'opt': optimizer.state_dict(),\n        'scaler': None if scaler is None else scaler.state_dict(),\n        'epoch': epoch,\n        'loss': loss_meter.avg,\n        'batch_size': batch_size,\n        'lr': lr\n    }\n    if rank == 0:\n        torch.save(save_dict, latest_path) \n        if (epoch + 1) % checkpoint_freq == 0:\n            torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:23:08.658785Z","iopub.execute_input":"2025-06-16T15:23:08.659109Z","iopub.status.idle":"2025-06-16T15:23:09.633170Z","shell.execute_reply.started":"2025-06-16T15:23:08.659085Z","shell.execute_reply":"2025-06-16T15:23:09.632438Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#encoder.state_dict().keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:23:09.633865Z","iopub.execute_input":"2025-06-16T15:23:09.634143Z","iopub.status.idle":"2025-06-16T15:23:09.637968Z","shell.execute_reply.started":"2025-06-16T15:23:09.634127Z","shell.execute_reply":"2025-06-16T15:23:09.637230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(epoch):\n    \"\"\"Run validation on the validation dataset and return average loss.\"\"\"\n    logger.info('Running validation...')\n    val_loss_meter = AverageMeter()\n    val_maskA_meter = AverageMeter()\n    val_maskB_meter = AverageMeter()\n    val_time_meter = AverageMeter()\n    \n    # Set models to eval mode\n    encoder.eval()\n    predictor.eval()\n    target_encoder.eval()\n    \n    with torch.no_grad():\n        for itr, (udata, masks_enc, masks_pred) in enumerate(val_loader):\n            # Load and process images\n            imgs = udata.to(device, non_blocking=True)\n            masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n            masks_2 = [u.unsqueeze(-1).to(device, non_blocking=True) for u in masks_pred]\n            \n            val_maskA_meter.update(len(masks_1[0][0]))\n            val_maskB_meter.update(1)\n            \n            # Forward pass\n            def val_step():\n                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n                    # Target encoding\n                    h = target_encoder(imgs)\n                    h = F.layer_norm(h, (h.size(-1),))\n                    B = len(h)\n                    h = apply_masks(h, masks_2)\n                    h = repeat_interleave_batch(h, B, repeat=len(masks_1))\n                    \n                    # Context encoding and prediction\n                    z = encoder(imgs, masks_1)\n                    z = predictor(z, masks_1, masks_2)\n                    \n                    # Loss calculation\n                    loss = F.smooth_l1_loss(z, h)\n                    return float(loss)\n            \n            loss, etime = gpu_timer(val_step)\n            val_loss_meter.update(loss)\n            val_time_meter.update(etime)\n            \n            # Log progress occasionally\n            if itr % (log_freq * 2) == 0:\n                logger.info(f'Val: [{epoch + 1}, {itr}] loss: {val_loss_meter.avg:.3f} '\n                           f'masks: {val_maskA_meter.avg:.1f} {val_maskB_meter.avg:.1f} '\n                           f'({val_time_meter.avg:.1f} ms)')\n    \n    # Set models back to training mode\n    encoder.train()\n    predictor.train()\n    \n    return val_loss_meter.avg","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:23:09.638719Z","iopub.execute_input":"2025-06-16T15:23:09.639513Z","iopub.status.idle":"2025-06-16T15:23:09.657399Z","shell.execute_reply.started":"2025-06-16T15:23:09.639486Z","shell.execute_reply":"2025-06-16T15:23:09.656650Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train loop","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:23:09.658162Z","iopub.execute_input":"2025-06-16T15:23:09.658790Z","iopub.status.idle":"2025-06-16T15:23:09.676103Z","shell.execute_reply.started":"2025-06-16T15:23:09.658772Z","shell.execute_reply":"2025-06-16T15:23:09.675531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING = 1\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:23:09.676895Z","iopub.execute_input":"2025-06-16T15:23:09.677127Z","iopub.status.idle":"2025-06-16T15:23:09.690662Z","shell.execute_reply.started":"2025-06-16T15:23:09.677106Z","shell.execute_reply":"2025-06-16T15:23:09.689964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add this configuration near your other parameters\nval_frequency = 300  # Run validation every 400 training steps\nbest_val_loss = float('inf')","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:23:09.691304Z","iopub.execute_input":"2025-06-16T15:23:09.691500Z","iopub.status.idle":"2025-06-16T15:23:09.704826Z","shell.execute_reply.started":"2025-06-16T15:23:09.691486Z","shell.execute_reply":"2025-06-16T15:23:09.704286Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING = 1\n# -- TRAINING LOOP\nglobal_step = 0\nfor epoch in range(start_epoch, num_epochs):\n    logger.info('Epoch %d' % (epoch + 1))\n\n    loss_meter = AverageMeter()\n    maskA_meter = AverageMeter()\n    maskB_meter = AverageMeter()\n    time_meter = AverageMeter()\n\n    for itr, (udata, masks_enc, masks_pred) in enumerate(data_loader):\n        new_validation = False\n        # print(masks_enc[0].shape)\n        # print(masks_pred[0].shape)\n        # print(masks_pred[0][0])\n        # print(udata.shape)\n        if global_step % val_frequency == 0:\n            new_validation = True\n            # Run validation and check if we need to save the model\n            val_loss = validate(epoch)\n            print(f\"Validation loss: {val_loss}\")\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                save_checkpoint(epoch)\n        \n        def load_imgs():\n            # -- unsupervised imgs\n            imgs = udata.to(device, non_blocking=True) # udata[0]\n            masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n            masks_2 = [u.unsqueeze(-1).to(device, non_blocking=True) for u in masks_pred]\n            return (imgs, masks_1, masks_2)\n        imgs, masks_enc, masks_pred = load_imgs()\n        maskA_meter.update(len(masks_enc[0][0]))\n        maskB_meter.update(1)\n\n        def train_step():\n            _new_lr = scheduler.step()\n            _new_wd = wd_scheduler.step()\n            # --\n\n            def forward_target():\n                with torch.no_grad():\n                    h = target_encoder(imgs)\n                    h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n                    B = len(h)\n                    # -- create targets (masked regions of h)\n                    h = apply_masks(h, masks_pred)\n                    h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n                    return h\n\n            def forward_context():\n                z = encoder(imgs, masks_enc)\n                z = predictor(z, masks_enc, masks_pred)\n                return z\n\n            def loss_fn(z, h):\n                loss = F.smooth_l1_loss(z, h)\n                return loss\n\n            # Step 1. Forward\n            with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n                h = forward_target()\n                z = forward_context()\n                loss = loss_fn(z, h)\n\n            #  Step 2. Backward & step\n            if use_bfloat16:\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                optimizer.step()\n            # grad_stats = grad_logger(encoder.named_parameters())\n            optimizer.zero_grad()\n\n            # Step 3. momentum update of target encoder\n            with torch.no_grad():\n                m = next(momentum_scheduler)\n                for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n                    param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n\n            return (float(loss), _new_lr, _new_wd, None)#grad_stats)\n        (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n        loss_meter.update(loss)\n        time_meter.update(etime)\n\n        # -- Logging\n        def log_stats():\n            if new_validation:\n                log_val_value = val_loss\n            else:\n                log_val_value = -1\n            csv_logger.log(epoch + 1, itr, loss, log_val_value, maskA_meter.val, maskB_meter.val, etime)\n            if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n                logger.info('[%d, %5d] loss: %.3f '\n                            'masks: %.1f %.1f '\n                            '[wd: %.2e] [lr: %.2e] '\n                            '[mem: %.2e] '\n                            '(%.1f ms)'\n                            % (epoch + 1, itr,\n                               loss_meter.avg,\n                               maskA_meter.avg,\n                               maskB_meter.avg,\n                               _new_wd,\n                               _new_lr,\n                               torch.cuda.max_memory_allocated() / 1024.**2,\n                               time_meter.avg))\n                print(f\"loss: {loss_meter.avg}, maskA: {maskA_meter.avg}, maskB: {maskB_meter.avg}\")\n\n                if grad_stats is not None:\n                    logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n                                % (epoch + 1, itr,\n                                   grad_stats.first_layer,\n                                   grad_stats.last_layer,\n                                   grad_stats.min,\n                                   grad_stats.max))\n\n        log_stats()\n\n        assert not np.isnan(loss), 'loss is nan'\n\n        global_step += 1\n\n    # -- Save Checkpoint after every epoch\n    logger.info('avg. loss %.3f' % loss_meter.avg)\n    save_checkpoint(epoch+1)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:23:09.705539Z","iopub.execute_input":"2025-06-16T15:23:09.705760Z","iopub.status.idle":"2025-06-16T15:25:49.183535Z","shell.execute_reply.started":"2025-06-16T15:23:09.705735Z","shell.execute_reply":"2025-06-16T15:25:49.182438Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_checkpoint(epoch, global_step=None, val_loss=None):\n    save_dict = {\n        'encoder': encoder.state_dict(),\n        'predictor': predictor.state_dict(),\n        'target_encoder': target_encoder.state_dict(),\n        'opt': optimizer.state_dict(),\n        'scaler': None if scaler is None else scaler.state_dict(),\n        'epoch': epoch,\n        'step': global_step,\n        'train_loss': loss_meter.avg,\n        'val_loss': val_loss,\n        'batch_size': batch_size,\n        'lr': lr\n    }\n    if rank == 0:\n        torch.save(save_dict, latest_path)\n        if (epoch + 1) % checkpoint_freq == 0:\n            torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))","metadata":{"execution":{"iopub.status.busy":"2025-06-16T15:25:49.184187Z","iopub.status.idle":"2025-06-16T15:25:49.184449Z","shell.execute_reply.started":"2025-06-16T15:25:49.184339Z","shell.execute_reply":"2025-06-16T15:25:49.184350Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}