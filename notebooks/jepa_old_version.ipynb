{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I-JEPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:37.818929Z",
     "iopub.status.busy": "2025-06-06T00:21:37.818624Z",
     "iopub.status.idle": "2025-06-06T00:21:37.823031Z",
     "shell.execute_reply": "2025-06-06T00:21:37.822489Z",
     "shell.execute_reply.started": "2025-06-06T00:21:37.818911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# core train\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:37.834636Z",
     "iopub.status.busy": "2025-06-06T00:21:37.834410Z",
     "iopub.status.idle": "2025-06-06T00:21:37.840539Z",
     "shell.execute_reply": "2025-06-06T00:21:37.839940Z",
     "shell.execute_reply.started": "2025-06-06T00:21:37.834622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Basic global config\n",
    "_GLOBAL_SEED = 0\n",
    "np.random.seed(_GLOBAL_SEED)\n",
    "torch.manual_seed(_GLOBAL_SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:37.874413Z",
     "iopub.status.busy": "2025-06-06T00:21:37.873857Z",
     "iopub.status.idle": "2025-06-06T00:21:37.879379Z",
     "shell.execute_reply": "2025-06-06T00:21:37.878830Z",
     "shell.execute_reply.started": "2025-06-06T00:21:37.874397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data\": {\n",
    "        \"batch_size\": 1,\n",
    "        \"image_folders\": ['/kaggle/input/fmri-train-1-normalized/data/noisy_func_train_1.npy',\n",
    "                '/kaggle/input/fmri-train-2-normalized/data/noisy_func_train_2.npy',\n",
    "                '/kaggle/input/fmri-train-3-normalized/data/noisy_func_train_3.npy'],\n",
    "        \"validation_folders\": ['/kaggle/input/fmri-val-normalized/data/noisy_func_val.npy'],\n",
    "        \"num_workers\": 2,\n",
    "        \"pin_mem\": True,\n",
    "        \"root_path\": \"/kaggle/input\",\n",
    "        \"use_horizontal_flip\": False\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"folder\": \"/kaggle/working/logs\",\n",
    "        \"write_tag\": \"jepa\"\n",
    "    },\n",
    "    \"mask\": {\n",
    "        \"allow_overlap\": False,\n",
    "        \"aspect_ratio\": [0.75, 1.5],\n",
    "        \"enc_mask_scale\": [0.85, 1.0],\n",
    "        \"min_keep\": 10,\n",
    "        \"num_enc_masks\": 1,\n",
    "        \"num_pred_masks\": 1,\n",
    "        \"patch_size\": 16,\n",
    "        \"pred_mask_scale\": [0.15, 0.2]\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"copy_data\": False,\n",
    "        \"load_checkpoint\": False,\n",
    "        \"model_name\": \"vit_huge\",\n",
    "        \"pred_depth\": 12,\n",
    "        \"pred_emb_dim\": 384,\n",
    "        \"read_checkpoint\": None,\n",
    "        \"use_bfloat16\": True\n",
    "    },\n",
    "    \"optimization\": {\n",
    "        \"ema\": [0.996, 1.0],\n",
    "        \"epochs\": 2,\n",
    "        \"final_lr\": 1.0e-5,\n",
    "        \"final_weight_decay\": 0.4,\n",
    "        \"ipe_scale\": 1.0,\n",
    "        \"lr\": 0.001,\n",
    "        \"start_lr\": 0.0002,\n",
    "        \"warmup\": 20,\n",
    "        \"weight_decay\": 0.04\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.336168Z",
     "iopub.status.busy": "2025-06-06T00:21:38.335898Z",
     "iopub.status.idle": "2025-06-06T00:21:38.347795Z",
     "shell.execute_reply": "2025-06-06T00:21:38.347279Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.336151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "resume_preempt = False\n",
    "rank = 0\n",
    "\n",
    "# -- META\n",
    "use_bfloat16 = args['meta']['use_bfloat16']\n",
    "model_name = args['meta']['model_name']\n",
    "load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
    "r_file = args['meta']['read_checkpoint']\n",
    "copy_data = args['meta']['copy_data']\n",
    "pred_depth = args['meta']['pred_depth']\n",
    "pred_emb_dim = args['meta']['pred_emb_dim']\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "# -- DATA\n",
    "batch_size = args['data']['batch_size']\n",
    "pin_mem = args['data']['pin_mem']\n",
    "num_workers = args['data']['num_workers']\n",
    "root_path = args['data']['root_path']\n",
    "image_folders = args['data']['image_folders']\n",
    "validation_folders = args['data']['validation_folders']\n",
    "# --\n",
    "\n",
    "# -- MASK\n",
    "allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
    "patch_size = args['mask']['patch_size']  # patch-size for model training\n",
    "num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
    "min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
    "enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
    "num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
    "pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
    "aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
    "# --\n",
    "\n",
    "# -- OPTIMIZATION\n",
    "ema = args['optimization']['ema']\n",
    "ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
    "wd = float(args['optimization']['weight_decay'])\n",
    "final_wd = float(args['optimization']['final_weight_decay'])\n",
    "num_epochs = args['optimization']['epochs']\n",
    "warmup = args['optimization']['warmup']\n",
    "start_lr = args['optimization']['start_lr']\n",
    "lr = args['optimization']['lr']\n",
    "final_lr = args['optimization']['final_lr']\n",
    "\n",
    "# -- LOGGING\n",
    "folder = args['logging']['folder']\n",
    "tag = args['logging']['write_tag']\n",
    "\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "dump = os.path.join(folder, 'params-ijepa.yaml')\n",
    "with open(dump, 'w') as f:\n",
    "    yaml.dump(args, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.349538Z",
     "iopub.status.busy": "2025-06-06T00:21:38.349265Z",
     "iopub.status.idle": "2025-06-06T00:21:38.364111Z",
     "shell.execute_reply": "2025-06-06T00:21:38.363483Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.349522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "log_timings = True\n",
    "log_freq = 10\n",
    "checkpoint_freq = 1\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.365092Z",
     "iopub.status.busy": "2025-06-06T00:21:38.364870Z",
     "iopub.status.idle": "2025-06-06T00:21:38.378959Z",
     "shell.execute_reply": "2025-06-06T00:21:38.378393Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.365070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gpu_timer(closure, log_timings=True):\n",
    "    \"\"\" Helper to time gpu-time to execute closure() \"\"\"\n",
    "    log_timings = log_timings and torch.cuda.is_available()\n",
    "\n",
    "    elapsed_time = -1.\n",
    "    if log_timings:\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "\n",
    "    result = closure()\n",
    "\n",
    "    if log_timings:\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_time = start.elapsed_time(end)\n",
    "\n",
    "    return result, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.379851Z",
     "iopub.status.busy": "2025-06-06T00:21:38.379615Z",
     "iopub.status.idle": "2025-06-06T00:21:38.394604Z",
     "shell.execute_reply": "2025-06-06T00:21:38.393996Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.379822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CSVLogger(object):\n",
    "\n",
    "    def __init__(self, fname, *argv):\n",
    "        self.fname = fname\n",
    "        self.types = []\n",
    "        # -- print headers\n",
    "        with open(self.fname, '+a') as f:\n",
    "            for i, v in enumerate(argv, 1):\n",
    "                self.types.append(v[0])\n",
    "                if i < len(argv):\n",
    "                    print(v[1], end=',', file=f)\n",
    "                else:\n",
    "                    print(v[1], end='\\n', file=f)\n",
    "\n",
    "    def log(self, *argv):\n",
    "        with open(self.fname, '+a') as f:\n",
    "            for i, tv in enumerate(zip(self.types, argv), 1):\n",
    "                end = ',' if i < len(argv) else '\\n'\n",
    "                print(tv[0] % tv[1], end=end, file=f)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.max = float('-inf')\n",
    "        self.min = float('inf')\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        try:\n",
    "            self.max = max(val, self.max)\n",
    "            self.min = min(val, self.min)\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.395940Z",
     "iopub.status.busy": "2025-06-06T00:21:38.395750Z",
     "iopub.status.idle": "2025-06-06T00:21:38.407749Z",
     "shell.execute_reply": "2025-06-06T00:21:38.407115Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.395925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -- log/checkpointing paths\n",
    "log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
    "save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
    "latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
    "load_path = None\n",
    "if load_model:\n",
    "    load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
    "\n",
    "# csv logger\n",
    "csv_logger = CSVLogger(log_file,\n",
    "                       ('%d', 'epoch'),\n",
    "                       ('%d', 'itr'),\n",
    "                       ('%.5f', 'train_loss'),\n",
    "                       ('%.5f', 'val_loss'),  # Added validation loss\n",
    "                       ('%.5f', 'mask-A'),\n",
    "                       ('%.5f', 'mask-B'),\n",
    "                       ('%d', 'time (ms)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.408608Z",
     "iopub.status.busy": "2025-06-06T00:21:38.408365Z",
     "iopub.status.idle": "2025-06-06T00:21:38.425550Z",
     "shell.execute_reply": "2025-06-06T00:21:38.424923Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.408589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from multiprocessing import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesMaskCollator:\n",
    "    def __init__(self, num_frames=300, frame_size=16, nenc=1, npred=1):\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.height = frame_size  # fixed spatial height of a frame\n",
    "        self.width = num_frames * frame_size  # time flows horizontally\n",
    "        self.nenc = nenc\n",
    "        self.npred = npred\n",
    "        self._itr_counter = Value('i', -1)\n",
    "\n",
    "    def step(self):\n",
    "        i = self._itr_counter\n",
    "        with i.get_lock():\n",
    "            i.value += 1\n",
    "            v = i.value\n",
    "        return v\n",
    "    \n",
    "    def collate_merge_batches(self, batch):\n",
    "        merged = torch.cat([item for item in batch], dim=0)\n",
    "        return merged\n",
    "\n",
    "    def _sample_frame_mask(self, generator, exclude_frames=None):\n",
    "        # Build list of available frame indices\n",
    "        choices = torch.tensor(\n",
    "            [i for i in range(self.num_frames) if (exclude_frames is None or i not in exclude_frames)],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        # Sample one index using the generator\n",
    "        idx = torch.randint(0, len(choices), (1,), generator=generator).item()\n",
    "        frame_idx = choices[idx]\n",
    "\n",
    "        # Calculate top-left corner in new layout (horizontal stacking)\n",
    "        top = 0\n",
    "        left = frame_idx * self.frame_size\n",
    "        mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
    "        mask[top:top+self.frame_size, left:left+self.frame_size] = 1\n",
    "        return mask, frame_idx\n",
    "\n",
    "    def build_encoder_mask_from_pred(self, pred_masks):\n",
    "        enc_mask = torch.ones((self.height, self.width), dtype=torch.int32)\n",
    "        for pred_mask in pred_masks:\n",
    "            enc_mask.view(-1)[pred_mask] = 0  # Zero out the masked regions\n",
    "        return torch.nonzero(enc_mask.flatten()).squeeze()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        collated_batch = self.collate_merge_batches(batch)\n",
    "        B = collated_batch.shape[0]\n",
    "\n",
    "        seed = self.step()\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "\n",
    "        collated_masks_enc, collated_masks_pred = [], []\n",
    "\n",
    "        for _ in range(B):\n",
    "            pred_masks = []\n",
    "            pred_frame_idxs = []\n",
    "            for _ in range(self.npred):\n",
    "                mask, idx = self._sample_frame_mask(generator=g, exclude_frames=pred_frame_idxs)\n",
    "                pred_masks.append(torch.nonzero(mask.flatten()).squeeze())\n",
    "                pred_frame_idxs.append(idx)\n",
    "\n",
    "            collated_masks_pred.append(pred_masks)\n",
    "\n",
    "            enc_masks = []\n",
    "            for _ in range(self.nenc):\n",
    "                enc_mask = self.build_encoder_mask_from_pred(pred_masks)\n",
    "                enc_masks.append(enc_mask)\n",
    "\n",
    "            collated_masks_enc.append(enc_masks)\n",
    "\n",
    "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
    "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
    "\n",
    "        return collated_batch, collated_masks_enc, collated_masks_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.560462Z",
     "iopub.status.busy": "2025-06-06T00:21:38.560249Z",
     "iopub.status.idle": "2025-06-06T00:21:38.576906Z",
     "shell.execute_reply": "2025-06-06T00:21:38.576258Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.560448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_masks(x, masks):\n",
    "    \"\"\"\n",
    "    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
    "    :param masks: list of tensors containing indices of patches in [N] to keep\n",
    "    \"\"\"\n",
    "    all_x = []\n",
    "    for m in masks:\n",
    "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
    "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
    "    return torch.cat(all_x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_collator = TimeSeriesMaskCollator() # defaults to 300 frames of size 16x16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.608487Z",
     "iopub.status.busy": "2025-06-06T00:21:38.608278Z",
     "iopub.status.idle": "2025-06-06T00:21:38.623514Z",
     "shell.execute_reply": "2025-06-06T00:21:38.622773Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.608473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch # Import PyTorch\n",
    "\n",
    "class Compose:\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of callables): list of transforms to compose.\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img_array):\n",
    "        \"\"\"\n",
    "        Applies the composed transforms to the input array.\n",
    "        The input can be a NumPy array. The output will be a torch.Tensor\n",
    "        if torch.from_numpy is the last transform.\n",
    "        Args:\n",
    "            img_array (numpy.ndarray): Input image array (C, H, W).\n",
    "        Returns:\n",
    "            torch.Tensor: Transformed image tensor.\n",
    "        \"\"\"\n",
    "        for t in self.transforms:\n",
    "            img_array = t(img_array) # Note: img_array will become a torch.Tensor at the end\n",
    "        return img_array\n",
    "\n",
    "\n",
    "# Build the transform list\n",
    "transform_list = []\n",
    "\n",
    "def reshape_to_2d_timeline(patch_time_series):\n",
    "    # Convert to PyTorch tensor immediately for permute/reshape\n",
    "    patch_time_series_tensor = torch.from_numpy(patch_time_series).float()\n",
    "    height_patch = patch_time_series_tensor.shape[0]  # H\n",
    "    width_patch = patch_time_series_tensor.shape[1]   # W\n",
    "    total_time_steps = patch_time_series_tensor.shape[2]  # T   \n",
    "\n",
    "    \n",
    "    # Apply the corrected reshaping logic here\n",
    "    # Original tensor shape (H, W, T) -> (16, 16, total_time_steps)\n",
    "    # Permute to (H, T, W)\n",
    "    intermediate_tensor = patch_time_series_tensor.permute(0, 2, 1)\n",
    "    \n",
    "    sample = intermediate_tensor.reshape(height_patch, total_time_steps * width_patch)\n",
    "\n",
    "    # Add a channel dimension: (1, H, T*W)\n",
    "    sample = sample.unsqueeze(0)\n",
    "\n",
    "transform_list += [reshape_to_2d_timeline]  # Add reshape transform\n",
    "transform_list += [lambda x: x.unsqueeze(0)] # add batch dimension\n",
    "\n",
    "# Composition of transforms\n",
    "transform = Compose(transform_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, noisy_images_paths: list, stats, transform=None, normalize=True):\n",
    "        \"\"\"Initialize fMRI dataset for denoising with memory-efficient loading,\n",
    "        extracting 16x16 patches with full time series per depth channel.\n",
    "\n",
    "        Args:\n",
    "            noisy_images_paths (list): List of paths to noisy fMRI volumes (.npy files)\n",
    "            stats: Dictionary containing 'mean' and 'std' for normalization.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            normalize (bool): Whether to apply channel-wise normalization.\n",
    "        \"\"\"\n",
    "        self.noisy_paths = noisy_images_paths\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.stats = stats\n",
    "\n",
    "        self.file_slice_mapping = [] # Stores (file_idx, z_idx, patch_y_idx, patch_x_idx)\n",
    "\n",
    "        patch_size = 16\n",
    "        dataset_length = 0\n",
    "\n",
    "        for i, path in enumerate(noisy_images_paths):\n",
    "            # Load metadata about the file shape without loading full content\n",
    "            data_shape = np.load(path, mmap_mode='r').shape\n",
    "            # Assuming data_shape is (H, W, Z, T)\n",
    "            total_height, total_width, depth_channels, total_time_steps = data_shape\n",
    "\n",
    "            # Calculate how many patches fit along each spatial dimension\n",
    "            num_patches_width = total_width // patch_size\n",
    "            num_patches_height = total_height // patch_size\n",
    "\n",
    "            # Iterate over all depth channels (Z)\n",
    "            for z_idx in range(depth_channels):\n",
    "                # Iterate through the spatial grid of patches\n",
    "                for patch_y_idx in range(num_patches_height):\n",
    "                    for patch_x_idx in range(num_patches_width):\n",
    "                        # Each combination of (file, z_idx, patch_y_idx, patch_x_idx) is a unique item\n",
    "                        self.file_slice_mapping.append((i, z_idx, patch_y_idx, patch_x_idx))\n",
    "                        dataset_length += 1\n",
    "\n",
    "        self.data_len = dataset_length\n",
    "        self.patch_size = patch_size # Store patch_size for __getitem__\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Use the mapping to determine which file, depth channel, and spatial patch to load\n",
    "        file_idx, z_idx, patch_y_idx, patch_x_idx = self.file_slice_mapping[index]\n",
    "\n",
    "        # Load data from the specific file\n",
    "        noisy_file_path = self.noisy_paths[file_idx]\n",
    "\n",
    "        # Load the full 4D array with mmap_mode to avoid loading everything into RAM\n",
    "        noisy_volume = np.load(noisy_file_path, mmap_mode='r')\n",
    "\n",
    "        # Calculate the starting and ending coordinates for the current patch\n",
    "        start_h = patch_y_idx * self.patch_size\n",
    "        end_h = start_h + self.patch_size\n",
    "        start_w = patch_x_idx * self.patch_size\n",
    "        end_w = start_w + self.patch_size\n",
    "\n",
    "        # Extract the 16x16 patch for the specific depth channel (z_idx)\n",
    "        # and include the entire time series.\n",
    "        # The resulting shape will be (patch_size, patch_size, time_steps)\n",
    "        patch_time_series = noisy_volume[start_h:end_h, start_w:end_w, z_idx, :].copy()\n",
    "\n",
    "        # Apply transformations if specified\n",
    "        if self.transform is not None:\n",
    "            # Your transform should expect a tensor of shape (C, H, W, T) or (H, W, T)\n",
    "            # depending on how you structure it. Adjust accordingly.\n",
    "            sample = self.transform(patch_time_series)\n",
    "\n",
    "        # Apply normalization if specified (after other transforms but before returning)\n",
    "        if self.normalize:\n",
    "            # Ensure sample is a tensor and normalize channel-wise\n",
    "            # Assuming sample shape is (C, H, W, T) where C=1 here\n",
    "            sample = self.normalize_sample(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def normalize_sample(self, sample):\n",
    "        \"\"\"Apply channel-wise normalization to the sample.\"\"\"\n",
    "        # Ensure the mean and std are on the same device as the sample\n",
    "        # Assuming self.stats['mean'] and self.stats['std'] are 1D tensors\n",
    "        mean = self.stats['mean'].to(sample.device)\n",
    "        std = self.stats['std'].to(sample.device)\n",
    "\n",
    "        mean = mean.view(1, -1, 1, 1)\n",
    "        std = std.view(1, -1, 1, 1)\n",
    "\n",
    "        # Normalize\n",
    "        return (sample - mean) / (std + 1e-8) # Add small epsilon for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:41.887548Z",
     "iopub.status.busy": "2025-06-06T00:21:41.887300Z",
     "iopub.status.idle": "2025-06-06T00:21:41.905000Z",
     "shell.execute_reply": "2025-06-06T00:21:41.904222Z",
     "shell.execute_reply.started": "2025-06-06T00:21:41.887518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stats = {\"mean\": torch.tensor([18598.638916899643]), \n",
    "         \"std\": torch.tensor([31381.96243127712])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "dataset = NoisyDataset(noisy_images_paths=image_folders, stats = stats, transform=transform, normalize=True)\n",
    "\n",
    "# Save the stats for future use\n",
    "torch.save(stats, os.path.join(folder, 'dataset_stats.pth'))\n",
    "# logger.info('Initial Dataset Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = NoisyDataset(noisy_images_paths=validation_folders, stats = stats, transform=transform, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:41.929636Z",
     "iopub.status.busy": "2025-06-06T00:21:41.929383Z",
     "iopub.status.idle": "2025-06-06T00:21:42.986019Z",
     "shell.execute_reply": "2025-06-06T00:21:42.985207Z",
     "shell.execute_reply.started": "2025-06-06T00:21:41.929620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:42.987090Z",
     "iopub.status.busy": "2025-06-06T00:21:42.986844Z",
     "iopub.status.idle": "2025-06-06T00:21:42.993249Z",
     "shell.execute_reply": "2025-06-06T00:21:42.992488Z",
     "shell.execute_reply.started": "2025-06-06T00:21:42.987065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    collate_fn=mask_collator,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    pin_memory=pin_mem,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=False)\n",
    "\n",
    "ipe = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=mask_collator,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    pin_memory=pin_mem,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.026668Z",
     "iopub.status.busy": "2025-06-06T00:21:43.026355Z",
     "iopub.status.idle": "2025-06-06T00:21:43.031077Z",
     "shell.execute_reply": "2025-06-06T00:21:43.030294Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.026644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model backbone (vision transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.032114Z",
     "iopub.status.busy": "2025-06-06T00:21:43.031848Z",
     "iopub.status.idle": "2025-06-06T00:21:43.046660Z",
     "shell.execute_reply": "2025-06-06T00:21:43.045942Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.032093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "\n",
    "def repeat_interleave_batch(x, B, repeat):\n",
    "    N = len(x) // B\n",
    "    x = torch.cat([\n",
    "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
    "        for i in range(N)\n",
    "    ], dim=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.047719Z",
     "iopub.status.busy": "2025-06-06T00:21:43.047480Z",
     "iopub.status.idle": "2025-06-06T00:21:43.102482Z",
     "shell.execute_reply": "2025-06-06T00:21:43.101488Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.047699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, timeline_pixel_width=16*300, patch_size=16, in_chans=1, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = timeline_pixel_width // patch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=1, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "class VisionTransformerPredictor(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=6,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        # --\n",
    "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n",
    "                                                requires_grad=False)\n",
    "        predictor_pos_embed = get_1d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n",
    "                                                      int(num_patches),\n",
    "                                                      cls_token=False)\n",
    "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        self.predictor_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
    "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        trunc_normal_(self.mask_token, std=self.init_std)\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks_x, masks):\n",
    "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
    "\n",
    "        if not isinstance(masks_x, list):\n",
    "            masks_x = [masks_x]\n",
    "\n",
    "        if not isinstance(masks, list):\n",
    "            masks = [masks]\n",
    "\n",
    "        # -- Batch Size\n",
    "        B = len(x) // len(masks_x)\n",
    "\n",
    "        # -- map from encoder-dim to pedictor-dim\n",
    "        x = self.predictor_embed(x)\n",
    "\n",
    "        # -- add positional embedding to x tokens\n",
    "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        x += apply_masks(x_pos_embed, masks_x)\n",
    "\n",
    "        _, N_ctxt, D = x.shape\n",
    "\n",
    "        # -- concat mask tokens to x\n",
    "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        pos_embs = apply_masks(pos_embs, masks)\n",
    "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
    "        # --\n",
    "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
    "        # --\n",
    "        pred_tokens += pos_embs\n",
    "        x = x.repeat(len(masks), 1, 1)\n",
    "        x = torch.cat([x, pred_tokens], dim=1)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for blk in self.predictor_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.predictor_norm(x)\n",
    "\n",
    "        # -- return preds for mask tokens\n",
    "        x = x[:, N_ctxt:]\n",
    "        x = self.predictor_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        timeline_pixel_width=(16*300),\n",
    "        patch_size=16,\n",
    "        in_chans=1,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        # --\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            timeline_pixel_width=timeline_pixel_width,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # --\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks=None):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "        x = x + pos_embed\n",
    "\n",
    "        # -- mask x\n",
    "        if masks is not None:\n",
    "            x = apply_masks(x, masks)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "\n",
    "def vit_predictor(**kwargs):\n",
    "    model = VisionTransformerPredictor(\n",
    "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_huge(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_giant(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.103790Z",
     "iopub.status.busy": "2025-06-06T00:21:43.103414Z",
     "iopub.status.idle": "2025-06-06T00:21:43.125973Z",
     "shell.execute_reply": "2025-06-06T00:21:43.125375Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.103761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    device,\n",
    "    r_path,\n",
    "    encoder,\n",
    "    predictor,\n",
    "    target_encoder,\n",
    "    opt,\n",
    "    scaler,\n",
    "    excluded_layers = None\n",
    "):\n",
    "    try:\n",
    "        checkpoint = torch.load(r_path, map_location=torch.device('cpu'))\n",
    "        epoch = checkpoint['epoch']\n",
    "\n",
    "        # -- loading encoder with filtering\n",
    "        pretrained_dict = checkpoint['encoder']\n",
    "        if excluded_layers:\n",
    "            # Filter out any keys that contain the excluded keywords\n",
    "            filtered_dict = {k: v for k, v in pretrained_dict.items() \n",
    "                            if not any(keyword in k for keyword in excluded_layers)}\n",
    "            logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n",
    "            pretrained_dict = filtered_dict\n",
    "            \n",
    "        msg = encoder.load_state_dict(pretrained_dict, strict=False)\n",
    "        logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "        # -- loading predictor\n",
    "        pretrained_dict = checkpoint['predictor']\n",
    "        if excluded_layers:\n",
    "            # Filter out any keys that contain the excluded keywords\n",
    "            filtered_dict = {k: v for k, v in pretrained_dict.items() \n",
    "                            if not any(keyword in k for keyword in excluded_layers)}\n",
    "            logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n",
    "            pretrained_dict = filtered_dict\n",
    "\n",
    "        msg = predictor.load_state_dict(pretrained_dict)\n",
    "        logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "        # -- loading target_encoder\n",
    "        if target_encoder is not None:\n",
    "            print(list(checkpoint.keys()))\n",
    "            pretrained_dict = checkpoint['target_encoder']\n",
    "            if excluded_layers:\n",
    "                # Filter out any keys that contain the excluded keywords\n",
    "                filtered_dict = {k: v for k, v in pretrained_dict.items() \n",
    "                                if not any(keyword in k for keyword in excluded_layers)}\n",
    "                logger.info(f\"Excluded {len(pretrained_dict) - len(filtered_dict)} parameters containing: {excluded_layers}\")\n",
    "                pretrained_dict = filtered_dict\n",
    "\n",
    "            msg = target_encoder.load_state_dict(pretrained_dict)\n",
    "            logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "        # -- loading optimizer\n",
    "        opt.load_state_dict(checkpoint['opt'])\n",
    "        if scaler is not None:\n",
    "            scaler.load_state_dict(checkpoint['scaler'])\n",
    "        logger.info(f'loaded optimizers from epoch {epoch}')\n",
    "        logger.info(f'read-path: {r_path}')\n",
    "        del checkpoint\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.info(f'Encountered exception when loading checkpoint {e}')\n",
    "        epoch = 0\n",
    "\n",
    "    return encoder, predictor, target_encoder, opt, scaler, epoch\n",
    "\n",
    "\n",
    "def init_model(\n",
    "    device,\n",
    "    patch_size=16,\n",
    "    model_name='vit_base',\n",
    "    timeline_pixel_width=16*300,\n",
    "    pred_depth=6,\n",
    "    pred_emb_dim=384\n",
    "):\n",
    "    encoder = vit_small(\n",
    "        timeline_pixel_width=timeline_pixel_width,\n",
    "        patch_size=patch_size)\n",
    "    predictor = vit_predictor(\n",
    "        num_patches=encoder.patch_embed.num_patches,\n",
    "        embed_dim=encoder.embed_dim,\n",
    "        predictor_embed_dim=pred_emb_dim,\n",
    "        depth=pred_depth,\n",
    "        num_heads=encoder.num_heads)\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, torch.nn.LayerNorm):\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "            torch.nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    for m in encoder.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    for m in predictor.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    encoder.to(device)\n",
    "    predictor.to(device)\n",
    "    logger.info(encoder)\n",
    "    return encoder, predictor\n",
    "\n",
    "\n",
    "def init_opt(\n",
    "    encoder,\n",
    "    predictor,\n",
    "    iterations_per_epoch,\n",
    "    start_lr,\n",
    "    ref_lr,\n",
    "    warmup,\n",
    "    num_epochs,\n",
    "    wd=1e-6,\n",
    "    final_wd=1e-6,\n",
    "    final_lr=0.0,\n",
    "    use_bfloat16=False,\n",
    "    ipe_scale=1.25\n",
    "):\n",
    "    param_groups = [\n",
    "        {\n",
    "            'params': (p for n, p in encoder.named_parameters()\n",
    "                       if ('bias' not in n) and (len(p.shape) != 1))\n",
    "        }, {\n",
    "            'params': (p for n, p in predictor.named_parameters()\n",
    "                       if ('bias' not in n) and (len(p.shape) != 1))\n",
    "        }, {\n",
    "            'params': (p for n, p in encoder.named_parameters()\n",
    "                       if ('bias' in n) or (len(p.shape) == 1)),\n",
    "            'WD_exclude': True,\n",
    "            'weight_decay': 0\n",
    "        }, {\n",
    "            'params': (p for n, p in predictor.named_parameters()\n",
    "                       if ('bias' in n) or (len(p.shape) == 1)),\n",
    "            'WD_exclude': True,\n",
    "            'weight_decay': 0\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    logger.info('Using AdamW')\n",
    "    optimizer = torch.optim.AdamW(param_groups)\n",
    "    scheduler = WarmupCosineSchedule(\n",
    "        optimizer,\n",
    "        warmup_steps=int(warmup*iterations_per_epoch),\n",
    "        start_lr=start_lr,\n",
    "        ref_lr=ref_lr,\n",
    "        final_lr=final_lr,\n",
    "        T_max=int(ipe_scale*num_epochs*iterations_per_epoch))\n",
    "    wd_scheduler = CosineWDSchedule(\n",
    "        optimizer,\n",
    "        ref_wd=wd,\n",
    "        final_wd=final_wd,\n",
    "        T_max=int(ipe_scale*num_epochs*iterations_per_epoch))\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_bfloat16 else None\n",
    "    return optimizer, scaler, scheduler, wd_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.129495Z",
     "iopub.status.busy": "2025-06-06T00:21:43.129161Z",
     "iopub.status.idle": "2025-06-06T00:21:43.149322Z",
     "shell.execute_reply": "2025-06-06T00:21:43.148608Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.129474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WarmupCosineSchedule(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        warmup_steps,\n",
    "        start_lr,\n",
    "        ref_lr,\n",
    "        T_max,\n",
    "        last_epoch=-1,\n",
    "        final_lr=0.\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "        self.start_lr = start_lr\n",
    "        self.ref_lr = ref_lr\n",
    "        self.final_lr = final_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.T_max = T_max - warmup_steps\n",
    "        self._step = 0.\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        if self._step < self.warmup_steps:\n",
    "            progress = float(self._step) / float(max(1, self.warmup_steps))\n",
    "            new_lr = self.start_lr + progress * (self.ref_lr - self.start_lr)\n",
    "        else:\n",
    "            # -- progress after warmup\n",
    "            progress = float(self._step - self.warmup_steps) / float(max(1, self.T_max))\n",
    "            new_lr = max(self.final_lr,\n",
    "                         self.final_lr + (self.ref_lr - self.final_lr) * 0.5 * (1. + math.cos(math.pi * progress)))\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = new_lr\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "\n",
    "class CosineWDSchedule(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        ref_wd,\n",
    "        T_max,\n",
    "        final_wd=0.\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "        self.ref_wd = ref_wd\n",
    "        self.final_wd = final_wd\n",
    "        self.T_max = T_max\n",
    "        self._step = 0.\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        progress = self._step / self.T_max\n",
    "        new_wd = self.final_wd + (self.ref_wd - self.final_wd) * 0.5 * (1. + math.cos(math.pi * progress))\n",
    "\n",
    "        if self.final_wd <= self.ref_wd:\n",
    "            new_wd = max(self.final_wd, new_wd)\n",
    "        else:\n",
    "            new_wd = min(self.final_wd, new_wd)\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            if ('WD_exclude' not in group) or not group['WD_exclude']:\n",
    "                group['weight_decay'] = new_wd\n",
    "        return new_wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.150061Z",
     "iopub.status.busy": "2025-06-06T00:21:43.149880Z",
     "iopub.status.idle": "2025-06-06T00:21:44.507386Z",
     "shell.execute_reply": "2025-06-06T00:21:44.506824Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.150047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -- init model\n",
    "encoder, predictor = init_model(\n",
    "    device=device,\n",
    "    patch_size=patch_size,\n",
    "    timeline_pixel_width=300*16,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim,\n",
    "    model_name=model_name)\n",
    "target_encoder = copy.deepcopy(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:44.508509Z",
     "iopub.status.busy": "2025-06-06T00:21:44.508247Z",
     "iopub.status.idle": "2025-06-06T00:21:44.519286Z",
     "shell.execute_reply": "2025-06-06T00:21:44.518480Z",
     "shell.execute_reply.started": "2025-06-06T00:21:44.508487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -- init optimizer and scheduler\n",
    "optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
    "    encoder=encoder,\n",
    "    predictor=predictor,\n",
    "    wd=wd,\n",
    "    final_wd=final_wd,\n",
    "    start_lr=start_lr,\n",
    "    ref_lr=lr,\n",
    "    final_lr=final_lr,\n",
    "    iterations_per_epoch=ipe,\n",
    "    warmup=warmup,\n",
    "    num_epochs=num_epochs,\n",
    "    ipe_scale=ipe_scale,\n",
    "    use_bfloat16=use_bfloat16)\n",
    "    # encoder = DistributedDataParallel(encoder, static_graph=True)\n",
    "    # predictor = DistributedDataParallel(predictor, static_graph=True)\n",
    "    # target_encoder = DistributedDataParallel(target_encoder)\n",
    "for p in target_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# -- momentum schedule\n",
    "momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
    "                      for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
    "\n",
    "start_epoch = 0\n",
    "# -- load training checkpoint\n",
    "if load_model:\n",
    "    encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
    "        device=device,\n",
    "        r_path=load_path,\n",
    "        encoder=encoder,\n",
    "        predictor=predictor,\n",
    "        target_encoder=target_encoder,\n",
    "        opt=optimizer,\n",
    "        scaler=scaler)\n",
    "    for _ in range(start_epoch*ipe):\n",
    "        scheduler.step()\n",
    "        wd_scheduler.step()\n",
    "        next(momentum_scheduler)\n",
    "        mask_collator.step()\n",
    "\n",
    "def save_checkpoint(epoch):\n",
    "    save_dict = {\n",
    "        'encoder': encoder.state_dict(),\n",
    "        'predictor': predictor.state_dict(),\n",
    "        'target_encoder': target_encoder.state_dict(),\n",
    "        'opt': optimizer.state_dict(),\n",
    "        'scaler': None if scaler is None else scaler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss_meter.avg,\n",
    "        'batch_size': batch_size,\n",
    "        'lr': lr\n",
    "    }\n",
    "    if rank == 0:\n",
    "        torch.save(save_dict, latest_path)\n",
    "        if (epoch + 1) % checkpoint_freq == 0:\n",
    "            torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:44.520538Z",
     "iopub.status.busy": "2025-06-06T00:21:44.520044Z",
     "iopub.status.idle": "2025-06-06T00:21:44.541673Z",
     "shell.execute_reply": "2025-06-06T00:21:44.541041Z",
     "shell.execute_reply.started": "2025-06-06T00:21:44.520515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate(epoch):\n",
    "    \"\"\"Run validation on the validation dataset and return average loss.\"\"\"\n",
    "    logger.info('Running validation...')\n",
    "    val_loss_meter = AverageMeter()\n",
    "    val_maskA_meter = AverageMeter()\n",
    "    val_maskB_meter = AverageMeter()\n",
    "    val_time_meter = AverageMeter()\n",
    "    \n",
    "    # Set models to eval mode\n",
    "    encoder.eval()\n",
    "    predictor.eval()\n",
    "    target_encoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for itr, (udata, masks_enc, masks_pred) in enumerate(val_loader):\n",
    "            # Load and process images\n",
    "            imgs = udata.to(device, non_blocking=True)\n",
    "            masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
    "            masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
    "            \n",
    "            val_maskA_meter.update(len(masks_1[0][0]))\n",
    "            val_maskB_meter.update(len(masks_2[0][0]))\n",
    "            \n",
    "            # Forward pass\n",
    "            def val_step():\n",
    "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
    "                    # Target encoding\n",
    "                    h = target_encoder(imgs)\n",
    "                    h = F.layer_norm(h, (h.size(-1),))\n",
    "                    B = len(h)\n",
    "                    h = apply_masks(h, masks_2)\n",
    "                    h = repeat_interleave_batch(h, B, repeat=len(masks_1))\n",
    "                    \n",
    "                    # Context encoding and prediction\n",
    "                    z = encoder(imgs, masks_1)\n",
    "                    z = predictor(z, masks_1, masks_2)\n",
    "                    \n",
    "                    # Loss calculation\n",
    "                    loss = F.smooth_l1_loss(z, h)\n",
    "                    return float(loss)\n",
    "            \n",
    "            loss, etime = gpu_timer(val_step)\n",
    "            val_loss_meter.update(loss)\n",
    "            val_time_meter.update(etime)\n",
    "            \n",
    "            # Log progress occasionally\n",
    "            if itr % (log_freq * 2) == 0:\n",
    "                logger.info(f'Val: [{epoch + 1}, {itr}] loss: {val_loss_meter.avg:.3f} '\n",
    "                           f'masks: {val_maskA_meter.avg:.1f} {val_maskB_meter.avg:.1f} '\n",
    "                           f'({val_time_meter.avg:.1f} ms)')\n",
    "    \n",
    "    # Set models back to training mode\n",
    "    encoder.train()\n",
    "    predictor.train()\n",
    "    \n",
    "    return val_loss_meter.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:44.542531Z",
     "iopub.status.busy": "2025-06-06T00:21:44.542292Z",
     "iopub.status.idle": "2025-06-06T00:21:44.560180Z",
     "shell.execute_reply": "2025-06-06T00:21:44.559476Z",
     "shell.execute_reply.started": "2025-06-06T00:21:44.542516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add this configuration near your other parameters\n",
    "val_frequency = 156  # Run validation every 100 training steps\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:44.561255Z",
     "iopub.status.busy": "2025-06-06T00:21:44.560991Z",
     "iopub.status.idle": "2025-06-06T00:27:46.053992Z",
     "shell.execute_reply": "2025-06-06T00:27:46.052938Z",
     "shell.execute_reply.started": "2025-06-06T00:21:44.561212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -- TRAINING LOOP\n",
    "global_step = 0\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    logger.info('Epoch %d' % (epoch + 1))\n",
    "\n",
    "    loss_meter = AverageMeter()\n",
    "    maskA_meter = AverageMeter()\n",
    "    maskB_meter = AverageMeter()\n",
    "    time_meter = AverageMeter()\n",
    "\n",
    "    for itr, (udata, masks_enc, masks_pred) in enumerate(data_loader):\n",
    "        new_validation = False\n",
    "        print(udata.shape)\n",
    "        if global_step % val_frequency == 0:\n",
    "            new_validation = True\n",
    "            # Run validation and check if we need to save the model\n",
    "            val_loss = validate(epoch)\n",
    "            print(f\"Validation loss: {val_loss}\")\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_checkpoint(epoch)\n",
    "        \n",
    "        def load_imgs():\n",
    "            # -- unsupervised imgs\n",
    "            imgs = udata.to(device, non_blocking=True) # udata[0]\n",
    "            masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
    "            masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
    "            return (imgs, masks_1, masks_2)\n",
    "        imgs, masks_enc, masks_pred = load_imgs()\n",
    "        maskA_meter.update(len(masks_enc[0][0]))\n",
    "        maskB_meter.update(len(masks_pred[0][0]))\n",
    "\n",
    "        def train_step():\n",
    "            _new_lr = scheduler.step()\n",
    "            _new_wd = wd_scheduler.step()\n",
    "            # --\n",
    "\n",
    "            def forward_target():\n",
    "                with torch.no_grad():\n",
    "                    h = target_encoder(imgs)\n",
    "                    h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
    "                    B = len(h)\n",
    "                    # -- create targets (masked regions of h)\n",
    "                    h = apply_masks(h, masks_pred)\n",
    "                    h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
    "                    return h\n",
    "\n",
    "            def forward_context():\n",
    "                z = encoder(imgs, masks_enc)\n",
    "                z = predictor(z, masks_enc, masks_pred)\n",
    "                return z\n",
    "\n",
    "            def loss_fn(z, h):\n",
    "                loss = F.smooth_l1_loss(z, h)\n",
    "                return loss\n",
    "\n",
    "            # Step 1. Forward\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
    "                h = forward_target()\n",
    "                z = forward_context()\n",
    "                loss = loss_fn(z, h)\n",
    "\n",
    "            #  Step 2. Backward & step\n",
    "            if use_bfloat16:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # grad_stats = grad_logger(encoder.named_parameters())\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step 3. momentum update of target encoder\n",
    "            with torch.no_grad():\n",
    "                m = next(momentum_scheduler)\n",
    "                for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
    "                    param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "\n",
    "            return (float(loss), _new_lr, _new_wd, None)#grad_stats)\n",
    "        (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
    "        loss_meter.update(loss)\n",
    "        time_meter.update(etime)\n",
    "\n",
    "        # -- Logging\n",
    "        def log_stats():\n",
    "            if new_validation:\n",
    "                log_val_value = val_loss\n",
    "            else:\n",
    "                log_val_value = None\n",
    "            csv_logger.log(epoch + 1, itr, loss, log_val_value, maskA_meter.val, maskB_meter.val, etime)\n",
    "            if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
    "                logger.info('[%d, %5d] loss: %.3f '\n",
    "                            'masks: %.1f %.1f '\n",
    "                            '[wd: %.2e] [lr: %.2e] '\n",
    "                            '[mem: %.2e] '\n",
    "                            '(%.1f ms)'\n",
    "                            % (epoch + 1, itr,\n",
    "                               loss_meter.avg,\n",
    "                               maskA_meter.avg,\n",
    "                               maskB_meter.avg,\n",
    "                               _new_wd,\n",
    "                               _new_lr,\n",
    "                               torch.cuda.max_memory_allocated() / 1024.**2,\n",
    "                               time_meter.avg))\n",
    "                print(f\"loss: {loss_meter.avg}, maskA: {maskA_meter.avg}, maskB: {maskB_meter.avg}\")\n",
    "\n",
    "                if grad_stats is not None:\n",
    "                    logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
    "                                % (epoch + 1, itr,\n",
    "                                   grad_stats.first_layer,\n",
    "                                   grad_stats.last_layer,\n",
    "                                   grad_stats.min,\n",
    "                                   grad_stats.max))\n",
    "\n",
    "        log_stats()\n",
    "\n",
    "        assert not np.isnan(loss), 'loss is nan'\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    # -- Save Checkpoint after every epoch\n",
    "    logger.info('avg. loss %.3f' % loss_meter.avg)\n",
    "    save_checkpoint(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-06T00:27:46.054844Z",
     "iopub.status.idle": "2025-06-06T00:27:46.055119Z",
     "shell.execute_reply": "2025-06-06T00:27:46.055013Z",
     "shell.execute_reply.started": "2025-06-06T00:27:46.054997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, global_step=None, val_loss=None):\n",
    "    save_dict = {\n",
    "        'encoder': encoder.state_dict(),\n",
    "        'predictor': predictor.state_dict(),\n",
    "        'target_encoder': target_encoder.state_dict(),\n",
    "        'opt': optimizer.state_dict(),\n",
    "        'scaler': None if scaler is None else scaler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'step': global_step,\n",
    "        'train_loss': loss_meter.avg,\n",
    "        'val_loss': val_loss,\n",
    "        'batch_size': batch_size,\n",
    "        'lr': lr\n",
    "    }\n",
    "    if rank == 0:\n",
    "        torch.save(save_dict, latest_path)\n",
    "        if (epoch + 1) % checkpoint_freq == 0:\n",
    "            torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7487278,
     "sourceId": 11909841,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7511375,
     "sourceId": 11947961,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7598871,
     "sourceId": 12071850,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7598877,
     "sourceId": 12071852,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
