{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I-JEPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:37.818929Z",
     "iopub.status.busy": "2025-06-06T00:21:37.818624Z",
     "iopub.status.idle": "2025-06-06T00:21:37.823031Z",
     "shell.execute_reply": "2025-06-06T00:21:37.822489Z",
     "shell.execute_reply.started": "2025-06-06T00:21:37.818911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# core train\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:37.834636Z",
     "iopub.status.busy": "2025-06-06T00:21:37.834410Z",
     "iopub.status.idle": "2025-06-06T00:21:37.840539Z",
     "shell.execute_reply": "2025-06-06T00:21:37.839940Z",
     "shell.execute_reply.started": "2025-06-06T00:21:37.834622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Basic global config\n",
    "_GLOBAL_SEED = 0\n",
    "np.random.seed(_GLOBAL_SEED)\n",
    "torch.manual_seed(_GLOBAL_SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:37.874413Z",
     "iopub.status.busy": "2025-06-06T00:21:37.873857Z",
     "iopub.status.idle": "2025-06-06T00:21:37.879379Z",
     "shell.execute_reply": "2025-06-06T00:21:37.878830Z",
     "shell.execute_reply.started": "2025-06-06T00:21:37.874397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data\": {\n",
    "        \"batch_size\": 64,\n",
    "        \"crop_scale\": [0.3, 1.0],\n",
    "        \"crop_size\": 224,\n",
    "        \"image_folders\": [\"ssl-s2l1c/data/ssl4eo-s12/train/S2L1C\",\n",
    "                          \"ssl-s2l2a/data/ssl4eo-s12/train/S2L2A\"],\n",
    "        \"validation_folders\": [\"ssl-s2l1c-val/data/ssl4eo-s12/val/S2L1C\",\n",
    "                                 \"ssl-s2l2a-val/data/ssl4eo-s12/val/S2L2A\"],\n",
    "        \"num_workers\": 2,\n",
    "        \"pin_mem\": True,\n",
    "        \"root_path\": \"/kaggle/input\",\n",
    "        \"use_horizontal_flip\": False\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"folder\": \"/kaggle/working/logs\",\n",
    "        \"write_tag\": \"jepa\"\n",
    "    },\n",
    "    \"mask\": {\n",
    "        \"allow_overlap\": False,\n",
    "        \"aspect_ratio\": [0.75, 1.5],\n",
    "        \"enc_mask_scale\": [0.85, 1.0],\n",
    "        \"min_keep\": 10,\n",
    "        \"num_enc_masks\": 1,\n",
    "        \"num_pred_masks\": 4,\n",
    "        \"patch_size\": 14,\n",
    "        \"pred_mask_scale\": [0.15, 0.2]\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"copy_data\": False,\n",
    "        \"load_checkpoint\": False,\n",
    "        \"model_name\": \"vit_huge\",\n",
    "        \"pred_depth\": 12,\n",
    "        \"pred_emb_dim\": 384,\n",
    "        \"read_checkpoint\": None,\n",
    "        \"use_bfloat16\": True\n",
    "    },\n",
    "    \"optimization\": {\n",
    "        \"ema\": [0.996, 1.0],\n",
    "        \"epochs\": 2,\n",
    "        \"final_lr\": 1.0e-5,\n",
    "        \"final_weight_decay\": 0.4,\n",
    "        \"ipe_scale\": 1.0,\n",
    "        \"lr\": 0.001,\n",
    "        \"start_lr\": 0.0002,\n",
    "        \"warmup\": 20,\n",
    "        \"weight_decay\": 0.04\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.336168Z",
     "iopub.status.busy": "2025-06-06T00:21:38.335898Z",
     "iopub.status.idle": "2025-06-06T00:21:38.347795Z",
     "shell.execute_reply": "2025-06-06T00:21:38.347279Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.336151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "resume_preempt = False\n",
    "rank = 0\n",
    "\n",
    "# -- META\n",
    "use_bfloat16 = args['meta']['use_bfloat16']\n",
    "model_name = args['meta']['model_name']\n",
    "load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
    "r_file = args['meta']['read_checkpoint']\n",
    "copy_data = args['meta']['copy_data']\n",
    "pred_depth = args['meta']['pred_depth']\n",
    "pred_emb_dim = args['meta']['pred_emb_dim']\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "# -- DATA\n",
    "use_horizontal_flip = args['data']['use_horizontal_flip']\n",
    "# --\n",
    "batch_size = args['data']['batch_size']\n",
    "pin_mem = args['data']['pin_mem']\n",
    "num_workers = args['data']['num_workers']\n",
    "root_path = args['data']['root_path']\n",
    "image_folders = args['data']['image_folders']\n",
    "validation_folders = args['data']['validation_folders']\n",
    "crop_size = args['data']['crop_size']\n",
    "crop_scale = args['data']['crop_scale']\n",
    "# --\n",
    "\n",
    "# -- MASK\n",
    "allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
    "patch_size = args['mask']['patch_size']  # patch-size for model training\n",
    "num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
    "min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
    "enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
    "num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
    "pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
    "aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
    "# --\n",
    "\n",
    "# -- OPTIMIZATION\n",
    "ema = args['optimization']['ema']\n",
    "ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
    "wd = float(args['optimization']['weight_decay'])\n",
    "final_wd = float(args['optimization']['final_weight_decay'])\n",
    "num_epochs = args['optimization']['epochs']\n",
    "warmup = args['optimization']['warmup']\n",
    "start_lr = args['optimization']['start_lr']\n",
    "lr = args['optimization']['lr']\n",
    "final_lr = args['optimization']['final_lr']\n",
    "\n",
    "# -- LOGGING\n",
    "folder = args['logging']['folder']\n",
    "tag = args['logging']['write_tag']\n",
    "\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "dump = os.path.join(folder, 'params-ijepa.yaml')\n",
    "with open(dump, 'w') as f:\n",
    "    yaml.dump(args, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.349538Z",
     "iopub.status.busy": "2025-06-06T00:21:38.349265Z",
     "iopub.status.idle": "2025-06-06T00:21:38.364111Z",
     "shell.execute_reply": "2025-06-06T00:21:38.363483Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.349522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "log_timings = True\n",
    "log_freq = 10\n",
    "checkpoint_freq = 1\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.365092Z",
     "iopub.status.busy": "2025-06-06T00:21:38.364870Z",
     "iopub.status.idle": "2025-06-06T00:21:38.378959Z",
     "shell.execute_reply": "2025-06-06T00:21:38.378393Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.365070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gpu_timer(closure, log_timings=True):\n",
    "    \"\"\" Helper to time gpu-time to execute closure() \"\"\"\n",
    "    log_timings = log_timings and torch.cuda.is_available()\n",
    "\n",
    "    elapsed_time = -1.\n",
    "    if log_timings:\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "\n",
    "    result = closure()\n",
    "\n",
    "    if log_timings:\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_time = start.elapsed_time(end)\n",
    "\n",
    "    return result, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.379851Z",
     "iopub.status.busy": "2025-06-06T00:21:38.379615Z",
     "iopub.status.idle": "2025-06-06T00:21:38.394604Z",
     "shell.execute_reply": "2025-06-06T00:21:38.393996Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.379822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CSVLogger(object):\n",
    "\n",
    "    def __init__(self, fname, *argv):\n",
    "        self.fname = fname\n",
    "        self.types = []\n",
    "        # -- print headers\n",
    "        with open(self.fname, '+a') as f:\n",
    "            for i, v in enumerate(argv, 1):\n",
    "                self.types.append(v[0])\n",
    "                if i < len(argv):\n",
    "                    print(v[1], end=',', file=f)\n",
    "                else:\n",
    "                    print(v[1], end='\\n', file=f)\n",
    "\n",
    "    def log(self, *argv):\n",
    "        with open(self.fname, '+a') as f:\n",
    "            for i, tv in enumerate(zip(self.types, argv), 1):\n",
    "                end = ',' if i < len(argv) else '\\n'\n",
    "                print(tv[0] % tv[1], end=end, file=f)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.max = float('-inf')\n",
    "        self.min = float('inf')\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        try:\n",
    "            self.max = max(val, self.max)\n",
    "            self.min = min(val, self.min)\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.395940Z",
     "iopub.status.busy": "2025-06-06T00:21:38.395750Z",
     "iopub.status.idle": "2025-06-06T00:21:38.407749Z",
     "shell.execute_reply": "2025-06-06T00:21:38.407115Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.395925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -- log/checkpointing paths\n",
    "log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
    "save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
    "latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
    "load_path = None\n",
    "if load_model:\n",
    "    load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
    "\n",
    "# csv logger\n",
    "csv_logger = CSVLogger(log_file,\n",
    "                       ('%d', 'epoch'),\n",
    "                       ('%d', 'itr'),\n",
    "                       ('%.5f', 'train_loss'),\n",
    "                       ('%.5f', 'val_loss'),  # Added validation loss\n",
    "                       ('%.5f', 'mask-A'),\n",
    "                       ('%.5f', 'mask-B'),\n",
    "                       ('%d', 'time (ms)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.408608Z",
     "iopub.status.busy": "2025-06-06T00:21:38.408365Z",
     "iopub.status.idle": "2025-06-06T00:21:38.425550Z",
     "shell.execute_reply": "2025-06-06T00:21:38.424923Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.408589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from multiprocessing import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.543045Z",
     "iopub.status.busy": "2025-06-06T00:21:38.542811Z",
     "iopub.status.idle": "2025-06-06T00:21:38.559019Z",
     "shell.execute_reply": "2025-06-06T00:21:38.558300Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.543030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class MaskCollator(object):\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_size=(224, 224),\n",
    "#         patch_size=16,\n",
    "#         enc_mask_scale=(0.2, 0.8),\n",
    "#         pred_mask_scale=(0.2, 0.8),\n",
    "#         aspect_ratio=(0.3, 3.0),\n",
    "#         nenc=1,\n",
    "#         npred=2,\n",
    "#         min_keep=4,\n",
    "#         allow_overlap=False\n",
    "#     ):\n",
    "#         super(MaskCollator, self).__init__()\n",
    "#         if not isinstance(input_size, tuple):\n",
    "#             input_size = (input_size, ) * 2\n",
    "#         self.patch_size = patch_size\n",
    "#         self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
    "#         self.enc_mask_scale = enc_mask_scale\n",
    "#         self.pred_mask_scale = pred_mask_scale\n",
    "#         self.aspect_ratio = aspect_ratio\n",
    "#         self.nenc = nenc\n",
    "#         self.npred = npred\n",
    "#         self.min_keep = min_keep  # minimum number of patches to keep\n",
    "#         self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
    "#         self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
    "\n",
    "#     def step(self):\n",
    "#         i = self._itr_counter\n",
    "#         with i.get_lock():\n",
    "#             i.value += 1\n",
    "#             v = i.value\n",
    "#         return v\n",
    "\n",
    "#     def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
    "#         _rand = torch.rand(1, generator=generator).item()\n",
    "#         # -- Sample block scale\n",
    "#         min_s, max_s = scale\n",
    "#         mask_scale = min_s + _rand * (max_s - min_s)\n",
    "#         max_keep = int(self.height * self.width * mask_scale)\n",
    "#         # -- Sample block aspect-ratio\n",
    "#         min_ar, max_ar = aspect_ratio_scale\n",
    "#         aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
    "#         # -- Compute block height and width (given scale and aspect-ratio)\n",
    "#         h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
    "#         w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
    "#         while h >= self.height:\n",
    "#             h -= 1\n",
    "#         while w >= self.width:\n",
    "#             w -= 1\n",
    "\n",
    "#         return (h, w)\n",
    "\n",
    "#     def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
    "#         h, w = b_size\n",
    "\n",
    "#         def constrain_mask(mask, tries=0):\n",
    "#             \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
    "#             N = max(int(len(acceptable_regions)-tries), 0)\n",
    "#             for k in range(N):\n",
    "#                 mask *= acceptable_regions[k]\n",
    "#         # --\n",
    "#         # -- Loop to sample masks until we find a valid one\n",
    "#         tries = 0\n",
    "#         timeout = og_timeout = 20\n",
    "#         valid_mask = False\n",
    "#         while not valid_mask:\n",
    "#             # -- Sample block top-left corner\n",
    "#             top = torch.randint(0, self.height - h, (1,))\n",
    "#             left = torch.randint(0, self.width - w, (1,))\n",
    "#             mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
    "#             mask[top:top+h, left:left+w] = 1\n",
    "#             # -- Constrain mask to a set of acceptable regions\n",
    "#             if acceptable_regions is not None:\n",
    "#                 constrain_mask(mask, tries)\n",
    "#             mask = torch.nonzero(mask.flatten())\n",
    "#             # -- If mask too small try again\n",
    "#             valid_mask = len(mask) > self.min_keep\n",
    "#             if not valid_mask:\n",
    "#                 timeout -= 1\n",
    "#                 if timeout == 0:\n",
    "#                     tries += 1\n",
    "#                     timeout = og_timeout\n",
    "#                     logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
    "#         mask = mask.squeeze()\n",
    "#         # --\n",
    "#         mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
    "#         mask_complement[top:top+h, left:left+w] = 0\n",
    "#         # --\n",
    "#         return mask, mask_complement\n",
    "\n",
    "#     def __call__(self, batch):\n",
    "#         '''\n",
    "#         Create encoder and predictor masks when collating imgs into a batch\n",
    "#         # 1. sample enc block (size + location) using seed\n",
    "#         # 2. sample pred block (size) using seed\n",
    "#         # 3. sample several enc block locations for each image (w/o seed)\n",
    "#         # 4. sample several pred block locations for each image (w/o seed)\n",
    "#         # 5. return enc mask and pred mask\n",
    "#         '''\n",
    "#         B = len(batch)\n",
    "\n",
    "#         collated_batch = torch.utils.data.default_collate(batch)\n",
    "\n",
    "#         seed = self.step()\n",
    "#         g = torch.Generator()\n",
    "#         g.manual_seed(seed)\n",
    "#         p_size = self._sample_block_size(\n",
    "#             generator=g,\n",
    "#             scale=self.pred_mask_scale,\n",
    "#             aspect_ratio_scale=self.aspect_ratio)\n",
    "#         e_size = self._sample_block_size(\n",
    "#             generator=g,\n",
    "#             scale=self.enc_mask_scale,\n",
    "#             aspect_ratio_scale=(1., 1.))\n",
    "\n",
    "#         collated_masks_pred, collated_masks_enc = [], []\n",
    "#         min_keep_pred = self.height * self.width\n",
    "#         min_keep_enc = self.height * self.width\n",
    "#         for _ in range(B):\n",
    "\n",
    "#             masks_p, masks_C = [], []\n",
    "#             for _ in range(self.npred):\n",
    "#                 mask, mask_C = self._sample_block_mask(p_size)\n",
    "#                 masks_p.append(mask)\n",
    "#                 masks_C.append(mask_C)\n",
    "#                 min_keep_pred = min(min_keep_pred, len(mask))\n",
    "#             collated_masks_pred.append(masks_p)\n",
    "\n",
    "#             acceptable_regions = masks_C\n",
    "#             try:\n",
    "#                 if self.allow_overlap:\n",
    "#                     acceptable_regions= None\n",
    "#             except Exception as e:\n",
    "#                 logger.warning(f'Encountered exception in mask-generator {e}')\n",
    "\n",
    "#             masks_e = []\n",
    "#             for _ in range(self.nenc):\n",
    "#                 mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
    "#                 masks_e.append(mask)\n",
    "#                 min_keep_enc = min(min_keep_enc, len(mask))\n",
    "#             collated_masks_enc.append(masks_e)\n",
    "\n",
    "#         collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
    "#         collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
    "#         # --\n",
    "#         collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
    "#         collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
    "\n",
    "#         return collated_batch, collated_masks_enc, collated_masks_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesMaskCollator:\n",
    "    def __init__(self, num_frames=300, frame_size=16, nenc=1, npred=1):\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.height = frame_size  # fixed spatial height of a frame\n",
    "        self.width = num_frames * frame_size  # time flows horizontally\n",
    "        self.nenc = nenc\n",
    "        self.npred = npred\n",
    "        self._itr_counter = Value('i', -1)\n",
    "\n",
    "    def step(self):\n",
    "        i = self._itr_counter\n",
    "        with i.get_lock():\n",
    "            i.value += 1\n",
    "            v = i.value\n",
    "        return v\n",
    "\n",
    "    def _sample_frame_mask(self, generator, exclude_frames=None):\n",
    "        # Build list of available frame indices\n",
    "        choices = torch.tensor(\n",
    "            [i for i in range(self.num_frames) if (exclude_frames is None or i not in exclude_frames)],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        # Sample one index using the generator\n",
    "        idx = torch.randint(0, len(choices), (1,), generator=generator).item()\n",
    "        frame_idx = choices[idx]\n",
    "\n",
    "        # Calculate top-left corner in new layout (horizontal stacking)\n",
    "        top = 0\n",
    "        left = frame_idx * self.frame_size\n",
    "        mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
    "        mask[top:top+self.frame_size, left:left+self.frame_size] = 1\n",
    "        return mask, frame_idx\n",
    "\n",
    "    def build_encoder_mask_from_pred(self, pred_masks):\n",
    "        enc_mask = torch.ones((self.height, self.width), dtype=torch.int32)\n",
    "        for pred_mask in pred_masks:\n",
    "            enc_mask.view(-1)[pred_mask] = 0  # Zero out the masked regions\n",
    "        return torch.nonzero(enc_mask.flatten()).squeeze()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        B = len(batch)\n",
    "        collated_batch = torch.utils.data.default_collate(batch)\n",
    "\n",
    "        seed = self.step()\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "\n",
    "        collated_masks_enc, collated_masks_pred = [], []\n",
    "\n",
    "        for _ in range(B):\n",
    "            pred_masks = []\n",
    "            pred_frame_idxs = []\n",
    "            for _ in range(self.npred):\n",
    "                mask, idx = self._sample_frame_mask(generator=g, exclude_frames=pred_frame_idxs)\n",
    "                pred_masks.append(torch.nonzero(mask.flatten()).squeeze())\n",
    "                pred_frame_idxs.append(idx)\n",
    "\n",
    "            collated_masks_pred.append(pred_masks)\n",
    "\n",
    "            enc_masks = []\n",
    "            for _ in range(self.nenc):\n",
    "                enc_mask = self.build_encoder_mask_from_pred(pred_masks)\n",
    "                enc_masks.append(enc_mask)\n",
    "\n",
    "            collated_masks_enc.append(enc_masks)\n",
    "\n",
    "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
    "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
    "\n",
    "        return collated_batch, collated_masks_enc, collated_masks_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.560462Z",
     "iopub.status.busy": "2025-06-06T00:21:38.560249Z",
     "iopub.status.idle": "2025-06-06T00:21:38.576906Z",
     "shell.execute_reply": "2025-06-06T00:21:38.576258Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.560448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_masks(x, masks):\n",
    "    \"\"\"\n",
    "    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
    "    :param masks: list of tensors containing indices of patches in [N] to keep\n",
    "    \"\"\"\n",
    "    all_x = []\n",
    "    for m in masks:\n",
    "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
    "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
    "    return torch.cat(all_x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.577723Z",
     "iopub.status.busy": "2025-06-06T00:21:38.577520Z",
     "iopub.status.idle": "2025-06-06T00:21:38.594428Z",
     "shell.execute_reply": "2025-06-06T00:21:38.593842Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.577709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# mask_collator = MaskCollator(\n",
    "#     input_size=crop_size,\n",
    "#     patch_size=patch_size,\n",
    "#     pred_mask_scale=pred_mask_scale,\n",
    "#     enc_mask_scale=enc_mask_scale,\n",
    "#     aspect_ratio=aspect_ratio,\n",
    "#     nenc=num_enc_masks,\n",
    "#     npred=num_pred_masks,\n",
    "#     allow_overlap=allow_overlap,\n",
    "#     min_keep=min_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_collator = TimeSeriesMaskCollator() # defaults to 300 frames of size 16x16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.595346Z",
     "iopub.status.busy": "2025-06-06T00:21:38.595131Z",
     "iopub.status.idle": "2025-06-06T00:21:38.606826Z",
     "shell.execute_reply": "2025-06-06T00:21:38.606278Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.595332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.608487Z",
     "iopub.status.busy": "2025-06-06T00:21:38.608278Z",
     "iopub.status.idle": "2025-06-06T00:21:38.623514Z",
     "shell.execute_reply": "2025-06-06T00:21:38.622773Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.608473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from skimage.transform import resize\n",
    "import torch # Import PyTorch\n",
    "\n",
    "class Compose:\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of callables): list of transforms to compose.\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img_array):\n",
    "        \"\"\"\n",
    "        Applies the composed transforms to the input array.\n",
    "        The input can be a NumPy array. The output will be a torch.Tensor\n",
    "        if torch.from_numpy is the last transform.\n",
    "        Args:\n",
    "            img_array (numpy.ndarray): Input image array (C, H, W).\n",
    "        Returns:\n",
    "            torch.Tensor: Transformed image tensor.\n",
    "        \"\"\"\n",
    "        for t in self.transforms:\n",
    "            img_array = t(img_array) # Note: img_array will become a torch.Tensor at the end\n",
    "        return img_array\n",
    "\n",
    "def random_resized_crop_np(img_array, size, scale=(0.08, 1.0), interpolation_order=1):\n",
    "    \"\"\"\n",
    "    Crop the given NumPy array to random size and aspect ratio, then resize.\n",
    "    Applies the same crop/resize to all channels.\n",
    "\n",
    "    Args:\n",
    "        img_array (numpy.ndarray): Input image array (C, H, W).\n",
    "        size (int or tuple): Desired output size of the crop (H_out, W_out).\n",
    "            If int, it's (size, size).\n",
    "        scale (tuple): Range of size of the origin size cropped.\n",
    "        interpolation_order (int): Order of interpolation for resizing (0-5).\n",
    "                                   0: Nearest-neighbor, 1: Bi-linear (default), 3: Bi-cubic.\n",
    "    Returns:\n",
    "        numpy.ndarray: Cropped and resized image array (C, new_H, new_W).\n",
    "    \"\"\"\n",
    "    #img_array = img_array.astype(np.float32)\n",
    "    C, H, W = img_array.shape\n",
    "    if isinstance(size, int):\n",
    "        target_h, target_w = size, size\n",
    "    else:\n",
    "        target_h, target_w = size\n",
    "\n",
    "    area = H * W\n",
    "    target_area = random.uniform(scale[0], scale[1]) * area\n",
    "    log_ratio = (np.log(3 / 4), np.log(4 / 3)) # Default aspect ratio range\n",
    "    aspect_ratio = np.exp(random.uniform(log_ratio[0], log_ratio[1]))\n",
    "\n",
    "    # Calculate crop dimensions\n",
    "    for attempt in range(10):\n",
    "        w = int(round(np.sqrt(target_area * aspect_ratio)))\n",
    "        h = int(round(np.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "        if 0 < w <= W and 0 < h <= H:\n",
    "            i = random.randint(0, H - h)\n",
    "            j = random.randint(0, W - w)\n",
    "            break\n",
    "    else: # Fallback to center crop if no valid random crop found\n",
    "        i = (H - h) // 2 if h < H else 0\n",
    "        j = (W - w) // 2 if w < W else 0\n",
    "        h = min(h, H - i)\n",
    "        w = min(w, W - j)\n",
    "\n",
    "    cropped_channels = []\n",
    "    for c in range(C):\n",
    "        cropped_channel = img_array[c, i:i+h, j:j+w]\n",
    "        resized_channel = resize(cropped_channel, (target_h, target_w),\n",
    "                                 order=interpolation_order,\n",
    "                                 mode='reflect',\n",
    "                                 anti_aliasing=True)\n",
    "        cropped_channels.append(resized_channel)\n",
    "\n",
    "    return np.stack(cropped_channels, axis=0)\n",
    "\n",
    "def random_horizontal_flip_np(img_array, p=0.5):\n",
    "    \"\"\"\n",
    "    Horizontally flip the given NumPy array randomly with a given probability.\n",
    "    Applies to all channels.\n",
    "    Args:\n",
    "        img_array (numpy.ndarray): Input image array (C, H, W).\n",
    "        p (float): probability of the image being flipped. Default value is 0.5.\n",
    "    Returns:\n",
    "        numpy.ndarray: Flipped or original image array.\n",
    "    \"\"\"\n",
    "    if random.random() < p:\n",
    "        return np.flip(img_array, axis=2).copy()\n",
    "    return img_array\n",
    "\n",
    "def scale_to_01_np(img_array, max_int_value=32767.0):\n",
    "    \"\"\"\n",
    "    Scales positive integer values in a NumPy array to the 0-1 range.\n",
    "    Assumes input values are positive and fit within max_int_value.\n",
    "    Args:\n",
    "        img_array (numpy.ndarray): Input image array.\n",
    "        max_int_value (float): The maximum possible integer value in the original array.\n",
    "                               For int16, this is typically 32767.\n",
    "    Returns:\n",
    "        numpy.ndarray: Scaled image array.\n",
    "    \"\"\"\n",
    "    return img_array / max_int_value\n",
    "\n",
    "\n",
    "def ensure_13_channels(img_array):\n",
    "    \"\"\"\n",
    "    Ensures the input array has exactly 13 channels by adding the pixel-wise mean\n",
    "    of the first 12 channels as the 13th channel.\n",
    "\n",
    "    Args:\n",
    "        img_array (numpy.ndarray): Input image array (C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array with exactly 13 channels\n",
    "    \"\"\"\n",
    "    C, H, W = img_array.shape\n",
    "    img_array = img_array.astype(np.float32)\n",
    "\n",
    "    # If already 13 channels, return as is\n",
    "    if C == 13:\n",
    "        return img_array\n",
    "\n",
    "    # If 12 channels, add pixel-wise mean as 13th channel\n",
    "    elif C == 12:\n",
    "        mean_channel = np.mean(img_array, axis=0, keepdims=True)  # Shape: (1, H, W)\n",
    "        return np.concatenate([img_array, mean_channel], axis=0)\n",
    "\n",
    "    # For other numbers of channels (unexpected case)\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 12 or 13 channels, but got {C}\")\n",
    "\n",
    "# Build the transform list\n",
    "transform_list = []\n",
    "\n",
    "transform_list += [ensure_13_channels]\n",
    "\n",
    "# Data preprocessing into smaller dimension + augmentation\n",
    "transform_list += [lambda x: random_resized_crop_np(x, crop_size, scale=crop_scale)]\n",
    "if use_horizontal_flip:\n",
    "    transform_list += [random_horizontal_flip_np]\n",
    "\n",
    "# Scaling to 0-1 range (applied to NumPy array)\n",
    "transform_list += [lambda x: scale_to_01_np(x, max_int_value=32767.0)]\n",
    "\n",
    "# Final conversion to PyTorch Tensor\n",
    "# torch.from_numpy will also ensure float32 dtype for the tensor if the numpy array is float32\n",
    "transform_list += [torch.from_numpy]\n",
    "\n",
    "\n",
    "# Composition of transforms\n",
    "transform = Compose(transform_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:38.624511Z",
     "iopub.status.busy": "2025-06-06T00:21:38.624243Z",
     "iopub.status.idle": "2025-06-06T00:21:41.823740Z",
     "shell.execute_reply": "2025-06-06T00:21:41.823002Z",
     "shell.execute_reply.started": "2025-06-06T00:21:38.624491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:41.825176Z",
     "iopub.status.busy": "2025-06-06T00:21:41.824850Z",
     "iopub.status.idle": "2025-06-06T00:21:41.829142Z",
     "shell.execute_reply": "2025-06-06T00:21:41.828593Z",
     "shell.execute_reply.started": "2025-06-06T00:21:41.825142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:41.830418Z",
     "iopub.status.busy": "2025-06-06T00:21:41.830036Z",
     "iopub.status.idle": "2025-06-06T00:21:41.848210Z",
     "shell.execute_reply": "2025-06-06T00:21:41.847585Z",
     "shell.execute_reply.started": "2025-06-06T00:21:41.830402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SSLLEODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_paths: list, transform=None, normalize=False, stats=None):\n",
    "        \"\"\"Initialize SSLLEO dataset from partial downloads.\n",
    "        \n",
    "        Args:\n",
    "            data_paths (list): Paths to the SSLEO dataset parts.\n",
    "            transform: Transformations to apply to each sample.\n",
    "            normalize (bool): Whether to apply channel-wise normalization.\n",
    "            stats (dict, optional): Dict containing 'mean' and 'std' for normalization.\n",
    "                                   If None and normalize=True, will use default values.\n",
    "        \"\"\"\n",
    "        self.data_paths = data_paths\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.stats = stats\n",
    "        \n",
    "        # Default stats if none provided but normalization requested\n",
    "        if self.normalize and self.stats is None:\n",
    "            self.stats = {\n",
    "                'mean': torch.tensor([0.0] * 13),  # Default zeros for 13 channels\n",
    "                'std': torch.tensor([1.0] * 13)    # Default ones for 13 channels\n",
    "            }\n",
    "        \n",
    "        # traverse each data path and retrieve the number .zarr files for dataset length count\n",
    "        self.sample_paths = []\n",
    "        for path in data_paths:\n",
    "            for file in os.listdir(path):\n",
    "                if file.endswith('.zarr'):\n",
    "                    self.sample_paths.append(os.path.join(path, file))\n",
    "            \n",
    "        self.data_len = len(self.sample_paths)*256  # each zarr file contains 256 samples (64*4)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # unpack images from zarr files\n",
    "        zarr_index = index // 256\n",
    "        sample_index = index % 256\n",
    "        zarr_path = self.sample_paths[zarr_index]\n",
    "        zarr_data = zarr.open(zarr_path, mode='r')\n",
    "\n",
    "        bands_data = zarr_data['bands'][:]  # bands data is of dimension (64, 4, 12/13, 264, 264)\n",
    "        bands_data = np.array(bands_data) \n",
    "\n",
    "        # combining the first two dimensions into one dimension\n",
    "        bands_data = bands_data.reshape(-1, *bands_data.shape[2:])\n",
    "\n",
    "        # retrieving a single sample\n",
    "        sample = bands_data[sample_index]\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        # Apply normalization if specified (after other transforms but before returning)\n",
    "        if self.normalize and isinstance(sample, torch.Tensor):\n",
    "            # Ensure sample is a tensor and normalize channel-wise\n",
    "            # Assuming sample shape is (C, H, W)\n",
    "            sample = self.normalize_sample(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def normalize_sample(self, sample):\n",
    "        \"\"\"Apply channel-wise normalization to the sample.\"\"\"\n",
    "        # Ensure the mean and std are on the same device as the sample\n",
    "        mean = self.stats['mean'].to(sample.device)\n",
    "        std = self.stats['std'].to(sample.device)\n",
    "        \n",
    "        # Expand dimensions to match the sample shape: (C) -> (C, 1, 1)\n",
    "        mean = mean.view(-1, 1, 1)\n",
    "        std = std.view(-1, 1, 1)\n",
    "        \n",
    "        # Normalize\n",
    "        return (sample - mean) / (std + 1e-8)  # Add small epsilon for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:41.850573Z",
     "iopub.status.busy": "2025-06-06T00:21:41.850201Z",
     "iopub.status.idle": "2025-06-06T00:21:41.869268Z",
     "shell.execute_reply": "2025-06-06T00:21:41.868660Z",
     "shell.execute_reply.started": "2025-06-06T00:21:41.850558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_dataset_stats(dataset, num_samples=None, batch_size=64):\n",
    "    \"\"\"\n",
    "    Calculate mean, std, and max of the dataset across channels.\n",
    "    \n",
    "    Args:\n",
    "        dataset: A dataset instance without normalization.\n",
    "        num_samples: Number of samples to use. If None, uses all samples.\n",
    "        batch_size: Batch size for processing.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing 'mean', 'std', and 'max' tensors.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    if num_samples is not None:\n",
    "        total_batches = min(num_samples // batch_size + (1 if num_samples % batch_size else 0), \n",
    "                          len(loader))\n",
    "    else:\n",
    "        total_batches = len(loader)\n",
    "    \n",
    "    channels_sum = 0\n",
    "    channels_squared_sum = 0\n",
    "    channels_max = None  # Will initialize after seeing the first batch\n",
    "    num_batches = 0\n",
    "    num_samples_seen = 0\n",
    "    \n",
    "    print(f\"Calculating dataset statistics across {total_batches} batches...\")\n",
    "    \n",
    "    for i, data in enumerate(loader):\n",
    "\n",
    "        if not isinstance(data, torch.Tensor):\n",
    "            print(f\"Warning: Batch {i} returned non-tensor data, skipping\")\n",
    "            continue\n",
    "            \n",
    "        data = data.view(data.size(0), data.size(1), -1)  # [B, C, H*W]\n",
    "\n",
    "        print(data.shape)\n",
    "        \n",
    "        if channels_max is None:\n",
    "            channels_max = torch.max(data, dim=2)[0].max(dim=0)[0]\n",
    "        else:\n",
    "            batch_max = torch.max(data, dim=2)[0].max(dim=0)[0]\n",
    "            channels_max = torch.max(channels_max, batch_max)\n",
    "        \n",
    "        channels_sum += torch.sum(data, dim=[0, 2])\n",
    "        channels_squared_sum += torch.sum(data**2, dim=[0, 2])\n",
    "        \n",
    "        batch_size_actual = data.size(0)\n",
    "        num_samples_seen += batch_size_actual\n",
    "        num_batches += 1\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {i}/{total_batches} batches ({num_samples_seen} samples)\")\n",
    "\n",
    "        if i+1==total_batches:\n",
    "            break\n",
    "    \n",
    "    pixels_per_sample = data.size(2)\n",
    "    \n",
    "    print(pixels_per_sample)\n",
    "    print(num_samples_seen)\n",
    "\n",
    "    mean = channels_sum / (num_samples_seen * pixels_per_sample)\n",
    "    std = torch.sqrt(\n",
    "        channels_squared_sum / (num_samples_seen * pixels_per_sample) - mean**2\n",
    "    )\n",
    "    \n",
    "    return {'mean': mean, 'std': std, 'max': channels_max}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:41.870062Z",
     "iopub.status.busy": "2025-06-06T00:21:41.869907Z",
     "iopub.status.idle": "2025-06-06T00:21:41.886608Z",
     "shell.execute_reply": "2025-06-06T00:21:41.885942Z",
     "shell.execute_reply.started": "2025-06-06T00:21:41.870050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Step 1: Create dataset without normalization to calculate statistics\n",
    "# dataset_for_stats = SSLLEODataset(\n",
    "#     data_paths=[os.path.join(root_path, path) for path in image_folders],\n",
    "#     transform=transform,  \n",
    "#     normalize=False\n",
    "# )\n",
    "\n",
    "# # Step 2: Calculate dataset statistics (can be time-consuming)\n",
    "# # You might want to save these stats after calculation\n",
    "# stats = calculate_dataset_stats(dataset_for_stats, num_samples=64)  # Using subset for speed\n",
    "# print(f\"Channel means: {stats['mean']}\")\n",
    "# print(f\"Channel stds: {stats['std']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:41.887548Z",
     "iopub.status.busy": "2025-06-06T00:21:41.887300Z",
     "iopub.status.idle": "2025-06-06T00:21:41.905000Z",
     "shell.execute_reply": "2025-06-06T00:21:41.904222Z",
     "shell.execute_reply.started": "2025-06-06T00:21:41.887518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stats = {\"mean\": torch.tensor([0.0673, 0.0660, 0.0688, 0.0720, 0.0799, 0.0970, 0.1048, 0.1051, 0.1102, 0.0845, 0.0675, 0.0926, 0.0832]), \n",
    "         \"std\": torch.tensor([0.0327, 0.0324, 0.0324, 0.0388, 0.0387, 0.0388, 0.0412, 0.0417, 0.0425, 0.0474, 0.0484, 0.0416, 0.0363])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:41.906021Z",
     "iopub.status.busy": "2025-06-06T00:21:41.905832Z",
     "iopub.status.idle": "2025-06-06T00:21:41.928788Z",
     "shell.execute_reply": "2025-06-06T00:21:41.928292Z",
     "shell.execute_reply.started": "2025-06-06T00:21:41.905999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Create your actual training dataset with normalization\n",
    "dataset = SSLLEODataset(\n",
    "    data_paths=[os.path.join(root_path, path) for path in image_folders],\n",
    "    transform=transform,\n",
    "    normalize=True,\n",
    "    stats=stats\n",
    ")\n",
    "\n",
    "# Save the stats for future use\n",
    "torch.save(stats, os.path.join(folder, 'dataset_stats.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:41.929636Z",
     "iopub.status.busy": "2025-06-06T00:21:41.929383Z",
     "iopub.status.idle": "2025-06-06T00:21:42.986019Z",
     "shell.execute_reply": "2025-06-06T00:21:42.985207Z",
     "shell.execute_reply.started": "2025-06-06T00:21:41.929620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:42.987090Z",
     "iopub.status.busy": "2025-06-06T00:21:42.986844Z",
     "iopub.status.idle": "2025-06-06T00:21:42.993249Z",
     "shell.execute_reply": "2025-06-06T00:21:42.992488Z",
     "shell.execute_reply.started": "2025-06-06T00:21:42.987065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    collate_fn=mask_collator,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    pin_memory=pin_mem,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=False)\n",
    "\n",
    "ipe = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:42.994363Z",
     "iopub.status.busy": "2025-06-06T00:21:42.994086Z",
     "iopub.status.idle": "2025-06-06T00:21:43.025619Z",
     "shell.execute_reply": "2025-06-06T00:21:43.024920Z",
     "shell.execute_reply.started": "2025-06-06T00:21:42.994342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create validation dataset using a portion of the data or separate validation data\n",
    "val_dataset = SSLLEODataset(\n",
    "    data_paths=[os.path.join(root_path, path) for path in validation_folders],  # Adjust path as needed\n",
    "    transform=transform,\n",
    "    normalize=True,\n",
    "    stats=stats\n",
    ")\n",
    "\n",
    "# Create validation dataloader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=mask_collator,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    pin_memory=pin_mem,\n",
    "    num_workers=num_workers // 2,  # Use fewer workers for validation\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "logger.info(f\"Created validation dataloader with {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.026668Z",
     "iopub.status.busy": "2025-06-06T00:21:43.026355Z",
     "iopub.status.idle": "2025-06-06T00:21:43.031077Z",
     "shell.execute_reply": "2025-06-06T00:21:43.030294Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.026644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model backbone (vision transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.032114Z",
     "iopub.status.busy": "2025-06-06T00:21:43.031848Z",
     "iopub.status.idle": "2025-06-06T00:21:43.046660Z",
     "shell.execute_reply": "2025-06-06T00:21:43.045942Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.032093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "\n",
    "def repeat_interleave_batch(x, B, repeat):\n",
    "    N = len(x) // B\n",
    "    x = torch.cat([\n",
    "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
    "        for i in range(N)\n",
    "    ], dim=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.047719Z",
     "iopub.status.busy": "2025-06-06T00:21:43.047480Z",
     "iopub.status.idle": "2025-06-06T00:21:43.102482Z",
     "shell.execute_reply": "2025-06-06T00:21:43.101488Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.047699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=13, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "class VisionTransformerPredictor(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=6,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        # --\n",
    "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n",
    "                                                requires_grad=False)\n",
    "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n",
    "                                                      int(num_patches**.5),\n",
    "                                                      cls_token=False)\n",
    "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        self.predictor_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
    "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        trunc_normal_(self.mask_token, std=self.init_std)\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks_x, masks):\n",
    "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
    "\n",
    "        if not isinstance(masks_x, list):\n",
    "            masks_x = [masks_x]\n",
    "\n",
    "        if not isinstance(masks, list):\n",
    "            masks = [masks]\n",
    "\n",
    "        # -- Batch Size\n",
    "        B = len(x) // len(masks_x)\n",
    "\n",
    "        # -- map from encoder-dim to pedictor-dim\n",
    "        x = self.predictor_embed(x)\n",
    "\n",
    "        # -- add positional embedding to x tokens\n",
    "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        x += apply_masks(x_pos_embed, masks_x)\n",
    "\n",
    "        _, N_ctxt, D = x.shape\n",
    "\n",
    "        # -- concat mask tokens to x\n",
    "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        pos_embs = apply_masks(pos_embs, masks)\n",
    "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
    "        # --\n",
    "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
    "        # --\n",
    "        pred_tokens += pos_embs\n",
    "        x = x.repeat(len(masks), 1, 1)\n",
    "        x = torch.cat([x, pred_tokens], dim=1)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for blk in self.predictor_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.predictor_norm(x)\n",
    "\n",
    "        # -- return preds for mask tokens\n",
    "        x = x[:, N_ctxt:]\n",
    "        x = self.predictor_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=13,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        # --\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size[0],\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # --\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches**.5),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks=None):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "        x = x + pos_embed\n",
    "\n",
    "        # -- mask x\n",
    "        if masks is not None:\n",
    "            x = apply_masks(x, masks)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "\n",
    "def vit_predictor(**kwargs):\n",
    "    model = VisionTransformerPredictor(\n",
    "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_huge(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_giant(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.103790Z",
     "iopub.status.busy": "2025-06-06T00:21:43.103414Z",
     "iopub.status.idle": "2025-06-06T00:21:43.125973Z",
     "shell.execute_reply": "2025-06-06T00:21:43.125375Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.103761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(\n",
    "    device,\n",
    "    r_path,\n",
    "    encoder,\n",
    "    predictor,\n",
    "    target_encoder,\n",
    "    opt,\n",
    "    scaler,\n",
    "):\n",
    "    try:\n",
    "        checkpoint = torch.load(r_path, map_location=torch.device('cpu'))\n",
    "        epoch = checkpoint['epoch']\n",
    "\n",
    "        # -- loading encoder\n",
    "        pretrained_dict = checkpoint['encoder']\n",
    "        msg = encoder.load_state_dict(pretrained_dict)\n",
    "        logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "        # -- loading predictor\n",
    "        pretrained_dict = checkpoint['predictor']\n",
    "        msg = predictor.load_state_dict(pretrained_dict)\n",
    "        logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "        # -- loading target_encoder\n",
    "        if target_encoder is not None:\n",
    "            print(list(checkpoint.keys()))\n",
    "            pretrained_dict = checkpoint['target_encoder']\n",
    "            msg = target_encoder.load_state_dict(pretrained_dict)\n",
    "            logger.info(f'loaded pretrained encoder from epoch {epoch} with msg: {msg}')\n",
    "\n",
    "        # -- loading optimizer\n",
    "        opt.load_state_dict(checkpoint['opt'])\n",
    "        if scaler is not None:\n",
    "            scaler.load_state_dict(checkpoint['scaler'])\n",
    "        logger.info(f'loaded optimizers from epoch {epoch}')\n",
    "        logger.info(f'read-path: {r_path}')\n",
    "        del checkpoint\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.info(f'Encountered exception when loading checkpoint {e}')\n",
    "        epoch = 0\n",
    "\n",
    "    return encoder, predictor, target_encoder, opt, scaler, epoch\n",
    "\n",
    "\n",
    "def init_model(\n",
    "    device,\n",
    "    patch_size=16,\n",
    "    model_name='vit_base',\n",
    "    crop_size=224,\n",
    "    pred_depth=6,\n",
    "    pred_emb_dim=384\n",
    "):\n",
    "    encoder = vit_small(\n",
    "        img_size=[crop_size],\n",
    "        patch_size=patch_size)\n",
    "    predictor = vit_predictor(\n",
    "        num_patches=encoder.patch_embed.num_patches,\n",
    "        embed_dim=encoder.embed_dim,\n",
    "        predictor_embed_dim=pred_emb_dim,\n",
    "        depth=pred_depth,\n",
    "        num_heads=encoder.num_heads)\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, torch.nn.LayerNorm):\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "            torch.nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    for m in encoder.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    for m in predictor.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    encoder.to(device)\n",
    "    predictor.to(device)\n",
    "    logger.info(encoder)\n",
    "    return encoder, predictor\n",
    "\n",
    "\n",
    "def init_opt(\n",
    "    encoder,\n",
    "    predictor,\n",
    "    iterations_per_epoch,\n",
    "    start_lr,\n",
    "    ref_lr,\n",
    "    warmup,\n",
    "    num_epochs,\n",
    "    wd=1e-6,\n",
    "    final_wd=1e-6,\n",
    "    final_lr=0.0,\n",
    "    use_bfloat16=False,\n",
    "    ipe_scale=1.25\n",
    "):\n",
    "    param_groups = [\n",
    "        {\n",
    "            'params': (p for n, p in encoder.named_parameters()\n",
    "                       if ('bias' not in n) and (len(p.shape) != 1))\n",
    "        }, {\n",
    "            'params': (p for n, p in predictor.named_parameters()\n",
    "                       if ('bias' not in n) and (len(p.shape) != 1))\n",
    "        }, {\n",
    "            'params': (p for n, p in encoder.named_parameters()\n",
    "                       if ('bias' in n) or (len(p.shape) == 1)),\n",
    "            'WD_exclude': True,\n",
    "            'weight_decay': 0\n",
    "        }, {\n",
    "            'params': (p for n, p in predictor.named_parameters()\n",
    "                       if ('bias' in n) or (len(p.shape) == 1)),\n",
    "            'WD_exclude': True,\n",
    "            'weight_decay': 0\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    logger.info('Using AdamW')\n",
    "    optimizer = torch.optim.AdamW(param_groups)\n",
    "    scheduler = WarmupCosineSchedule(\n",
    "        optimizer,\n",
    "        warmup_steps=int(warmup*iterations_per_epoch),\n",
    "        start_lr=start_lr,\n",
    "        ref_lr=ref_lr,\n",
    "        final_lr=final_lr,\n",
    "        T_max=int(ipe_scale*num_epochs*iterations_per_epoch))\n",
    "    wd_scheduler = CosineWDSchedule(\n",
    "        optimizer,\n",
    "        ref_wd=wd,\n",
    "        final_wd=final_wd,\n",
    "        T_max=int(ipe_scale*num_epochs*iterations_per_epoch))\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_bfloat16 else None\n",
    "    return optimizer, scaler, scheduler, wd_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.129495Z",
     "iopub.status.busy": "2025-06-06T00:21:43.129161Z",
     "iopub.status.idle": "2025-06-06T00:21:43.149322Z",
     "shell.execute_reply": "2025-06-06T00:21:43.148608Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.129474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WarmupCosineSchedule(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        warmup_steps,\n",
    "        start_lr,\n",
    "        ref_lr,\n",
    "        T_max,\n",
    "        last_epoch=-1,\n",
    "        final_lr=0.\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "        self.start_lr = start_lr\n",
    "        self.ref_lr = ref_lr\n",
    "        self.final_lr = final_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.T_max = T_max - warmup_steps\n",
    "        self._step = 0.\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        if self._step < self.warmup_steps:\n",
    "            progress = float(self._step) / float(max(1, self.warmup_steps))\n",
    "            new_lr = self.start_lr + progress * (self.ref_lr - self.start_lr)\n",
    "        else:\n",
    "            # -- progress after warmup\n",
    "            progress = float(self._step - self.warmup_steps) / float(max(1, self.T_max))\n",
    "            new_lr = max(self.final_lr,\n",
    "                         self.final_lr + (self.ref_lr - self.final_lr) * 0.5 * (1. + math.cos(math.pi * progress)))\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = new_lr\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "\n",
    "class CosineWDSchedule(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        ref_wd,\n",
    "        T_max,\n",
    "        final_wd=0.\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "        self.ref_wd = ref_wd\n",
    "        self.final_wd = final_wd\n",
    "        self.T_max = T_max\n",
    "        self._step = 0.\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        progress = self._step / self.T_max\n",
    "        new_wd = self.final_wd + (self.ref_wd - self.final_wd) * 0.5 * (1. + math.cos(math.pi * progress))\n",
    "\n",
    "        if self.final_wd <= self.ref_wd:\n",
    "            new_wd = max(self.final_wd, new_wd)\n",
    "        else:\n",
    "            new_wd = min(self.final_wd, new_wd)\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            if ('WD_exclude' not in group) or not group['WD_exclude']:\n",
    "                group['weight_decay'] = new_wd\n",
    "        return new_wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:43.150061Z",
     "iopub.status.busy": "2025-06-06T00:21:43.149880Z",
     "iopub.status.idle": "2025-06-06T00:21:44.507386Z",
     "shell.execute_reply": "2025-06-06T00:21:44.506824Z",
     "shell.execute_reply.started": "2025-06-06T00:21:43.150047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -- init model\n",
    "encoder, predictor = init_model(\n",
    "    device=device,\n",
    "    patch_size=patch_size,\n",
    "    crop_size=crop_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim,\n",
    "    model_name=model_name)\n",
    "target_encoder = copy.deepcopy(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:44.508509Z",
     "iopub.status.busy": "2025-06-06T00:21:44.508247Z",
     "iopub.status.idle": "2025-06-06T00:21:44.519286Z",
     "shell.execute_reply": "2025-06-06T00:21:44.518480Z",
     "shell.execute_reply.started": "2025-06-06T00:21:44.508487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -- init optimizer and scheduler\n",
    "optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
    "    encoder=encoder,\n",
    "    predictor=predictor,\n",
    "    wd=wd,\n",
    "    final_wd=final_wd,\n",
    "    start_lr=start_lr,\n",
    "    ref_lr=lr,\n",
    "    final_lr=final_lr,\n",
    "    iterations_per_epoch=ipe,\n",
    "    warmup=warmup,\n",
    "    num_epochs=num_epochs,\n",
    "    ipe_scale=ipe_scale,\n",
    "    use_bfloat16=use_bfloat16)\n",
    "    # encoder = DistributedDataParallel(encoder, static_graph=True)\n",
    "    # predictor = DistributedDataParallel(predictor, static_graph=True)\n",
    "    # target_encoder = DistributedDataParallel(target_encoder)\n",
    "for p in target_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# -- momentum schedule\n",
    "momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
    "                      for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
    "\n",
    "start_epoch = 0\n",
    "# -- load training checkpoint\n",
    "if load_model:\n",
    "    encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
    "        device=device,\n",
    "        r_path=load_path,\n",
    "        encoder=encoder,\n",
    "        predictor=predictor,\n",
    "        target_encoder=target_encoder,\n",
    "        opt=optimizer,\n",
    "        scaler=scaler)\n",
    "    for _ in range(start_epoch*ipe):\n",
    "        scheduler.step()\n",
    "        wd_scheduler.step()\n",
    "        next(momentum_scheduler)\n",
    "        mask_collator.step()\n",
    "\n",
    "def save_checkpoint(epoch):\n",
    "    save_dict = {\n",
    "        'encoder': encoder.state_dict(),\n",
    "        'predictor': predictor.state_dict(),\n",
    "        'target_encoder': target_encoder.state_dict(),\n",
    "        'opt': optimizer.state_dict(),\n",
    "        'scaler': None if scaler is None else scaler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss_meter.avg,\n",
    "        'batch_size': batch_size,\n",
    "        'lr': lr\n",
    "    }\n",
    "    if rank == 0:\n",
    "        torch.save(save_dict, latest_path)\n",
    "        if (epoch + 1) % checkpoint_freq == 0:\n",
    "            torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:44.520538Z",
     "iopub.status.busy": "2025-06-06T00:21:44.520044Z",
     "iopub.status.idle": "2025-06-06T00:21:44.541673Z",
     "shell.execute_reply": "2025-06-06T00:21:44.541041Z",
     "shell.execute_reply.started": "2025-06-06T00:21:44.520515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate(epoch):\n",
    "    \"\"\"Run validation on the validation dataset and return average loss.\"\"\"\n",
    "    logger.info('Running validation...')\n",
    "    val_loss_meter = AverageMeter()\n",
    "    val_maskA_meter = AverageMeter()\n",
    "    val_maskB_meter = AverageMeter()\n",
    "    val_time_meter = AverageMeter()\n",
    "    \n",
    "    # Set models to eval mode\n",
    "    encoder.eval()\n",
    "    predictor.eval()\n",
    "    target_encoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for itr, (udata, masks_enc, masks_pred) in enumerate(val_loader):\n",
    "            # Load and process images\n",
    "            imgs = udata.to(device, non_blocking=True)\n",
    "            masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
    "            masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
    "            \n",
    "            val_maskA_meter.update(len(masks_1[0][0]))\n",
    "            val_maskB_meter.update(len(masks_2[0][0]))\n",
    "            \n",
    "            # Forward pass\n",
    "            def val_step():\n",
    "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
    "                    # Target encoding\n",
    "                    h = target_encoder(imgs)\n",
    "                    h = F.layer_norm(h, (h.size(-1),))\n",
    "                    B = len(h)\n",
    "                    h = apply_masks(h, masks_2)\n",
    "                    h = repeat_interleave_batch(h, B, repeat=len(masks_1))\n",
    "                    \n",
    "                    # Context encoding and prediction\n",
    "                    z = encoder(imgs, masks_1)\n",
    "                    z = predictor(z, masks_1, masks_2)\n",
    "                    \n",
    "                    # Loss calculation\n",
    "                    loss = F.smooth_l1_loss(z, h)\n",
    "                    return float(loss)\n",
    "            \n",
    "            loss, etime = gpu_timer(val_step)\n",
    "            val_loss_meter.update(loss)\n",
    "            val_time_meter.update(etime)\n",
    "            \n",
    "            # Log progress occasionally\n",
    "            if itr % (log_freq * 2) == 0:\n",
    "                logger.info(f'Val: [{epoch + 1}, {itr}] loss: {val_loss_meter.avg:.3f} '\n",
    "                           f'masks: {val_maskA_meter.avg:.1f} {val_maskB_meter.avg:.1f} '\n",
    "                           f'({val_time_meter.avg:.1f} ms)')\n",
    "    \n",
    "    # Set models back to training mode\n",
    "    encoder.train()\n",
    "    predictor.train()\n",
    "    \n",
    "    return val_loss_meter.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:44.542531Z",
     "iopub.status.busy": "2025-06-06T00:21:44.542292Z",
     "iopub.status.idle": "2025-06-06T00:21:44.560180Z",
     "shell.execute_reply": "2025-06-06T00:21:44.559476Z",
     "shell.execute_reply.started": "2025-06-06T00:21:44.542516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add this configuration near your other parameters\n",
    "val_frequency = 120  # Run validation every 100 training steps\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T00:21:44.561255Z",
     "iopub.status.busy": "2025-06-06T00:21:44.560991Z",
     "iopub.status.idle": "2025-06-06T00:27:46.053992Z",
     "shell.execute_reply": "2025-06-06T00:27:46.052938Z",
     "shell.execute_reply.started": "2025-06-06T00:21:44.561212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -- TRAINING LOOP\n",
    "global_step = 0\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    logger.info('Epoch %d' % (epoch + 1))\n",
    "\n",
    "    loss_meter = AverageMeter()\n",
    "    maskA_meter = AverageMeter()\n",
    "    maskB_meter = AverageMeter()\n",
    "    time_meter = AverageMeter()\n",
    "\n",
    "    for itr, (udata, masks_enc, masks_pred) in enumerate(data_loader):\n",
    "        new_validation = False\n",
    "        print(udata.shape)\n",
    "        if global_step % val_frequency == 0:\n",
    "            new_validation = True\n",
    "            # Run validation and check if we need to save the model\n",
    "            val_loss = validate(epoch)\n",
    "            print(f\"Validation loss: {val_loss}\")\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_checkpoint(epoch)\n",
    "        \n",
    "        def load_imgs():\n",
    "            # -- unsupervised imgs\n",
    "            imgs = udata.to(device, non_blocking=True) # udata[0]\n",
    "            masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
    "            masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
    "            return (imgs, masks_1, masks_2)\n",
    "        imgs, masks_enc, masks_pred = load_imgs()\n",
    "        maskA_meter.update(len(masks_enc[0][0]))\n",
    "        maskB_meter.update(len(masks_pred[0][0]))\n",
    "\n",
    "        def train_step():\n",
    "            _new_lr = scheduler.step()\n",
    "            _new_wd = wd_scheduler.step()\n",
    "            # --\n",
    "\n",
    "            def forward_target():\n",
    "                with torch.no_grad():\n",
    "                    h = target_encoder(imgs)\n",
    "                    h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
    "                    B = len(h)\n",
    "                    # -- create targets (masked regions of h)\n",
    "                    h = apply_masks(h, masks_pred)\n",
    "                    h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
    "                    return h\n",
    "\n",
    "            def forward_context():\n",
    "                z = encoder(imgs, masks_enc)\n",
    "                z = predictor(z, masks_enc, masks_pred)\n",
    "                return z\n",
    "\n",
    "            def loss_fn(z, h):\n",
    "                loss = F.smooth_l1_loss(z, h)\n",
    "                return loss\n",
    "\n",
    "            # Step 1. Forward\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
    "                h = forward_target()\n",
    "                z = forward_context()\n",
    "                loss = loss_fn(z, h)\n",
    "\n",
    "            #  Step 2. Backward & step\n",
    "            if use_bfloat16:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # grad_stats = grad_logger(encoder.named_parameters())\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step 3. momentum update of target encoder\n",
    "            with torch.no_grad():\n",
    "                m = next(momentum_scheduler)\n",
    "                for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
    "                    param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "\n",
    "            return (float(loss), _new_lr, _new_wd, None)#grad_stats)\n",
    "        (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
    "        loss_meter.update(loss)\n",
    "        time_meter.update(etime)\n",
    "\n",
    "        # -- Logging\n",
    "        def log_stats():\n",
    "            if new_validation:\n",
    "                log_val_value = val_loss\n",
    "            else:\n",
    "                log_val_value = None\n",
    "            csv_logger.log(epoch + 1, itr, loss, log_val_value, maskA_meter.val, maskB_meter.val, etime)\n",
    "            if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
    "                logger.info('[%d, %5d] loss: %.3f '\n",
    "                            'masks: %.1f %.1f '\n",
    "                            '[wd: %.2e] [lr: %.2e] '\n",
    "                            '[mem: %.2e] '\n",
    "                            '(%.1f ms)'\n",
    "                            % (epoch + 1, itr,\n",
    "                               loss_meter.avg,\n",
    "                               maskA_meter.avg,\n",
    "                               maskB_meter.avg,\n",
    "                               _new_wd,\n",
    "                               _new_lr,\n",
    "                               torch.cuda.max_memory_allocated() / 1024.**2,\n",
    "                               time_meter.avg))\n",
    "                print(f\"loss: {loss_meter.avg}, maskA: {maskA_meter.avg}, maskB: {maskB_meter.avg}\")\n",
    "\n",
    "                if grad_stats is not None:\n",
    "                    logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
    "                                % (epoch + 1, itr,\n",
    "                                   grad_stats.first_layer,\n",
    "                                   grad_stats.last_layer,\n",
    "                                   grad_stats.min,\n",
    "                                   grad_stats.max))\n",
    "\n",
    "        log_stats()\n",
    "\n",
    "        assert not np.isnan(loss), 'loss is nan'\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    # -- Save Checkpoint after every epoch\n",
    "    logger.info('avg. loss %.3f' % loss_meter.avg)\n",
    "    save_checkpoint(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-06T00:27:46.054844Z",
     "iopub.status.idle": "2025-06-06T00:27:46.055119Z",
     "shell.execute_reply": "2025-06-06T00:27:46.055013Z",
     "shell.execute_reply.started": "2025-06-06T00:27:46.054997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, global_step=None, val_loss=None):\n",
    "    save_dict = {\n",
    "        'encoder': encoder.state_dict(),\n",
    "        'predictor': predictor.state_dict(),\n",
    "        'target_encoder': target_encoder.state_dict(),\n",
    "        'opt': optimizer.state_dict(),\n",
    "        'scaler': None if scaler is None else scaler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'step': global_step,\n",
    "        'train_loss': loss_meter.avg,\n",
    "        'val_loss': val_loss,\n",
    "        'batch_size': batch_size,\n",
    "        'lr': lr\n",
    "    }\n",
    "    if rank == 0:\n",
    "        torch.save(save_dict, latest_path)\n",
    "        if (epoch + 1) % checkpoint_freq == 0:\n",
    "            torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7487278,
     "sourceId": 11909841,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7511375,
     "sourceId": 11947961,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7598871,
     "sourceId": 12071850,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7598877,
     "sourceId": 12071852,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
