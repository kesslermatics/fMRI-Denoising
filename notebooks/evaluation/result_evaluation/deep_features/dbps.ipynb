{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba7f010",
   "metadata": {},
   "source": [
    "─────────────────────────────────────────────────────────────\n",
    "# Deep BOLD Perceptual Similarity (DBPS) - Model Implementation\n",
    "\n",
    "Basis:\n",
    "  - Inspired by Zhang et al. (2018), \"The Unreasonable Effectiveness of Deep Features\n",
    "    as a Perceptual Metric\" (https://arxiv.org/abs/1801.03924)\n",
    "  - Instead of natural images (based on ImageNet), we apply the concept to fMRI BOLD slices.\n",
    "  - Replace VGG features with a custom CNN feature extractor trained on fMRI GT data.\n",
    "\n",
    "Goal:\n",
    "  - Learn a feature extractor that measures perceptual similarity between\n",
    "    GT (BOLD-activated) fMRI slices and denoised (or noised) slices.\n",
    "  - Compute slice-wise similarity distances as a quantitative denoising metric.\n",
    "\n",
    "This notebook builds:\n",
    "  1. Data loading pipeline for GT slices.\n",
    "  2. A simple CNN-based feature extractor.\n",
    "  3. LPIPS-like feature-distance calculation (no 2AFC head).\n",
    "  4. Training loop on GT data to learn domain-specific features.\n",
    "\n",
    "─────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736ab61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292da388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ─── Device Setup ──────────────────────────────────────────────\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa98051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT volume shape: (64, 64, 156, 300)\n"
     ]
    }
   ],
   "source": [
    "# ─── Data Paths ────────────────────────────────────────────────\n",
    "gt_path = '../inputs/ground_truth/gt_func_train_1.npy'  # shape (64, 64, 156, 300)\n",
    "# We memory-map the data to avoid large RAM usage (7.5 GB for float64).\n",
    "gt = np.load(gt_path, mmap_mode='r')\n",
    "print(\"GT volume shape:\", gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76bf6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FMRI2DSliceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading 2D fMRI slices from a 4D fMRI volume (64, 64, Z, T).\n",
    "    Each sample is a single 2D slice (shape: 64×64), loaded as a float32 tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, volume: np.ndarray):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            volume (np.ndarray): 4D numpy array, shape (H, W, Z, T).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.volume = volume\n",
    "        self.H, self.W, self.Z, self.T = volume.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        # The dataset size is the total number of 2D slices (Z * T)\n",
    "        return self.Z * self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Given a flat index (0…Z*T), map to (z, t) and return 2D slice.\n",
    "        \"\"\"\n",
    "        z = idx // self.T\n",
    "        t = idx % self.T\n",
    "        # Load as float32 tensor\n",
    "        slice_2d = self.volume[:, :, z, t].astype(np.float32)\n",
    "        return torch.from_numpy(slice_2d).unsqueeze(0)  # shape: (1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0077e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFMRIEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A small CNN feature extractor for fMRI slices.\n",
    "    We keep it shallow (3 conv blocks) to avoid overfitting.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input: (1, 64, 64)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AvgPool2d(2)  # downsample after each conv\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st conv block\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # 2nd conv block\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # 3rd conv block\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x  # output shape: (64, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be64726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 46800 samples, Batch size: 32\n",
      "Encoder architecture:\n",
      "SimpleFMRIEncoder(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      ")\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Loss function: MSELoss()\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Create Dataset & DataLoader\n",
    "# We sample random 2D slices from the 4D fMRI volume as training samples.\n",
    "# This way, our CNN learns to handle realistic anatomical + functional structures.\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "dataset = FMRI2DSliceDataset(gt)\n",
    "batch_size = 32\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "print(f\"Dataset size: {len(dataset)} samples, Batch size: {batch_size}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Instantiate the CNN feature extractor (our Encoder).\n",
    "# It takes 1-channel (64,64) slices and compresses them into deep feature maps.\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "encoder = SimpleFMRIEncoder().to(device)\n",
    "print(\"Encoder architecture:\")\n",
    "print(encoder)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Loss & Optimizer\n",
    "# We use a simple \"autoencoding-like\" objective: downsample input slice as target.\n",
    "# No explicit anatomical labels needed, purely self-supervised.\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "print(\"Optimizer:\", optimizer)\n",
    "print(\"Loss function:\", loss_fn)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Training loop\n",
    "# Each epoch:\n",
    "#   - Feed random 2D slices to the encoder\n",
    "#   - Let the encoder learn robust fMRI-specific feature representations\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "epochs = 5  # adjust as needed!\n",
    "for epoch in range(epochs):\n",
    "    encoder.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = batch.to(device)  # shape (B, 1, 64, 64)\n",
    "\n",
    "        # Forward pass: extract deep feature maps\n",
    "        features = encoder(batch)\n",
    "\n",
    "        # Target: downsampled input slice to shape (B, 1, 8, 8)\n",
    "        target = F.avg_pool2d(batch, kernel_size=8)\n",
    "        target = target.repeat(1, 64, 1, 1)  # match output channels (64)\n",
    "\n",
    "        # Compute MSE loss\n",
    "        loss = loss_fn(features, target)\n",
    "\n",
    "        # Backpropagation & optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for statistics\n",
    "        total_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        # Print every 50 steps for better progress visibility\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}/{len(loader)} - Batch Loss: {loss.item():.6f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Average Loss: {avg_loss:.6f}\")\n",
    "    print(\"─────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd99617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
